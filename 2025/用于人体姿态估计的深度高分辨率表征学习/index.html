<!DOCTYPE html>
<html lang="zh-CN,en,default">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.mviai.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":true,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="自动摘要: 	摘要	本文主要研究人体姿态估计问题，重点是学习可靠的高分辨率表征方法。大多数现有的方法从由高到低分辨率网络产生的低分辨率表征中恢复高分辨率表征。相反，我们提议的网络在整个过程中保持高分辨率的表征 ……..">
<meta property="og:type" content="article">
<meta property="og:title" content="用于人体姿态估计的深度高分辨率表征学习">
<meta property="og:url" content="http://blog.mviai.com/2025/%E7%94%A8%E4%BA%8E%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="落叶无痕">
<meta property="og:description" content="自动摘要: 	摘要	本文主要研究人体姿态估计问题，重点是学习可靠的高分辨率表征方法。大多数现有的方法从由高到低分辨率网络产生的低分辨率表征中恢复高分辨率表征。相反，我们提议的网络在整个过程中保持高分辨率的表征 ……..">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://blog.mviai.com/images/1673923760528-00c02157-878d-4f62-8bc5-3b6fc9124d9c.png">
<meta property="og:image" content="http://blog.mviai.com/images/1672794976971-6ba530b1-b8f4-4551-a156-c7326c41b246.png">
<meta property="og:image" content="http://blog.mviai.com/images/1672798692307-e6c9a661-d6d4-4e24-ad77-82fe51738534.png">
<meta property="og:image" content="http://blog.mviai.com/images/5686f3ac2b46bee09ca6d226720cb2f3.svg">
<meta property="og:image" content="http://blog.mviai.com/images/53a2b7f0ffc15c986c17cab29f1f1751.svg">
<meta property="og:image" content="http://blog.mviai.com/images/ba877a269c83a43689005bf8128e90a0.svg">
<meta property="og:image" content="http://blog.mviai.com/images/88751a6e5876edba3afbfb240502d054.svg">
<meta property="og:image" content="http://blog.mviai.com/images/1672816866860-ebc276fa-b986-47b5-955a-aeec82b95924.png">
<meta property="og:image" content="http://blog.mviai.com/images/1672817520397-ae26ad72-c84e-4ee1-bd97-5ca0f3611095.png">
<meta property="og:image" content="http://blog.mviai.com/images/fddad9a571de4a77bab2806f05c14584.svg">
<meta property="og:image" content="http://blog.mviai.com/images/9e8dc067353184c149cda0259ade9dfb.svg">
<meta property="og:image" content="http://blog.mviai.com/images/1672817875877-54e633f4-c96e-463b-81dd-c6339451cf30.png">
<meta property="og:image" content="http://blog.mviai.com/images/53b0800f400b2cd7a075c861c0680705.svg">
<meta property="og:image" content="http://blog.mviai.com/images/de699466f12bf76cbc95c3c9fdafc913.svg">
<meta property="og:image" content="http://blog.mviai.com/images/91ac16bced43e3877b07e3bccc8dd793.svg">
<meta property="og:image" content="http://blog.mviai.com/images/4ceba5c631dff9ce903a63d3134db557.svg">
<meta property="og:image" content="http://blog.mviai.com/images/25dad6816112f09c1a4566ac8f6e8541.svg">
<meta property="og:image" content="http://blog.mviai.com/images/7ca2ff984e95d4a9e44fe7498e281020.svg">
<meta property="og:image" content="http://blog.mviai.com/images/7ca2ff984e95d4a9e44fe7498e281020.svg">
<meta property="og:image" content="http://blog.mviai.com/images/cf8b160dfad40c7e65cae69b4a9e1a21.svg">
<meta property="og:image" content="http://blog.mviai.com/images/2addd237806c5ad1ae2619c7bc89e57b.svg">
<meta property="og:image" content="http://blog.mviai.com/images/40dcdd3ee089f48dea0ec908d090b03b.svg">
<meta property="og:image" content="http://blog.mviai.com/images/79b1e218b789236f15ef57584bd18cf5.svg">
<meta property="og:image" content="http://blog.mviai.com/images/1672826424969-abb85e06-6a70-4366-b81c-208bc0c7f0b8.png">
<meta property="og:image" content="http://blog.mviai.com/images/1672884373537-bbfc940e-c843-4c00-91e6-344ba3f66133.png">
<meta property="og:image" content="http://blog.mviai.com/images/1672885715872-e9f52339-dc43-4500-8f7d-df107605458e.png">
<meta property="og:image" content="http://blog.mviai.com/images/1672886104998-cfaceee2-6570-4364-bdc6-076094441adb.png">
<meta property="og:image" content="http://blog.mviai.com/images/1672897634490-34dfbcf1-d60d-4cfb-99ab-d4475996e27f.png">
<meta property="og:image" content="http://blog.mviai.com/images/1672887826229-ce33bfa5-b241-4dca-8e7d-feb9035e1832.png">
<meta property="og:image" content="http://blog.mviai.com/images/1672889017753-0483edaa-a5b8-435c-8157-8820e3061fe0.png">
<meta property="og:image" content="http://blog.mviai.com/images/1672897694925-6418a145-1539-4e74-956b-832d8922db39.png">
<meta property="og:image" content="http://blog.mviai.com/images/1672897781070-54343961-61be-4008-b572-14d070d30040.png">
<meta property="og:image" content="http://blog.mviai.com/images/1672902625970-64d95ab0-d91c-4a32-8bf2-327465d861ce.png">
<meta property="og:image" content="http://blog.mviai.com/images/1672903389844-8c341bd6-0ccf-4627-a23c-33ca6340a8b6.png">
<meta property="og:image" content="http://blog.mviai.com/images/1672903744521-31d070a1-2e0d-47b3-8b88-335c30acf534.png">
<meta property="article:published_time" content="2025-01-22T04:37:40.000Z">
<meta property="article:modified_time" content="2025-01-22T12:37:41.150Z">
<meta property="article:author" content="SindreYang">
<meta property="article:tag" content="生活">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.mviai.com/images/1673923760528-00c02157-878d-4f62-8bc5-3b6fc9124d9c.png">

<link rel="canonical" href="http://blog.mviai.com/2025/%E7%94%A8%E4%BA%8E%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>用于人体姿态估计的深度高分辨率表征学习 | 落叶无痕</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">落叶无痕</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">72</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">321</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <span class="exturl github-corner" data-url="aHR0cHM6Ly9naXRodWIuY29tL1NpbmRyZVlhbmc=" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></span>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.mviai.com/2025/%E7%94%A8%E4%BA%8E%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="SindreYang">
      <meta itemprop="description" content="沉淀后我愿意做一个温暖的人。有自己的喜好，有自己的原则，有自己的信仰，不急功近利，不浮夸轻薄，宠辱不惊，淡定安逸，心静如水。------不忘初心，方得始终">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="落叶无痕">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          用于人体姿态估计的深度高分辨率表征学习
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-01-22 12:37:40 / 修改时间：20:37:41" itemprop="dateCreated datePublished" datetime="2025-01-22T12:37:40+08:00">2025-01-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/HRNET%E8%A1%8D%E7%94%9F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">HRNET衍生网络</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Changyan：</span>
    
    
      <a title="changyan" href="/2025/%E7%94%A8%E4%BA%8E%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/#SOHUCS" itemprop="discussionUrl">
        <span id="changyan_count_unit" class="post-comments-count hc-comment-count" data-xid="2025/用于人体姿态估计的深度高分辨率表征学习/" itemprop="commentCount"></span>
      </a>
    
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>自动摘要: 	摘要	本文主要研究人体姿态估计问题，重点是学习可靠的高分辨率表征方法。大多数现有的方法从由高到低分辨率网络产生的低分辨率表征中恢复高分辨率表征。相反，我们提议的网络在整个过程中保持高分辨率的表征 ……..</p>
<span id="more"></span>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>本文主要研究人体姿态估计问题，重点是学习可靠的高分辨率表征方法。大多数现有的方法从由高到低分辨率网络产生的低分辨率表征中恢复高分辨率表征。相反，我们提议的网络在整个过程中保持高分辨率的表征。</p>
<p>第一阶段从高分辨率子网开始，逐步增加高到低分辨率子网，形成多个阶段，并联多分辨率子网。我们进行重复的多尺度融合，使得每个高分辨率到低分辨率的表征一遍又一遍地接收来自其他并行表征的信息，从而产生丰富的高分辨率表征。因此，预测的关键点热图可能更准确，空间更精确。通过对 COCO 关键点检测数据集和 MPII 人体姿态数据集的优越姿态估计结果，验证了该网络的有效性。此外，我们还在 PoseTrack 数据集上展示了我们的网络在姿态跟踪方面的优越性。代码和模型已经在<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xlb3hpYW9iaW4vZGVlcC1oaWdoLXJlc29sdXRpb24tbmV0LnB5dG9yY2glRTMlODAlODI=" title="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch%E3%80%82">https://github.com/leoxiaobin/deep-high-resolution-net.pytorch。<i class="fa fa-external-link"></i></span></p>
<p><img src="/images/1673923760528-00c02157-878d-4f62-8bc5-3b6fc9124d9c.png"><br>HRNet网络流程图</p>
<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>在计算机视觉中，二维人体姿态估计一直是一个基本而又具有挑战性的问题。目标是定位人体解剖关键点(如肘部、手腕等)或部位。它有很多应用，包括人体动作识别、人机交互、动画等。本文主要研究单人姿态估计，这是多人姿态估计[6, 27, 33, 39, 47, 57, 41, 46, 17, 71] ，视频姿态估计和跟踪[49, 72]等相关问题的基础。</p>
<p>最近的发展表明，深卷积神经网络已经实现了最先进的性能。大多数现有的方法通过网络传递输入，通常由高分辨率到低分辨率的子网络串联组成，然后提高分辨率。例如，沙漏[40]通过一个对称的从低到高的过程来恢复高分辨率。SimpleBaseline [72]采用了几个转置的卷积层来生成高分辨率的表征。此外，扩张卷积还用于炸毁高到低分辨率网络(例如 VGGNet 或 ResNet)的后面几层[27, 77]。</p>
<p>我们提出了一种新的体系结构，即高分辨率网络(HRNet) ，它能够在整个过程中保持高分辨率的表征。首先从高分辨率子网开始，逐步增加高分辨率子网到低分辨率子网，形成多个阶段，并行连接多分辨率子网。通过在并行多分辨率子网上反复交换信息，实现多尺度融合。我们通过网络输出的高分辨率表征来估计关键点。得到的网络如图1所示。</p>
<p><img src="/images/1672794976971-6ba530b1-b8f4-4551-a156-c7326c41b246.png"><br>图1. 说明拟议的人力资源网的结构。它由并行的高分辨率到低分辨率子网组成，在多分辨率子网之间进行重复的信息交换(多尺度融合)。水平方向和垂直方向分别对应于网络的深度和特征映射的比例尺。</p>
<p>我们的网络有两个好处相比现有的广泛使用的网络[40, 27, 77, 72]的姿态估计。(i)我们的方法并行连接高分辨率到低分辨率的子网络，而不是像大多数现有解决方案那样串联。因此，我们的方法能够保持高分辨率，而不是通过从低到高的过程恢复分辨率，因此预测的热图可能在空间上更精确。(ii)大多数现有的融合方案集合了低级别和高级别的表征。相反，我们执行重复的多尺度融合，以提高高分辨率表征的帮助下，相同深度和相似水平的低分辨率表征，反之亦然，导致高分辨率表征也为姿态估计丰富。因此，我们的预测热图可能更准确。</p>
<p>我们经验证明了优于两个基准数据集的关键点检测性能: COCO 关键点检测数据集[36]和 MPII Human Pose 数据集[2]。此外，我们在 PoseTrack 数据集上展示了我们的网络在视频姿态跟踪方面的优势[1]。</p>
<h1 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h1><p>大多数传统的单人姿态估计解决方案采用概率图形模型或图形结构模型[79, 50] ，最近通过利用深度学习来更好地建模一元和成对能量[9, 65, 45]或模仿迭代推理过程[13]。如今，深卷积神经网络提供了主要的解决方案[20, 35, 62, 42, 43, 48, 58, 16]。有两种主流方法: 回归关键点的位置[66, 7] ，估计关键点热图[13, 14, 78] ，然后选择热值最高的位置作为关键点。</p>
<p>大多数用于关键点热图估计的卷积神经网络由一个类似于分类网络的干子网络组成，该子网络降低了分辨率，主体产生与其输入相同分辨率的表征，然后回归估计热图，估计关键点位置，然后转换为完全分辨率。主体部分主要采用从高到低和从低到高的框架结构，可能增加多尺度融合和中(深)监控。</p>
<p><strong>从高到低，从低到高</strong>。高到低的过程旨在产生低分辨率和高级别的表征，而低到高的过程旨在产生高分辨率的表征[4, 11, 23, 72, 40, 62]。为了提高性能，这两个过程都可能重复几次[77, 40, 14]。</p>
<p>具有代表性的网络设计模式包括: (i)对称的从高到低和从低到高的过程。Hourglass及其后续[40, 14, 77, 31]设计了从低到高的过程，作为从高到低过程的一面镜子。(ii)由大至小及由小至大。高到低的过程是基于 ImageNet 分类网络的，例如[11, 72]中采用的 ResNet，而低到高的过程仅仅是几个双线性上采样[11]或转位卷积[72]层。(iii)与扩张卷积相结合。在[27, 51, 35]中，在 ResNet 或 VGGNet 的最后两个阶段采用扩张卷积来消除空间分辨率损失，然后进行轻微的低-高过程以进一步提高分辨率，避免仅使用扩张卷积的昂贵计算成本[11, 27, 51]。图2描述了四个代表性的姿态估计网络。</p>
<p><img src="/images/1672798692307-e6c9a661-d6d4-4e24-ad77-82fe51738534.png"><br>图2. 依赖于高到低和低到高框架的代表性姿态估计网络的说明。(a)Hourglass[40]；(b)层叠式金字塔网络[11]；(c) 简单基线 [72] : 用于低到高处理的转置卷积；(d)与扩张卷积相结合[27]。右下角的图例:  reg. &#x3D; 规则卷积， dilated &#x3D; 扩张卷积， trans. &#x3D;  移位卷积，strided &#x3D; 跨步卷积，concat. &#x3D; 连接。在(a)中，从高到低和从低到高的过程是对称的。在(b)、(c)和(d)中，从高到低的过程(分类网络(ResNet 或 VGGNet)的一部分)很重，从低到高的过程很轻。在(a)和(b)中，高到低和低到高过程的相同分辨率层之间的跳跃连接(虚线)主要旨在融合低级和高级特征。在(b)中，正确的部分，细分网络，结合了通过卷积处理的低级和高级特征。</p>
<p><strong>多尺度融合</strong>。直接的方法是将多分辨率图像分别输入多个网络并聚合输出响应图[64]。Hourglass[40]及其扩展[77, 31]通过跳跃连接逐步将高到低过程中的低级特性结合到低到高过程中的相同分辨率的高级特性中。在级联金字塔网络[11]中，全球网络逐步将高到低过程中的低到高级特征结合到低到高过程中，然后精炼网络结合通过卷积处理的低到高级特征。我们的方法重复多尺度融合，其部分灵感来自深度融合及其扩展[67, 73, 59, 80, 82]。</p>
<p><strong>中级监督</strong>。早期用于图像分类的中级监督或深度监督[34, 61]也被用于帮助深度网络培训和提高热图估计质量，例如[69, 40, 64, 3, 11]。沙漏方法[40]和卷积姿态机方法[69]处理中间热图作为剩余子网络的输入或输入的一部分。</p>
<p><strong>我们的方法</strong>。我们的网络并行地连接高到低的子网络。它在整个过程中保持高分辨率的表征，以便进行空间精确的热图估计。它通过反复融合由高到低子网产生的表征，生成可靠的高分辨率表征。我们的方法不同于大多数现有的工作，它们需要一个单独的低到高的上抽样过程，并聚合低层和高层的表征。该方法不需要中间热图监控，在关键点检测精度和计算复杂度和参数方面具有优势。</p>
<p>有相关的多尺度网络用于分类和分割[5, 8, 74, 81, 30, 76, 55, 56, 24, 83, 55, 52, 18]。我们的工作部分地受到其中一些[56, 24, 83, 55]的启发，它们之间存在明显的差异，使它们不适用于我们的问题。由于每个子网络(深度，批量归一化)和多尺度融合缺乏适当的设计，卷积神经网络结构[56]和相互连接的 CNN [83]无法产生高质量的分割结果。网格网络[18]是许多权重共享的 U-Net 的组合，由跨多分辨率表征的两个独立的融合过程组成: 在第一阶段，信息只从高分辨率发送到低分辨率; 在第二阶段，信息只从低分辨率发送到高分辨率，因此竞争较小。多尺度密集集[24]不是目标，也不能产生可靠的高分辨率表征。  </p>
<h1 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h1><p>人体姿态估计，又称关键点检测，目的是从图像 I 的大小 W × H × 3中检测 K 关键点或部位(如肘部、手腕等)的位置。最先进的方法将这个问题转化为估计 W’  × H’，{<img src="/images/5686f3ac2b46bee09ca6d226720cb2f3.svg"><br>}的 K 热图，其中每个热图 HK 表征 kth 关键点的位置置信度。</p>
<p>我们遵循广泛采用的流水线[40, 72, 11] ，使用卷积网络来预测人类关键点，卷积网络由两个步进卷积组成，降低分辨率，主体输出与其输入特征映射相同分辨率的特征映射，回归估计热图关键点位置被选择并转换为完全分辨率。我们将重点放在主体的设计上，并介绍我们的高分辨率网络(HRNet) ，如图1所示。</p>
<p><strong>有序多分辨率子网络</strong>。现有的姿态估计网络是通过串联高分辨率到低分辨率的子网建立起来的，其中每个子网构成一个阶段，由一系列卷积组成，相邻子网之间有一个下采样层，使分辨率降低一半。</p>
<p>设<img src="/images/53a2b7f0ffc15c986c17cab29f1f1751.svg"><br>为某阶段的子网，r 为分辨率指标(其分辨率为第一子网分辨率的<img src="/images/ba877a269c83a43689005bf8128e90a0.svg"><br>。具有 S (例如，4)阶段的从高到低的网络可以表示为: <img src="/images/88751a6e5876edba3afbfb240502d054.svg"><br>                                                            (1)</p>
<p><strong>并行多分辨率子网</strong>。第一阶段从高分辨率子网开始，逐步增加高到低分辨率子网，形成新的阶段，并行连接多分辨率子网。因此，后一阶段的并行子网的分辨率包括前一阶段的分辨率和后一阶段的分辨率。</p>
<p>给出了一个包含4个并行子网络的网络结构实例，<img src="/images/1672816866860-ebc276fa-b986-47b5-955a-aeec82b95924.png"><br>                                                     （2）</p>
<p><strong>重复多尺度融合</strong>。我们在并行子网络之间引入交换单元，使每个子网络重复接收来自其他并行子网络的信息。下面是一个显示信息交换方案的示例。我们将第三阶段划分为几个交换块(例如，3)，每个块由3个平行卷积单元组成，每个平行卷积单元之间有一个交换单元，具体如下：<img src="/images/1672817520397-ae26ad72-c84e-4ee1-bd97-5ca0f3611095.png"><br>                                          （3）其中<img src="/images/fddad9a571de4a77bab2806f05c14584.svg"><br>表示某阶段中第二个块的第 r 个分辨率的卷积单位， <img src="/images/9e8dc067353184c149cda0259ade9dfb.svg"><br>表示相应的交换单位。</p>
<p><img src="/images/1672817875877-54e633f4-c96e-463b-81dd-c6339451cf30.png"><br>图3. 说明交换单元如何分别从左到右聚合高、中和低分辨率的信息。右图例: 跨步3 × 3 &#x3D; 跨步3 × 3卷积，上采样。1 × 1 &#x3D; 1 × 1卷积后的最近邻上采样。</p>
<p>我们在图3中说明了交换单元，并在下面给出了公式。为了方便讨论，我们删除了下标 s 和上标 b。输入是 s 响应映射: {<img src="/images/53b0800f400b2cd7a075c861c0680705.svg"><br>}。输出是 s 响应映射: { <img src="/images/de699466f12bf76cbc95c3c9fdafc913.svg"><br>} ，其分辨率和宽度与输入相同。每个输出是输入映射的一个聚合，<img src="/images/91ac16bced43e3877b07e3bccc8dd793.svg"><br>。跨阶段的交换单元有一个额外的输出映射<img src="/images/4ceba5c631dff9ce903a63d3134db557.svg"><br>。</p>
<p>函数 a (<img src="/images/25dad6816112f09c1a4566ac8f6e8541.svg"><br>)由从分辨率 i 到分辨率 k 的上采样或下采样<img src="/images/7ca2ff984e95d4a9e44fe7498e281020.svg"><br>组成。采用跨距3 × 3卷积进行下采样。例如，一个步长3 × 3卷积，步长2为 2 × 下采样，两个连续的步长3 × 3卷积，步长2为 4 × 下采样。对于上采样，我们采用1 × 1卷积后的简单最近邻采样来校准通道数。如果 i &#x3D; k，则 a (· ，·)只是一个标识连接: a (<img src="/images/7ca2ff984e95d4a9e44fe7498e281020.svg"><br>。</p>
<p><strong>热图估计</strong>。我们简单地从最后一个交换单元的高分辨率表征输出回归热图，这在经验上是有效的。损失函数被定义为均方差，用于比较预测的热图和地面事实热图。地面真相热标准差是通过应用以每个关键点坐标为中心的1个像素的二维高斯分布生成的。  <strong>网络实例化</strong>。通过遵循 ResNet 的设计原则，实例化了关键点热图估计网络，将深度分布到每个阶段，通道数分布到每个分辨率。</p>
<p>主体，即我们的HRNet，包含四个阶段和四个并行的子网，其分辨率逐渐减少到一半，因此宽度(通道数目)增加到两倍。第一阶段包含4个剩余单元，其中每个单元与 ResNet-50相同，由宽度为64的bottleneck形成，然后是一个3 × 3卷积，将特征映射的宽度减少到 C。第2、第3、第4阶段分别包含1、4、3个交换块。一个交换块包含4个剩余单元，其中每个单元包含每个分辨率的两个3 × 3卷积和一个跨分辨率的交换单元。总共有8个交换单元，即进行了8次多尺度融合。</p>
<p>在我们的实验中，我们研究了一个小网络和一个大网络: HRNet-W32和 HRNet-W48，其中32和48分别代表最后三个阶段的高分辨率子网的宽度(C)。其他三个并行子网的宽度分别为 HRNet-W32的64, 128, 256和 HRNet-W48的96, 192, 384。</p>
<h1 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h1><h2 id="4-1-COCO关键点检测"><a href="#4-1-COCO关键点检测" class="headerlink" title="4.1  COCO关键点检测"></a>4.1  COCO关键点检测</h2><p><strong>数据集</strong>. COCO 数据集[36]包含超过200,000张图像和250,000个人实例，标有17个关键点。我们在 COCO train 2017数据集上训练我们的模型，包括57K 图像和150K 人实例。我们在 val2017集和 test-dev2017集上评估我们的方法，它们分别包含5000个图像和20K 个图像。</p>
<p><strong>评估指标</strong>. 标准的评估度量基于对象关键点相似度(OKS) : <img src="/images/cf8b160dfad40c7e65cae69b4a9e1a21.svg"><br>。这里的<img src="/images/2addd237806c5ad1ae2619c7bc89e57b.svg"><br>是检测到的关键点和对应的地面真值之间的欧几里得度量，<img src="/images/40dcdd3ee089f48dea0ec908d090b03b.svg"><br>是地面真值的可见性标志，s 是对象刻度，而<img src="/images/79b1e218b789236f15ef57584bd18cf5.svg"><br> 是控制衰减的每个关键点常数。我们报告了标准平均准确率召回率得分1: AP50(在 OKS &#x3D; 0.50的 AP) AP75，AP (在10个位置的 AP 得分平均值，OKS &#x3D; 0.50, 0.55，… ，0.90, 0.95) ; 中等物体APM，大物体APL，以及在 OKS &#x3D; 0.50, 0.55，… ，0.90, 0.955的 AR。</p>
<p><strong>训练</strong>. 我们将人类检测框的高度或宽度扩展到固定的长宽比: 高度: 宽度 &#x3D; 4:3，然后从图像中裁剪盒子，将其调整为固定大小，256 × 192或384 × 288。数据增强包括随机旋转([ -45。，45。]) ，随机刻度([0.65，1.35])和翻转。在[68]之后，半身数据增加也包括在内。 我们使用 Adam 优化器[32]。学习时间表遵循设置[72]。基础学习率设置为1e-3，在第170和200 epochs 分别下降到1e-4和1e-5。训练过程在210 epochs 内结束。</p>
<p><strong>测试</strong>. 使用类似于[47,11,72]的两阶段自顶向下范例: 使用人检测器检测人实例，然后预测检测关键点。我们对验证集和 test-dev 集使用 SimpleBaseline2[72]提供的相同的人员检测器。按照通常的做法[72, 40, 11] ，我们通过平均原始图像和翻转图像的头图来计算热图。每个关键点位置通过调整最高热值位置来预测，从最高响应到第二高响应的方向有四分之一的偏移。</p>
<p><img src="/images/1672826424969-abb85e06-6a70-4366-b81c-208bc0c7f0b8.png"><br>表1. COCO 验证集的比较。Pretrain &#x3D; 对 ImageNet 分类任务的主干进行预训练。OHKM &#x3D; 在线硬关键点挖掘[11]</p>
<p><strong>验证集上的结果</strong>. 我们在表1中报告了我们的方法和其他最先进的方法的结果。我们的小型网络-HRNet-W32，从头开始训练，输入大小为256 × 192，获得73.4 AP 分数，优于其他同样输入大小的方法。(i)与Hourglass[40]相比，我们的小网络提高了6.5点 AP，我们网络的 GFLOP 低得多，不到一半，而参数数量相似，我们的略大。(ii)与 CPN [11] w&#x2F;o 和 w&#x2F;OHKM 相比，我们的网络模型规模稍大，复杂度稍高，分别获得了4.8和4.0点的增益。(iii)与之前表现最好的 SimpleBaseline [72]相比，我们的小网 HRNet-W32获得了显着的改善: 模型大小和 GFLOPs 相似的主干 ResNet-50增加了3.0分，模型大小(# Params)和 GLOP 是我们的两倍的主干 ResNet-152增加了1.4分。</p>
<p>我们的网络可以受益于(i)从为 ImageNet 分类问题预先训练的模型的训练：HRNet-W32的增益为1.0点；(ii)通过增加宽度来增加容量：我们的大网 HRNet-W48分别在输入大小256 × 192和384 × 288方面得到0.7和0.5的改进。</p>
<p>考虑到输入大小为384 × 288，我们的 HRNet-W32和 HRNet-W48分别得到了75.8和76.3 AP，比输入大小256 × 192分别提高了1.4和1.2个级别。与使用 ResNet-152作为骨干的 SimpleBaseline [72]相比，我们的 HRNet-W32和 HRNet-W48在 AP 方面分别以45% 和92.4% 的计算成本获得1.5和2.0分。</p>
<p><img src="/images/1672884373537-bbfc940e-c843-4c00-91e6-344ba3f66133.png"><br>表2. COCO 测试开发集的比较。为姿态估计网络计算 #Params 和 FLOP，不包括用于人类检测和关键点分组的参数和 FLOP。</p>
<p><strong>Test-dev 集上的结果</strong>. 表2报告了我们的方法和现有的最先进的方法的姿态估计性能。我们的方法明显优于自底向上的方法。另一方面，我们的小型网络 HRNet-W32实现了74.9的 AP。它优于所有其他自顶向下的方法，并且在模型大小(# Params)和计算复杂度(GFLOPs)方面更有效率。我们的大型模型，HRNet-W48，达到了最高的75.5 AP。与具有相同输入大小的 SimpleBaseline [72]相比，我们的小型和大型网络分别得到了1.2和1.8的改进。通过 AI 挑战赛[70]提供的额外训练数据，我们的单个大型网络可以获得77.0的 AP。</p>
<h2 id="4-2-MPII-人体姿态估计"><a href="#4-2-MPII-人体姿态估计" class="headerlink" title="4.2 MPII 人体姿态估计"></a>4.2 MPII 人体姿态估计</h2><p><strong>数据集</strong>. MPII 人体姿势数据集[2]包括从现实世界活动与全身姿势注释的广泛范围采取的图像。有大约25K 的图像与40K 的主题，其中有12K 的主题测试和其余的主题训练集。数据增强和训练策略与 MS COCO 相同，只是输入量被裁剪为256 × 256，以便与其他方法进行公平比较。</p>
<p><strong>测试</strong>. 测试程序与 COCO 的测试程序基本相同，只是我们采用了标准的测试策略，使用提供的人员箱代替检测到的人员箱。在[14, 77, 62]之后，执行一个六级金字塔测试程序。</p>
<p><strong>评估指标</strong>. 使用标准度量[2] ，即 PCKh (正确关键点的头部归一化概率)得分。联合是正确的，如果它落在地面真理位置的 αl 像素，其中 α 是一个常数，l 是头大小，相当于地面真理头边界盒对角线长度的60% 。报告 <span class="exturl" data-url="bWFpbHRvOiYjODA7JiN4NDM7JiM3NTsmI3g2ODsmIzY0OyYjNDg7JiM0NjsmI3gzNTs=" title="mailto:&#80;&#x43;&#75;&#x68;&#64;&#48;&#46;&#x35;">&#80;&#x43;&#75;&#x68;&#64;&#48;&#46;&#x35;<i class="fa fa-external-link"></i></span>(α &#x3D; 0.5)分数。</p>
<p><img src="/images/1672885715872-e9f52339-dc43-4500-8f7d-df107605458e.png"><br>表3. MPII 测试集的性能比较(<span class="exturl" data-url="bWFpbHRvOiYjODA7JiN4NDM7JiN4NGI7JiMxMDQ7JiN4NDA7JiM0ODsmI3gyZTsmI3gzNTs=" title="mailto:&#80;&#x43;&#x4b;&#104;&#x40;&#48;&#x2e;&#x35;">&#80;&#x43;&#x4b;&#104;&#x40;&#48;&#x2e;&#x35;<i class="fa fa-external-link"></i></span>)。</p>
<p><img src="/images/1672886104998-cfaceee2-6570-4364-bdc6-076094441adb.png"><br>表4. 表3中报告的一些顶级方法的#Params  和 GFLOP。该 GFLOP 计算的输入大小为256 × 256。</p>
<p><strong>测试集的结果</strong>. 表3和表4显示了 <span class="exturl" data-url="bWFpbHRvOiYjeDUwOyYjNjc7JiM3NTsmIzEwNDsmI3g0MDsmIzQ4OyYjNDY7JiN4MzU7" title="mailto:&#x50;&#67;&#75;&#104;&#x40;&#48;&#46;&#x35;">&#x50;&#67;&#75;&#104;&#x40;&#48;&#46;&#x35;<i class="fa fa-external-link"></i></span>的结果，模型大小和最高性能方法的 GFLOP。我们通过使用 ResNet-152作为输入大小为256 × 256的主干来重新实现 SimpleBaseline [72]。我们的 HRNet-W32达到92.3 <span class="exturl" data-url="bWFpbHRvOiYjODA7JiM3NTsmIzY3OyYjeDY4OyYjeDQwOyYjNDg7JiM0NjsmI3gzNTs=" title="mailto:&#80;&#75;&#67;&#x68;&#x40;&#48;&#46;&#x35;">&#80;&#75;&#67;&#x68;&#x40;&#48;&#46;&#x35;<i class="fa fa-external-link"></i></span>分，并优于叠加沙漏方法[40]及其扩展[58,14,77,31,62]。我们的结果与20183年11月16日之前公布的排行榜上最好的结果相同[62]。</p>
<p>我们想指出的是，该方法[62] ，补充我们的方法，利用组合模型来学习人体的配置，并采用多层次的中间监督，我们的方法也可以从中受益。我们还测试了我们的大型网络 HRNetW48，得到了相同的结果92.3。原因可能是这个数据集中的性能趋于饱和。</p>
<h2 id="4-3-姿势跟踪应用"><a href="#4-3-姿势跟踪应用" class="headerlink" title="4.3 姿势跟踪应用"></a>4.3 姿势跟踪应用</h2><p><strong>数据集</strong>. PoseTrack[28]是一个用于人体姿态估计和视频关节跟踪的大规模基准。该数据集基于流行的 MPII Human Pose 数据集提供的原始视频，包含550个视频序列和66,374帧。视频序列分为292,50,208个视频，分别用于培训，验证和测试。训练视频的长度在41-151帧之间，视频中心的30帧被密集地注释。验证&#x2F;测试视频中的帧数介于65 - 298帧之间。来自 MPII Pose 数据集的关键帧周围的30帧被密集注释，之后每四帧都被注释。总共有大约23,000个标签框架和153,615个姿势注释。</p>
<p><strong>评估指标</strong>. 我们从两个方面评估结果: 帧式多人姿态估计和多人姿态跟踪。姿态估计是通过平均精度(mAP)来评估的，如[51, 28]中所做的。多人姿态跟踪是通过多目标跟踪精度(MOTA)来评估的[38, 28]。详情见[28]。</p>
<p><strong>训练</strong>. 我们训练我们的 HRNet-W48在 PoseTrack2017训练集上进行单人姿态估计，其中网络是由在 COCO 数据集上预先训练的模型初始化的。我们从训练框架中的注释关键点中提取人盒作为网络的输入，方法是将所有关键点(对于一个人)的边界盒扩展15% 的长度。包括数据增强在内的训练设置几乎与 COCO 相同，只是学习时间表不同(就像现在的微调一样) : 学习率从1e-4开始，在第10 epochs 下降到1e-5，在第15 epochs 下降到1e-6; 迭代在20 epochs 内结束。</p>
<p><img src="/images/1672897634490-34dfbcf1-d60d-4cfb-99ab-d4475996e27f.png"><br>图4. MPII (顶部)和 COCO (底部)数据集中一些示例图像的定性结果：包含视点和外观变化，遮挡，多人和常见的成像伪影。</p>
<p><strong>测试</strong>. 我们跟踪[72]，跟踪跨帧的姿势。它包括三个步骤: 人体盒检测和传播、人体姿态估计和交叉相邻帧的姿态关联。我们使用与 SimpleBaseline [72]中使用的相同的人盒检测器，并通过根据 FlowNet 2.0[26]4计算的光流传播预测的关键点将检测到的盒子传播到附近的帧中，然后对盒子移除进行非最大抑制。姿态关联方案是基于一帧中的关键点与相邻帧中根据光流传播的关键点之间的目标关键点相似性。然后利用贪婪匹配算法计算邻近帧中关键点之间的对应关系。更多的细节在[72]中给出。</p>
<p><img src="/images/1672887826229-ce33bfa5-b241-4dca-8e7d-feb9035e1832.png"><br>表5. PoseTrack2017测试集的姿态跟踪结果。</p>
<p><strong>PoseTrack 2017 测试集的结果</strong>. 表5报告了结果。我们的大型网络 -HRNet-W48取得了优异的成绩，74.9 mAP 得分和57.9 MOTA 得分。</p>
<p>与使用 ResNet-152作为骨干的第二个最佳方法 SimpleBaseline [72]中的 FlowTrack 相比，我们的方法在 mAP 和 MOTA 方面分别获得0.3和0.1点增益。相对于 FlowTrack [72]的优势与 COCO 关键点检测和 MPII 人体姿态估计数据集的优势一致。这进一步说明了我们的姿态估计网络的有效性。</p>
<h2 id="4-4-消融实验"><a href="#4-4-消融实验" class="headerlink" title="4.4 消融实验"></a>4.4 消融实验</h2><p>我们研究了该方法中每个组件对 COCO 关键点检测数据集的影响。除了对输入量影响的研究外，所有结果均在256 × 192的输入量范围内得到。</p>
<p><img src="/images/1672889017753-0483edaa-a5b8-435c-8157-8820e3061fe0.png"><br>表6. 重复多尺度融合中交换单元的消融研究。 Int. exchange across &#x3D; 跨阶段的中间交换，Int. exchange within &#x3D; 阶段内中间交换。  </p>
<p><strong>重复多尺度融合</strong>. 我们对多尺度重复融合的效果进行了实证分析。我们研究我们网络的三种变体。(a) W&#x2F;O 中间交换单元(1个融合) : 除最后一个交换单元外，多分辨率子网之间不存在交换。(b) 仅 W&#x2F;跨级交换单元(3个融合) : 每个阶段内并行子网之间没有交换。(c) W&#x2F;跨阶段和阶段内交换单元(共8个融合) : 这是我们提出的方法。所有的网络都是从头开始训练的。表6 中给出的 COCO 验证集的结果表明，多尺度融合是有益的，更多的融合导致更好的性能。</p>
<p><strong>分辨率维护</strong>. 我们研究了人力资源网的一个变种的性能: 所有的四个高分辨率到低分辨率的子网都是在开始时加入的，而且深度是相同的，融合方案与我们的相同。我们的 HRNet-W32和变体(具有类似的 #Params 和 GFLOPs)都是从头开始训练的，并在 COCO 验证集上进行测试。该变体获得的 AP 为72.5，低于我们的小型网络 HRNet-W32的73.4 AP。我们认为这是因为从低分辨率子网的早期阶段提取的低层特征没有多大帮助。此外，参数相似、计算复杂度相似的简单高分辨率网络，如果没有低分辨率并行子网络，其性能会大大降低。</p>
<p><strong>代表解决方案</strong>. 本文从两个方面研究了表征分辨率对姿态估计性能的影响: 从高到低检查每个分辨率的特征映射估计的热图的质量，以及输入大小对热图质量的影响。</p>
<p><img src="/images/1672897694925-6418a145-1539-4e74-956b-832d8922db39.png"><br>图5. 高低表征的消融研究。1 × ，2 × ，4 × 分别对应于高、中、低分辨率的表征。</p>
<p><img src="/images/1672897781070-54343961-61be-4008-b572-14d070d30040.png"><br>图6. 说明了 HRNet 和 SimpleBaseline [72]的性能如何受到输入大小的影响。</p>
<p>我们通过为 ImageNet 分类预先训练的模型来初始化我们的小型和大型网络。我们的网络从高到低的解决方案输出四个响应图。在最低分辨率响应图上的热图预测质量太低，AP 得分低于10分。图5中报告了其他三张地图上的 AP 得分。比较结果表明，分辨率对关键点预测质量有一定的影响。</p>
<p>图6显示了与 SimpleBaseline (ResNet-50)[72]相比，输入图像大小如何影响性能。我们可以发现，对于较小的输入规模的改善比较大的输入规模更显著，例如，改善是4.0分为256 × 192和6.3分为128 × 96。原因是我们在整个过程中保持了高分辨率。这意味着我们的方法更有利于实际应用中的计算成本也是一个重要因素。另一方面，我们的输入大小为256 × 192的方法优于 SimpleBaseline [72] ，输入大小为384 × 288。</p>
<h1 id="5-结论和未来工作"><a href="#5-结论和未来工作" class="headerlink" title="5. 结论和未来工作"></a>5. 结论和未来工作</h1><p>在本文中，我们提出了一个高分辨率的人类姿态估计网络，产生准确和空间精确的关键点热图。其成功有两个方面: (1)在整个过程中保持高分辨率而不需要恢复高分辨率; (2)反复融合多分辨率表征，提供可靠的高分辨率表征。</p>
<p>未来的工作包括应用于其他密集的预测任务，例如，语义分割，目标检测，人脸对齐，图像翻译，以及研究聚合多分辨率表征在一个不那么轻松的方式。所有这些都在 <span class="exturl" data-url="aHR0cHM6Ly9qaW5nZG9uZ3dhbmcyMDE3Lw==" title="https://jingdongwang2017/">https://jingdongwang2017<i class="fa fa-external-link"></i></span>. github.io&#x2F;Projects&#x2F;HRNet&#x2F;index.html.  </p>
<h2 id="MPII-验证集的附录结果"><a href="#MPII-验证集的附录结果" class="headerlink" title="MPII 验证集的附录结果"></a>MPII 验证集的附录结果</h2><p>我们在 MPII 验证集[2]上提供结果。我们的模型在 MPII 训练集的子集上进行训练，并在2975幅图像的拒绝验证集上进行评估。训练过程与整个 MPII 训练集的训练过程相同。热图被计算为用于测试的原始图像和翻转图像的热图的平均值。在[77,62]之后，我们还执行了六尺度金字塔测试程序(多尺度测试)。结果如表7所示。</p>
<p><img src="/images/1672902625970-64d95ab0-d91c-4a32-8bf2-327465d861ce.png"><br>表7. MPII 验证集的性能比较(<span class="exturl" data-url="bWFpbHRvOiYjODA7JiM2NzsmI3g0YjsmI3g2ODsmIzY0OyYjeDMwOyYjNDY7JiN4MzU7" title="mailto:&#80;&#67;&#x4b;&#x68;&#64;&#x30;&#46;&#x35;">&#80;&#67;&#x4b;&#x68;&#64;&#x30;&#46;&#x35;<i class="fa fa-external-link"></i></span>)。</p>
<h2 id="关于-PoseTrack-数据集的更多结果"><a href="#关于-PoseTrack-数据集的更多结果" class="headerlink" title="关于 PoseTrack 数据集的更多结果"></a>关于 PoseTrack 数据集的更多结果</h2><p> 我们提供的结果，所有的关键点上的姿势跟踪数据集[1]。表8显示了 PoseTrack2017数据集上的多人姿态估计性能。我们的 HRNet-W48在验证和测试集上达到77.3和74.9分 mAP，并且比以前的最先进的方法[72]分别高出0.6分和0.3分。我们在 PoseTrack2017测试集中提供了更详细的多人姿态跟踪性能结果，作为本文报告结果的补充，如表9所示。</p>
<p><img src="/images/1672903389844-8c341bd6-0ccf-4627-a23c-33ca6340a8b6.png"><br>表8. PoseTrack2017数据集上的多人姿态估计性能(MAP)。“ *”指在训练 + 验证集上训练的模型。</p>
<p><img src="/images/1672903744521-31d070a1-2e0d-47b3-8b88-335c30acf534.png"><br>表9. PoseTrack2017测试集上的多人姿态跟踪性能(MOTA)。“ *”指在训练 + 验证集上训练的模型。</p>
<h2 id="ImageNet-验证集的结果"><a href="#ImageNet-验证集的结果" class="headerlink" title="ImageNet 验证集的结果"></a>ImageNet 验证集的结果</h2><p>我们将网络应用于图像分类任务。这些模型是在 ImageNet 2013分类数据集上进行训练和评估的[54]。我们训练我们的模型为100个 epochs ，批量大小为256。初始学习率设置为0.1，并在第30、60和90 epochs 减少10倍。我们的模型可以达到与那些专门为图像分类而设计的网络相当的性能，例如 ResNet [22]。我们的 HRNet-W32单模型的前5位验证误差为6.5% ，单模型的前1位验证误差为22.7% 。我们的 HRNet-W48 性能更好: 前5位错误占6.1% ，前1位错误占22.1% 。我们使用在 ImageNet 数据集上训练的模型来初始化我们的姿态估计网络的参数。</p>
<h1 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h1><p>[1] M. Andriluka, U. Iqbal, A. Milan, E. Insafutdinov, L. Pishchulin, J. Gall, and B. Schiele. Posetrack: A bench-mark for human pose estimation and tracking. In CVPR, pages 5167–5176, 2018. 2, 9[2] M. Andriluka, L. Pishchulin, P. V. Gehler, and B. Schiele. 2d human pose estimation: New benchmark and state of the art analysis. In CVPR, pages 3686–3693, 2014. 2, 6, 9[3] V. Belagiannis and A. Zisserman. Recurrent human pose estimgation. In FG, pages 468–475, 2017. 3[4] A. Bulat and G. Tzimiropoulos. Human pose estimation via convolutional part heatmap regression. In ECCV, volume 9911 of Lecture Notes in Computer Science, pages 717–732. Springer, 2016. 2, 6[5] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos. A unified multi-scale deep convolutional neural network for fast object detection. In ECCV, pages 354–370, 2016. 3[6] Z. Cao, T. Simon, S. Wei, and Y. Sheikh. Realtime multiperson 2d pose estimation using part affinity fields. In CVPR, pages 1302–1310, 2017. 1, 5[7] J. Carreira, P. Agrawal, K. Fragkiadaki, and J. Malik. Human pose estimation with iterative error feedback. In CVPR, pages 4733–4742, 2016. 2[8] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Trans. Pattern Anal. Mach. Intell., 40(4):834–848, 2018. 3[9] X. Chen and A. L. Yuille. Articulated pose estimation by a graphical model with image dependent pairwise relations. In NIPS, pages 1736–1744, 2014. 2[10] Y. Chen, C. Shen, X. Wei, L. Liu, and J. Yang. Adversarial posenet: A structure-aware convolutional network for human pose estimation. In ICCV, pages 1221–1230, 2017. 6[11] Y. Chen, Z. Wang, Y. Peng, Z. Zhang, G. Yu, and J. Sun. Cascaded pyramid network for multi-person pose estimation. CoRR, abs&#x2F;1711.07319, 2017. 2, 3, 5, 6[12] C. Chou, J. Chien, and H. Chen. Self adversarial training for human pose estimation. CoRR, abs&#x2F;1707.02439, 2017. 6[13] X. Chu, W. Ouyang, H. Li, and X. Wang. Structured feature learning for pose estimation. In CVPR, pages 4715–4723,2016. 2[14] X. Chu, W. Yang, W. Ouyang, C. Ma, A. L. Yuille, and X. Wang. Multi-context attention for human pose estimation. In CVPR, pages 5669–5678, 2017. 2, 6[15] A. Doering, U. Iqbal, and J. Gall. Joint flow: Temporal flow fields for multi person tracking, 2018. 7[16] X. Fan, K. Zheng, Y. Lin, and S. Wang. Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation. In CVPR, pages 1347–1355, 2015. 2[17] H. Fang, S. Xie, Y. Tai, and C. Lu. RMPE: regional multiperson pose estimation. In ICCV, pages 2353–2362, 2017.1, 5[18] D. Fourure, R. Emonet, E. Fromont, D. Muselet, ´A. Tremeau, and C. Wolf. Residual conv-deconv grid net- ´work for semantic segmentation. In British Machine Vision Conference 2017, BMVC 2017, London, UK, September 4-7, 2017, 2017. 3[19] R. Girdhar, G. Gkioxari, L. Torresani, M. Paluri, and D. Tran. Detect-and-track: Efficient pose estimation in videos. In CVPR, pages 350–359, 2018. 7, 9[20] G. Gkioxari, A. Toshev, and N. Jaitly. Chained predictions using convolutional neural networks. In ECCV, pages 728–743, 2016. 2[21] K. He, G. Gkioxari, P. Dollar, and R. B. Girshick. Mask ´R-CNN. In ICCV, pages 2980–2988, 2017. 5[22] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016. 9[23] P. Hu and D. Ramanan. Bottom-up and top-down reasoning with hierarchical rectified gaussians. In CVPR, pages 5600–5609, 2016. 2[24] G. Huang, D. Chen, T. Li, F. Wu, L. van der Maaten, and K. Q. Weinberger. Multi-scale dense convolutional networks for efficient prediction. CoRR, abs&#x2F;1703.09844, 2017. 3[25] S. Huang, M. Gong, and D. Tao. A coarse-fine network for keypoint localization. In ICCV, pages 3047–3056. IEEE Computer Society, 2017. 5[26] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In CVPR, pages 1647–1655, 2017. 7[27] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In ECCV, pages 34–50, 2016.1, 2, 3, 6, 7[28] U. Iqbal, A. Milan, and J. Gall. Posetrack: Joint multi-person pose estimation and tracking. In CVPR, pages 4654–4663,2017. 6, 7[29] S. Jin, X. Ma, Z. Han, Y. Wu, W. Yang, W. Liu, C. Qian, and W. Ouyang. Towards multi-person pose tracking: Bottom-up and top-down methods. In ICCV PoseTrack Workshop, 2017.7[30] A. Kanazawa, A. Sharma, and D. W. Jacobs. Locally scale-invariant convolutional neural networks. CoRR, abs&#x2F;1412.5104, 2014. 3[31] L. Ke, M. Chang, H. Qi, and S. Lyu. Multi-scale structure-aware network for human pose estimation. CoRR, abs&#x2F;1803.09894, 2018. 2, 3, 6[32] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs&#x2F;1412.6980, 2014. 5[33] M. Kocabas, S. Karagoz, and E. Akbas. Multiposenet: Fast multi-person pose estimation using pose residual network. In ECCV, volume 11215 of Lecture Notes in Computer Science, pages 437–453. Springer, 2018. 1, 5[34] C. Lee, S. Xie, P. W. Gallagher, Z. Zhang, and Z. Tu. Deeplysupervised nets. In AISTATS, 2015. 3[35] I. Lifshitz, E. Fetaya, and S. Ullman. Human pose estimation using deep consensus voting. In ECCV, pages 246–260,2016. 2, 3[36] T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. Microsoft COCO: com- ´mon objects in context. In ECCV, pages 740–755, 2014. 2,4[37] D. C. Luvizon, H. Tabia, and D. Picard. Human pose regression by combining indirect part detection and contextual information. CoRR, abs&#x2F;1710.02322, 2017. 6[38] A. Milan, L. Leal-Taixe, I. D. Reid, S. Roth, and ´K. Schindler. MOT16: A benchmark for multi-object tracking. CoRR, abs&#x2F;1603.00831, 2016. 7[39] A. Newell, Z. Huang, and J. Deng. Associative embedding: End-to-end learning for joint detection and grouping. In NIPS, pages 2274–2284, 2017. 1, 5[40] A. Newell, K. Yang, and J. Deng. Stacked hourglass networks for human pose estimation. In ECCV, pages 483–499,2016. 1, 2, 3, 5, 6, 7, 9[41] X. Nie, J. Feng, J. Xing, and S. Yan. Pose partition networks for multi-person pose estimation. In ECCV, September 2018.1[42] X. Nie, J. Feng, and S. Yan. Mutual learning to adapt for joint human parsing and pose estimation. In ECCV, September. 2[43] X. Nie, J. Feng, Y. Zuo, and S. Yan. Human pose estimation with parsing induced learner. In CVPR, June 2018. 2[44] G. Ning, Z. Zhang, and Z. He. Knowledge-guided deep fractal neural networks for human pose estimation. IEEE Trans. Multimedia, 20(5):1246–1259, 2018. 6[45] W. Ouyang, X. Chu, and X. Wang. Multi-source deep learning for human pose estimation. In CVPR, pages 2337–2344, 2014. 2[46] G. Papandreou, T. Zhu, L.-C. Chen, S. Gidaris, J. Tompson, and K. Murphy. Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model. In ECCV, September 2018. 1, 5[47] G. Papandreou, T. Zhu, N. Kanazawa, A. Toshev, J. Tompson, C. Bregler, and K. Murphy. Towards accurate multiperson pose estimation in the wild. In CVPR, pages 3711– 3719, 2017. 1, 5[48] X. Peng, Z. Tang, F. Yang, R. S. Feris, and D. Metaxas. Jointly optimize data augmentation and network training: Adversarial data augmentation in human pose estimation. In CVPR, June 2018. 2[49] T. Pfister, J. Charles, and A. Zisserman. Flowing convnets for human pose estimation in videos. In ICCV, pages 1913– 1921, 2015. 1[50] L. Pishchulin, M. Andriluka, P. V. Gehler, and B. Schiele. Poselet conditioned pictorial structures. In CVPR, pages 588–595, 2013. 2[51] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In CVPR, pages 4929–4937, 2016. 3, 7[52] T. Pohlen, A. Hermans, M. Mathias, and B. Leibe. Fullresolution residual networks for semantic segmentation in street scenes. In CVPR, 2017. 3[53] PoseTrack. PoseTrack Leader Board. <span class="exturl" data-url="aHR0cHM6Ly9wb3NldHJhY2submV0L2xlYWRlcmJvYXJkLnBocA==" title="https://posetrack.net/leaderboard.php">https://posetrack.net/leaderboard.php<i class="fa fa-external-link"></i></span>. 7[54] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein, A. C. Berg, and F. Li. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015. 9[55] M. Samy, K. Amer, K. Eissa, M. Shaker, and M. ElHelw. Nunet: Deep residual wide field of view convolutional neural network for semantic segmentation. In CVPRW, June 2018.3[56] S. Saxena and J. Verbeek. Convolutional neural fabrics. In NIPS, pages 4053–4061, 2016. 3[57] T. Sekii. Pose proposal networks. In ECCV, September 2018. 1[58] K. Sun, C. Lan, J. Xing, W. Zeng, D. Liu, and J. Wang. Human pose estimation using global and local normalization. In ICCV, pages 5600–5608, 2017. 2, 6[59] K. Sun, M. Li, D. Liu, and J. Wang. IGCV3: interleaved lowrank group convolutions for efficient deep neural networks. In BMVC, page 101. BMVA Press, 2018. 3[60] X. Sun, B. Xiao, F. Wei, S. Liang, and Y. Wei. Integral human pose regression. In ECCV, pages 536–553, 2018. 5[61] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In CVPR, pages 1–9, 2015. 3[62] W. Tang, P. Yu, and Y. Wu. Deeply learned compositional models for human pose estimation. In ECCV, September 2018. 2, 6, 7, 9[63] Z. Tang, X. Peng, S. Geng, L. Wu, S. Zhang, and D. N. Metaxas. Quantized densely connected u-nets for efficient landmark localization. In ECCV, pages 348–364, 2018. 6[64] J. Tompson, R. Goroshin, A. Jain, Y. LeCun, and C. Bregler. Efficient object localization using convolutional networks. In CVPR, pages 648–656, 2015. 3[65] J. J. Tompson, A. Jain, Y. LeCun, and C. Bregler. Joint training of a convolutional network and a graphical model for human pose estimation. In NIPS, pages 1799–1807, 2014. 2[66] A. Toshev and C. Szegedy. Deeppose: Human pose estimation via deep neural networks. In CVPR, pages 1653–1660, 2014. 2[67] J. Wang, Z. Wei, T. Zhang, and W. Zeng. Deeply-fused nets. CoRR, abs&#x2F;1605.07716, 2016. 3[68] Z. Wang, W. Li, B. Yin, Q. Peng, T. Xiao, Y. Du, Z. Li, X. Zhang, G. Yu, and J. Sun. Mscoco keypoints challenge 2018. In Joint Recognition Challenge Workshop at ECCV 2018, 2018. 4[69] S. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh. Convolutional pose machines. In CVPR, pages 4724–4732, 2016. 3, 6[70] J. Wu, H. Zheng, B. Zhao, Y. Li, B. Yan, R. Liang, W. Wang, S. Zhou, G. Lin, Y. Fu, et al. Ai challenger: A largescale dataset for going deeper in image understanding. arXiv preprint arXiv:1711.06475, 2017. 6[71] F. Xia, P. Wang, X. Chen, and A. L. Yuille. Joint multiperson pose estimation and semantic part segmentation. In CVPR, pages 6080–6089, 2017. 1[72] B. Xiao, H. Wu, and Y. Wei. Simple baselines for human pose estimation and tracking. In ECCV, pages 472–487, 2018. 1, 2, 3, 5, 6, 7, 8, 9[73] G. Xie, J. Wang, T. Zhang, J. Lai, R. Hong, and G. Qi. Interleaved structured sparse convolutional neural networks. In CVPR, pages 8847–8856. IEEE Computer Society, 2018. 3[74] S. Xie and Z. Tu. Holistically-nested edge detection. In ICCV, pages 1395–1403, 2015. 3[75] Y. Xiu, J. Li, H. Wang, Y. Fang, and C. Lu. Pose flow: Efficient online pose tracking. In BMVC, page 53, 2018. 9[76] Y. Xu, T. Xiao, J. Zhang, K. Yang, and Z. Zhang. Scale-invariant convolutional neural networks. CoRR,abs&#x2F;1411.6369, 2014. 3[77] W. Yang, S. Li, W. Ouyang, H. Li, and X. Wang. Learning feature pyramids for human pose estimation. In ICCV, pages 1290–1299, 2017. 1, 2, 3, 5, 6, 7, 9[78] W. Yang, W. Ouyang, H. Li, and X. Wang. End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation. In CVPR, pages3073–3082, 2016. 2[79] Y. Yang and D. Ramanan. Articulated pose estimation with flexible mixtures-of-parts. In CVPR, pages 1385–1392, 2011. 2[80] T. Zhang, G. Qi, B. Xiao, and J. Wang. Interleaved group convolutions. In ICCV, pages 4383–4392, 2017. 3[81] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene parsing network. In CVPR, pages 6230–6239, 2017. 3[82] L. Zhao, M. Li, D. Meng, X. Li, Z. Zhang, Y. Zhuang, Z. Tu, and J. Wang. Deep convolutional neural networks with merge-and-run mappings. In IJCAI, pages 3170–3176, 2018. 3[83] Y. Zhou, X. Hu, and B. Zhang. Interlinked convolutional neural networks for face parsing. In ISNN, pages 222–231, 2015. 3[84] X. Zhu, Y. Jiang, and Z. Luo. Multi-person pose estimation for posetrack with enhanced part affinity fields. In ICCV PoseTrack Workshop, 2017.7</p>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="SindreYang 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="SindreYang 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>SindreYang
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://blog.mviai.com/2025/%E7%94%A8%E4%BA%8E%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/" title="用于人体姿态估计的深度高分辨率表征学习">http://blog.mviai.com/2025/用于人体姿态估计的深度高分辨率表征学习/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/images/wechat.png">
            <span class="icon">
              <i class="fa fa-wechat"></i>
            </span>

            <span class="label">WeChat</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91_MeshSegNet-%E7%89%99%E9%BD%BF%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C/" rel="prev" title="论文翻译_MeshSegNet-牙齿分割网络">
      <i class="fa fa-chevron-left"></i> 论文翻译_MeshSegNet-牙齿分割网络
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/KCNet-%E9%80%9A%E8%BF%87%E6%A0%B8%E7%9B%B8%E5%85%B3%E5%92%8C%E5%9B%BE%E6%B1%A0%E5%8C%96%E6%9D%A5%E6%8C%96%E6%8E%98%E7%82%B9%E4%BA%91%E7%9A%84%E5%B1%80%E9%83%A8%E7%BB%93%E6%9E%84/" rel="next" title="KCNet-通过核相关和图池化来挖掘点云的局部结构">
      KCNet-通过核相关和图池化来挖掘点云的局部结构 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="SOHUCS"></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E7%AE%80%E4%BB%8B"><span class="nav-number">2.</span> <span class="nav-text">1. 简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="nav-number">3.</span> <span class="nav-text">2. 相关工作</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-%E6%96%B9%E6%B3%95"><span class="nav-number">4.</span> <span class="nav-text">3. 方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-%E5%AE%9E%E9%AA%8C"><span class="nav-number">5.</span> <span class="nav-text">4. 实验</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-COCO%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B"><span class="nav-number">5.1.</span> <span class="nav-text">4.1  COCO关键点检测</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-MPII-%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1"><span class="nav-number">5.2.</span> <span class="nav-text">4.2 MPII 人体姿态估计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-%E5%A7%BF%E5%8A%BF%E8%B7%9F%E8%B8%AA%E5%BA%94%E7%94%A8"><span class="nav-number">5.3.</span> <span class="nav-text">4.3 姿势跟踪应用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="nav-number">5.4.</span> <span class="nav-text">4.4 消融实验</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-%E7%BB%93%E8%AE%BA%E5%92%8C%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C"><span class="nav-number">6.</span> <span class="nav-text">5. 结论和未来工作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#MPII-%E9%AA%8C%E8%AF%81%E9%9B%86%E7%9A%84%E9%99%84%E5%BD%95%E7%BB%93%E6%9E%9C"><span class="nav-number">6.1.</span> <span class="nav-text">MPII 验证集的附录结果</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E-PoseTrack-%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E6%9B%B4%E5%A4%9A%E7%BB%93%E6%9E%9C"><span class="nav-number">6.2.</span> <span class="nav-text">关于 PoseTrack 数据集的更多结果</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ImageNet-%E9%AA%8C%E8%AF%81%E9%9B%86%E7%9A%84%E7%BB%93%E6%9E%9C"><span class="nav-number">6.3.</span> <span class="nav-text">ImageNet 验证集的结果</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE%EF%BC%9A"><span class="nav-number">7.</span> <span class="nav-text">参考文献：</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SindreYang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">SindreYang</p>
  <div class="site-description" itemprop="description">沉淀后我愿意做一个温暖的人。有自己的喜好，有自己的原则，有自己的信仰，不急功近利，不浮夸轻薄，宠辱不惊，淡定安逸，心静如水。------不忘初心，方得始终</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">321</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1NpbmRyZVlhbmc=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;SindreYang"><i class="fa fa-fw fa-github"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnl4QG12aWFpLmNvbQ==" title="E-Mail → mailto:yx@mviai.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        
  <div class="languages">
    <label class="lang-select-label">
      <i class="fa fa-language"></i>
      <span>简体中文</span>
      <i class="fa fa-angle-up" aria-hidden="true"></i>
    </label>
    <select class="lang-select" data-canonical="">
      
        <option value="zh-CN" data-href="/2025/%E7%94%A8%E4%BA%8E%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/" selected="">
          简体中文
        </option>
      
        <option value="en" data-href="/en/2025/%E7%94%A8%E4%BA%8E%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/" selected="">
          English
        </option>
      
    </select>
  </div>

        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SindreYang</span>
</div><!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/love.js"></script>
<!-- 背景波浪 -->
<script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>


<!-- 腾讯企业邮箱 -->
<style>
.bizmail_loginpanel {
    font-size: 12px;
    width: 300px;
    height: auto;
    background: transparent;
    margin-left: auto;
    margin-right: auto;
}

.bizmail_LoginBox {
    padding: 10px 15px;
}


.bizmail_loginpanel form {
    margin: 0;
    padding: 0;
}

.bizmail_loginpanel input.text {
    font-size: 12px;
    width: 100px;
    height: 20px;
    margin: 0 2px;
    background-color: transparent;
    border:1px solid transparent;
    box-shadow: none;
    color: black;
}

.bizmail_loginpanel .bizmail_column {
    height: 28px;
}

.bizmail_loginpanel .bizmail_column label {
    display: block;
    float: left;
    width: 30px;
    height: 24px;
    line-height: 24px;
    font-size: 12px;
}

.bizmail_loginpanel .bizmail_column .bizmail_inputArea {
    float: left;
    width: 240px;
}

.bizmail_loginpanel .bizmail_column span {
    font-size: 12px;
    word-wrap: break-word;
    margin-left: 2px;
    line-height: 200%;
}

.bizmail_loginpanel .bizmail_SubmitArea {
    margin-left: 30px;
    clear: both;
}

.bizmail_loginpanel .bizmail_SubmitArea a {
    font-size: 12px;
    margin-left: 5px;
}

.bizmail_loginpanel select {
    width: 110px;
    height: 20px;
    margin: 0 2px;
}
.bizmail_loginpanel input {

    background-color: rgba(83, 126, 236, 0.562);
}


</style>

<script type="text/javascript">
function checkInput() {
    var e = document.form1.uin,
        i = document.form1.pwd;
    return 0 == e.value.length ? e.focus() : 0 == i.value.length ? i.focus() : (document.form1.submit(), setTimeout(" document.form1.pwd.value = '' ", 500)), !1
}

function writeLoginPanel(e) {
    if (e && e.domainlist && -1 != e.domainlist.indexOf(".")) {
        var a = "return checkInput()",
            t = '<div id="divLoginpanelHor" class="bizmail_loginpanel" style="width:550px;"><div class="bizmail_LoginBox"><form name="form1" action="https://exmail.qq.com/cgi-bin/login" target="_blank" method="post" onsubmit="' + a + '"><input type="hidden" name="firstlogin" value="false" /><input type="hidden" name="errtemplate" value="dm_loginpage" /><input type="hidden" name="aliastype" value="other" /><input type="hidden" name="dmtype" value="bizmail" /><input type="hidden" name="p" value="" /><label>\u8d26\u53f7:</label><input type="text" name="uin" class="text" value="" />@#domainlist#<label>&nbsp&nbsp&nbsp;\u5bc6\u7801:</label><input type="password" name="pwd" class="text" value="" /><input type="submit" class="" name="" value="\u767b\u5f55" />&nbsp;<a href="https://exmail.qq.com/cgi-bin/readtemplate?check=false&t=biz_rf_portal#recovery" target="_blank">\u5fd8\u8bb0\u5bc6\u7801\uff1f</a></form></div></div>',
            n = '<div id="divLoginpanelVer" class="bizmail_loginpanel"><div class="bizmail_LoginBox"><form name="form1" action="https://exmail.qq.com/cgi-bin/login" target="_blank" method="post" onsubmit="' + a + '"><input type="hidden" name="firstlogin" value="false" /><input type="hidden" name="errtemplate" value="dm_loginpage" /><input type="hidden" name="aliastype" value="other" /><input type="hidden" name="dmtype" value="bizmail" /><input type="hidden" name="p" value="" /><div class="bizmail_column"><label>\u8d26\u53f7:</label><div class="bizmail_inputArea"><input type="text" name="uin" class="text" value="" />@#domainlist#</div></div><div class="bizmail_column"><label>\u5bc6\u7801:</label><div class="bizmail_inputArea"><input type="password" name="pwd" class="text" value="" /></div></div><div class="bizmail_SubmitArea"><input type="submit" class="" name="" style="width:66px;" value="\u767b\u5f55" /><a href="https://exmail.qq.com/cgi-bin/readtemplate?check=false&t=biz_rf_portal#recovery" target="_blank">\u5fd8\u8bb0\u5bc6\u7801\uff1f</a></div></form></div></div>',
            l = e.domainlist.split(";");
        if (1 == l.length) var m = '<span>#domain#</span><input type="hidden" name="domain" value="#domain#" />'.replace(/#domain#/g, l[0]);
        else {
            m = '<select name="domain">';
            for (i = 0; i < l.length; i++) m += '<option value="' + l[i] + '">' + l[i] + "</option>";
            m += "</select>"
        }
        e.mode && "vertical" != e.mode && "both" != e.mode || document.write(n.replace(/#domainlist#/g, m)), "horizontal" != e.mode && "both" != e.mode || document.write(t.replace(/#domainlist#/g, m))
    }
}

</script>      

<script type="text/javascript"> writeLoginPanel({domainlist:"mviai.com", mode:"horizontal"});</script>      


        








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>


  




  
<script src="/js/local-search.js"></script>









<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  

  <script>
  NexT.utils.loadComments(document.querySelector('#SOHUCS'), () => {
    var appid = 'cyxmItxjS';
    var conf = 'e5e71132d9086bb54aeeba6e88e87df9';
    var width = window.innerWidth || document.documentElement.clientWidth;
    if (width < 960) {
      window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://cy-cdn.kuaizhan.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>');
    } else {
      var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:appid,conf:conf})});
    }
  });
  </script>
  <script src="https://cy-cdn.kuaizhan.com/upload/plugins/plugins.count.js"></script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"left","width":75,"height":150},"mobile":{"show":true},"log":false});</script></body>
</html>




