<!DOCTYPE html>
<html lang="zh-CN,en,default">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.mviai.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":true,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="自动摘要: 	0预备	pytorch文档官网：[https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;basics&#x2F;data_tutorial.html](https:&#x2F;&#x2F;pytor ……..">
<meta property="og:type" content="article">
<meta property="og:title" content="1">
<meta property="og:url" content="http://blog.mviai.com/2025/1._%E7%86%9F%E6%82%89pytorch_----__1_%E7%AE%80%E4%BB%8B/index.html">
<meta property="og:site_name" content="落叶无痕">
<meta property="og:description" content="自动摘要: 	0预备	pytorch文档官网：[https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;basics&#x2F;data_tutorial.html](https:&#x2F;&#x2F;pytor ……..">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://blog.mviai.com/images/1663903200094-f2162a02-6873-488b-9656-1676b682e895.png">
<meta property="og:image" content="http://blog.mviai.com/images/1663903704921-a99d8b2a-5561-4eec-8714-73b87e118fbb.png">
<meta property="og:image" content="http://blog.mviai.com/images/1663904839326-3917b1b8-9699-4aa5-97cf-ce051aeab41f.png">
<meta property="og:image" content="http://blog.mviai.com/images/1663917513718-78f251f8-80dd-40e0-a944-81f7995cf0e0.png">
<meta property="og:image" content="http://blog.mviai.com/images/1663917688593-90d27341-61a5-44bd-badd-5dae8c40b844.png">
<meta property="og:image" content="http://blog.mviai.com/images/1663927938832-6e025f0e-1290-4cac-8125-1eee158ed6c8.png">
<meta property="og:image" content="http://blog.mviai.com/images/1663928045828-42aaa406-147a-4d00-9ea7-83b3cae5eafe.png">
<meta property="og:image" content="http://blog.mviai.com/images/1663928322171-52a74b58-fe03-40fb-be2a-2da8048d96e3.png">
<meta property="og:image" content="http://blog.mviai.com/images/1663920076655-22b99832-67c2-4ca6-98e5-063a48d96239.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664167576769-63c59284-d609-4d0d-8ee3-8401ec791fab.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664167612000-826dfac0-e11a-4618-b99f-8729f374c316.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664166594725-db9fdfc7-dba4-4575-9fc8-61cd672d7ee1.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664167390409-5a92ff89-4959-4aed-84ee-2760a7b9e476.png">
<meta property="og:image" content="http://blog.mviai.com/images/1666246012986-4405cdc6-799f-4df9-8aad-0eea4210a803.png">
<meta property="og:image" content="http://blog.mviai.com/images/1666246182269-b80a9f9f-8b8f-4a5d-8114-f2c9d111ca1a.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664508490222-2f0a191a-67e2-4f47-a46b-5ea101f8c4b3.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664509552333-65437ee3-8980-4865-ad26-1898650c6da5.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664516494012-96fdc55d-674f-48e4-90e7-795017d15dc0.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664516865522-42e48b64-8e34-4cf9-b9fe-860f3ecb140e.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664517767297-0eac88df-76d0-4a94-9b8d-085bb7307ddd.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664518328048-9de8182e-574b-40e5-b5a1-247ae8f9f90a.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519221519-470ddf93-291c-493a-ae31-056aff5f71e7.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519277899-51a0f06e-bf0d-48fc-9179-55224c5afba8.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519277909-0d26bba1-a89b-4e78-a8c8-30479e27f696.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519277977-4f01b2b4-606e-4d79-9350-51ee44bcfe3f.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519277887-dded2e8a-3176-484d-be8e-8845e74570d8.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519277885-590a404e-577d-4b4b-a67f-25e4d5e03f96.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519278197-953c0942-2cd8-43c3-b91b-8423ce4ccda9.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519278214-8c63902d-8656-4705-b6fa-f914916a0e61.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519278260-d5fae855-e4c8-48f2-8727-ad99f63b94d2.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519278267-55bb3a79-b62e-445d-aeda-cb030aceb4d5.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519278301-f14fbc95-fb33-4bf1-86cf-119165b2837e.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519278496-7262ee29-3eb6-40ff-a8ae-e4c91e040df3.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519278555-3b35f18b-fc42-4dd9-963c-e0aed5ff9010.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519278509-c42e1196-2b00-4c1c-8810-9c9c763c1518.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519278577-560d6d53-756f-457b-a377-69933c39d278.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519278660-1fc8479d-e031-408c-afa3-78610ce349e6.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519278827-b7a0c098-9b15-4f70-bea7-861df56fb0df.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519278853-55cfaf77-930d-43ca-8d03-ba0d50ac1c70.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519278903-14a0288d-2a4c-4599-b7e9-e215c135ce76.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519278933-dccf2030-0e8f-4768-ac53-9b418250087b.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519278950-f4f617dd-376b-4512-a20e-c95b7ff511f8.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519279074-6137d31f-c734-4eec-a397-c77295550202.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519279116-668d0faf-f9e6-4d9d-8bcd-84174e965e81.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519279185-57d099ae-5aa6-47df-991d-8b89dca5aba1.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519279274-f7c3e437-bb1c-4e3f-9a74-2eaf3734f45b.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519279320-bb253098-8634-4c1a-a885-778cb148c419.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519279354-8bf6a458-09f0-488b-b805-553f6d17f4b1.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519279384-674869f3-b82b-410b-901f-becdde3056b0.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519279527-0a7772be-59a9-4485-8648-0bab201927da.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519279571-f294468e-5c2c-43b7-965d-65cdcb3edc53.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519279674-8acd7f58-363d-4217-8ce2-d03231aba307.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519279632-8215efed-9de7-42c6-981f-99da56008521.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519279777-dd5bd72c-cf04-4440-9216-44ee73611588.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519279821-7c7b1e2c-210c-477c-baf5-b28f357b2bc3.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519279888-1419609c-0366-49cd-be2b-f8e7ff32e0b9.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519279904-2b54d6ce-7c34-422e-a1c2-334d637a0b52.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519280046-9c1f6097-e42d-4416-8352-5186b438a78d.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519280060-2b10f080-862a-4f80-9f0a-d8bfbed53796.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519280176-2a9ee32e-096f-4d62-a9f0-87730f553518.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519535629-62d4862a-4222-4951-a283-51e937f002b4.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664519806466-4596a647-cfe3-48f6-8b1f-d1929a7a508d.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664520343079-77487b7c-39e7-408f-964f-e94202f1df41.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664521184614-1804774b-6e3e-43aa-b859-4ebd7d1315ee.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664521636164-14143440-7a23-4a84-87e1-75e50ab4946f.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664522453024-58bfaa74-4546-4b58-98d1-2ce2d2fd9518.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664522621707-c1f427bf-2ae9-4f41-9e59-82508c283e6b.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664155645002-196ca169-d256-4ac3-b3c7-701bc320a4af.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664160246563-932bb215-da7e-4b61-b96f-9608a79c9617.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664157380937-284c9da3-66fa-4a85-a883-b4a4f0cfdf9e.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664353494073-451a286a-423f-463e-bb99-c4c331f3c5f5.png">
<meta property="og:image" content="http://blog.mviai.com/images/1664358122608-2c8dbc03-91bf-4caa-8579-17219b05c365.png">
<meta property="og:image" content="http://blog.mviai.com/images/1665286572199-29ead83d-9bd2-47b5-b9b9-8ba46e6fbc1a.jpeg">
<meta property="article:published_time" content="2025-01-22T04:37:41.000Z">
<meta property="article:modified_time" content="2025-01-22T12:37:41.600Z">
<meta property="article:author" content="SindreYang">
<meta property="article:tag" content="生活">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.mviai.com/images/1663903200094-f2162a02-6873-488b-9656-1676b682e895.png">

<link rel="canonical" href="http://blog.mviai.com/2025/1._%E7%86%9F%E6%82%89pytorch_----__1_%E7%AE%80%E4%BB%8B/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>1 | 落叶无痕</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">落叶无痕</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">72</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">321</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <span class="exturl github-corner" data-url="aHR0cHM6Ly9naXRodWIuY29tL1NpbmRyZVlhbmc=" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></span>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.mviai.com/2025/1._%E7%86%9F%E6%82%89pytorch_----__1_%E7%AE%80%E4%BB%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="SindreYang">
      <meta itemprop="description" content="沉淀后我愿意做一个温暖的人。有自己的喜好，有自己的原则，有自己的信仰，不急功近利，不浮夸轻薄，宠辱不惊，淡定安逸，心静如水。------不忘初心，方得始终">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="落叶无痕">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          1
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-01-22 12:37:41 / 修改时间：20:37:41" itemprop="dateCreated datePublished" datetime="2025-01-22T12:37:41+08:00">2025-01-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%88%86%E5%89%B2%E5%9F%B9%E8%AE%AD%E8%AE%A1%E5%88%92/" itemprop="url" rel="index"><span itemprop="name">分割培训计划</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Changyan：</span>
    
    
      <a title="changyan" href="/2025/1._%E7%86%9F%E6%82%89pytorch_----__1_%E7%AE%80%E4%BB%8B/#SOHUCS" itemprop="discussionUrl">
        <span id="changyan_count_unit" class="post-comments-count hc-comment-count" data-xid="2025/1._熟悉pytorch_----__1_简介/" itemprop="commentCount"></span>
      </a>
    
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>自动摘要: 	0预备	pytorch文档官网：[<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvYmFzaWNzL2RhdGFfdHV0b3JpYWwuaHRtbF0=" title="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html]">https://pytorch.org/tutorials/beginner/basics/data_tutorial.html]<i class="fa fa-external-link"></i></span>(<span class="exturl" data-url="aHR0cHM6Ly9weXRvci8=" title="https://pytor/">https://pytor<i class="fa fa-external-link"></i></span> ……..</p>
<span id="more"></span>

<h1 id="0-预备"><a href="#0-预备" class="headerlink" title="0 预备"></a>0 预备</h1><p>pytorch文档官网：<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvYmFzaWNzL2RhdGFfdHV0b3JpYWwuaHRtbA==" title="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">https://pytorch.org/tutorials/beginner/basics/data_tutorial.html<i class="fa fa-external-link"></i></span>中文官方文档(翻译的不是pytorch最新版本)：<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLmFwYWNoZWNuLm9yZy8jL2RvY3MvMS43LzA3" title="https://pytorch.apachecn.org/#/docs/1.7/07">https://pytorch.apachecn.org/#/docs/1.7/07<i class="fa fa-external-link"></i></span></p>
<h2 id="Anaconda安装"><a href="#Anaconda安装" class="headerlink" title="Anaconda安装"></a>Anaconda安装</h2><p>Conda官方文档：<span class="exturl" data-url="aHR0cHM6Ly9kb2NzLmNvbmRhLmlvL3Byb2plY3RzL2NvbmRhL2VuL2xhdGVzdC9jb21tYW5kcy9jb25maWcuaHRtbCNDb25maWclMjBTdWJjb21tYW5kcw==" title="https://docs.conda.io/projects/conda/en/latest/commands/config.html#Config%20Subcommands">https://docs.conda.io/projects/conda/en/latest/commands/config.html#Config%20Subcommands<i class="fa fa-external-link"></i></span>Anaconda下载网址：<span class="exturl" data-url="aHR0cHM6Ly93d3cuYW5hY29uZGEuY29tL3Byb2R1Y3RzL2Rpc3RyaWJ1dGlvbg==" title="https://www.anaconda.com/products/distribution">https://www.anaconda.com/products/distribution<i class="fa fa-external-link"></i></span><img src="/images/1663903200094-f2162a02-6873-488b-9656-1676b682e895.png"></p>
<p>安装完成之后，从“开始”菜单中，单击“Anaconda Powershell Prompt”桌面应用。在命令行中输入”python”，回车进入python编辑中，退出按快捷键Ctrl+z。<img src="/images/1663903704921-a99d8b2a-5561-4eec-8714-73b87e118fbb.png"></p>
<p>配置环境变量：目的是为了在cmd中可以使用conda。打开控制面板 –&gt; 系统和安全 –&gt; 系统 –&gt; 高级设置 –&gt; 环境变量 –&gt; 分别设置上下两个Path：<img src="/images/1663904839326-3917b1b8-9699-4aa5-97cf-ce051aeab41f.png"><br>双击Path –&gt; 新建：把Anaconda3\Scripts文件路径添加C:\ProgramData\Anaconda3\Scripts –&gt; 点击确定</p>
<h3 id="conda进入base基本环境"><a href="#conda进入base基本环境" class="headerlink" title="conda进入base基本环境"></a>conda进入base基本环境</h3><p>安装了 Anaconda 之后，打开 cmd ，输入 Python 之后，提示 Python 不是内部或外部命令 ，但是在 Anaconda Powershell Prompt中可以正常运行解决方式：Win+R ，输入 cmd ，回车，输入</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda info --envs</span><br></pre></td></tr></table></figure>
<p><img src="/images/1663917513718-78f251f8-80dd-40e0-a944-81f7995cf0e0.png"><br>然后选择一个环境，输入</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">activate xxx</span><br></pre></td></tr></table></figure>
<p>例如，输入</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">activate base</span><br></pre></td></tr></table></figure>
<p>再输入python就可以用了<img src="/images/1663917688593-90d27341-61a5-44bd-badd-5dae8c40b844.png"><br>于是，cmd 中就能正常运行 Python 了。</p>
<h3 id="conda退出base环境及其常用命令"><a href="#conda退出base环境及其常用命令" class="headerlink" title="conda退出base环境及其常用命令"></a>conda退出base环境及其常用命令</h3><p>退出当前base环境</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure>

<p>常用命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#创建一个名为pytorch的环境，指定Python版本是3.9</span><br><span class="line">conda create --name pytorch python=3.9</span><br><span class="line"></span><br><span class="line">#查看当前拥有的所有的环境</span><br><span class="line">conda info -e</span><br><span class="line"></span><br><span class="line">#环境切换，切换到名为pytorch的环境</span><br><span class="line">source activate pytorch</span><br><span class="line">conda activate YourEnvs (第一个命令无效时使用)</span><br><span class="line"></span><br><span class="line">#删除一个名为pytorch的环境</span><br><span class="line">conda remove --name pytorch --all</span><br><span class="line"></span><br><span class="line">#安装python包</span><br><span class="line">conda install 包名</span><br><span class="line">conda install -n 环境名 包名</span><br><span class="line">#如果不用-n指定环境名称，则被安装在当前活跃环境</span><br><span class="line">#也可以通过-c指定通过某个channel安装</span><br><span class="line"></span><br><span class="line">#查看当前环境下已安装的包</span><br><span class="line">conda list</span><br><span class="line"></span><br><span class="line">#查看某个指定环境的已安装包</span><br><span class="line">conda list -n 环境名</span><br><span class="line"></span><br><span class="line">#查找包信息</span><br><span class="line">conda search 包名</span><br><span class="line"></span><br><span class="line">#更新package</span><br><span class="line">conda update -n 环境名 包名</span><br><span class="line"></span><br><span class="line">#删除package</span><br><span class="line">conda remove -n 环境名 包名</span><br><span class="line"></span><br><span class="line">#更新conda，保持conda最新</span><br><span class="line">conda update conda</span><br><span class="line"></span><br><span class="line">#更新anaconda</span><br><span class="line">conda update anaconda</span><br></pre></td></tr></table></figure>


<h2 id="管理员windows-powershell进入base环境"><a href="#管理员windows-powershell进入base环境" class="headerlink" title="管理员windows powershell进入base环境"></a>管理员windows powershell进入base环境</h2><p>打开管理员windows powershell，输入如下代码，回车</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda init powershell</span><br></pre></td></tr></table></figure>
<p><img src="/images/1663927938832-6e025f0e-1290-4cac-8125-1eee158ed6c8.png"><br>关闭后重新打开powershell，若在命令行最前面出现“（base）”，则成功；否则，若出现红色如下警告提示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">无法加载文件C:\XXX\WindowsPowerShell\profile.ps1，因为在此系统上禁止运行脚本</span><br></pre></td></tr></table></figure>
<p><img src="/images/1663928045828-42aaa406-147a-4d00-9ea7-83b3cae5eafe.png"><br>执行命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get-ExecutionPolicy</span><br></pre></td></tr></table></figure>
<p>若回复 Restricted，表示状态是禁止的。执行命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set-ExecutionPolicy RemoteSigned</span><br></pre></td></tr></table></figure>
<p>将出现如下几个选项，输入 Y（或A） 并回车，设置完毕。<img src="/images/1663928322171-52a74b58-fe03-40fb-be2a-2da8048d96e3.png"><br>重新打开powershell，即可看到命令行开头有“（base）”。</p>
<h2 id="取消打开管理员就进入conda-base环境的方法："><a href="#取消打开管理员就进入conda-base环境的方法：" class="headerlink" title="取消打开管理员就进入conda base环境的方法："></a>取消打开管理员就进入conda base环境的方法：</h2><p><strong>方法一：</strong>在终端cmd中修改配置：在终端输入conda config –show，会显示所有的配置信息。注意到有：<img src="/images/1663920076655-22b99832-67c2-4ca6-98e5-063a48d96239.png"><br>用conda config –set来修改此配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --set auto_activate_base false</span><br></pre></td></tr></table></figure>
<p>重启终端即可</p>
<p><strong>方法二</strong>：修改配置文件在用户路径下（一般为C:\users\username，linux的话就是&#x2F;home&#x2F;username路径）有一个名为.condarc的文件，是conda的配置信息。本机路径是C:\users\sy打开之后是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - http://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">  - http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">  - http://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">ssl_verify: true</span><br><span class="line">show_channel_urls: true</span><br></pre></td></tr></table></figure>
<p>在里面添加一句：auto_activate_base: false 保存即可。</p>
<h2 id="sudo安装"><a href="#sudo安装" class="headerlink" title="sudo安装"></a>sudo安装</h2><p>Windows系统命令行使用sudo安装，提示“不是内部命令”，解决方案：新建一个文本文件，将下面代码复制粘贴到文件中，并重命名文本文件为 sudo.vbs （注意后缀改成了 .vbs）。将 sudo.vbs 所在路径添加到环境变量 PATH 中，这样就可以在任意路径下使用 sudo 命令获取管理员权限了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">&#x27;ShellExecute 方法  </span><br><span class="line">  </span><br><span class="line">&#x27;作用: 用于运行一个程序或脚本。  </span><br><span class="line">  </span><br><span class="line">&#x27;语法  </span><br><span class="line">&#x27;      .ShellExecute &quot;application&quot;, &quot;parameters&quot;, &quot;dir&quot;, &quot;verb&quot;, window  </span><br><span class="line">&#x27;      .ShellExecute &#x27;some program.exe&#x27;, &#x27;&quot;some parameters with spaces&quot;&#x27;, , &quot;runas&quot;, 1  </span><br><span class="line">  </span><br><span class="line">&#x27;关键字  </span><br><span class="line">&#x27;   application   要运行的程序或脚本名称  </span><br><span class="line">&#x27;   parameters    运行程序或脚本所需的参数  </span><br><span class="line">&#x27;   dir           工作路径，若未指定则使用当前路径  </span><br><span class="line">&#x27;   verb          要执行的动作 (值可以是 runas/open/edit/print)  </span><br><span class="line">&#x27;                   runas 动作通常用于提升权限  </span><br><span class="line">&#x27;   window        程序或脚本执行时的窗口样式 (normal=1, hide=0, 2=Min, 3=max, 4=restore, 5=current, 7=min/inactive, 10=default)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">Set UAC = CreateObject(&quot;Shell.Application&quot;)  </span><br><span class="line">Set Shell = CreateObject(&quot;WScript.Shell&quot;)  </span><br><span class="line">If WScript.Arguments.count&lt;1 Then  </span><br><span class="line">    WScript.echo &quot;语法:  sudo &lt;command&gt; [args]&quot;  </span><br><span class="line">ElseIf WScript.Arguments.count=1 Then  </span><br><span class="line">    UAC.ShellExecute WScript.arguments(0), &quot;&quot;, &quot;&quot;, &quot;runas&quot;, 1  </span><br><span class="line">&#x27;    WScript.Sleep 1500  </span><br><span class="line">&#x27;    Dim ret  </span><br><span class="line">&#x27;    ret = Shell.Appactivate(&quot;用户账户控制&quot;)  </span><br><span class="line">&#x27;    If ret = true Then  </span><br><span class="line">&#x27;        Shell.sendkeys &quot;%y&quot;          </span><br><span class="line">&#x27;    Else  </span><br><span class="line">&#x27;        WScript.echo &quot;自动获取管理员权限失败，请手动确认。&quot;  </span><br><span class="line">&#x27;    End If  </span><br><span class="line">Else  </span><br><span class="line">    Dim ucCount  </span><br><span class="line">    Dim args  </span><br><span class="line">    args = NULL  </span><br><span class="line">    For ucCount=1 To (WScript.Arguments.count-1) Step 1  </span><br><span class="line">        args = args &amp; &quot; &quot; &amp; WScript.Arguments(ucCount)  </span><br><span class="line">    Next  </span><br><span class="line">    UAC.ShellExecute WScript.arguments(0), args, &quot;&quot;, &quot;runas&quot;, 5  </span><br><span class="line">End If </span><br></pre></td></tr></table></figure>


<h2 id="pycharm配置设置"><a href="#pycharm配置设置" class="headerlink" title="pycharm配置设置"></a>pycharm配置设置</h2><p>解析器选择的是本地base环境：<img src="/images/1664167576769-63c59284-d609-4d0d-8ee3-8401ec791fab.png"><br>点击确定：<img src="/images/1664167612000-826dfac0-e11a-4618-b99f-8729f374c316.png"><br>在项目中更改：文件–&gt;设置，找到如下图所示的地方更改：<img src="/images/1664166594725-db9fdfc7-dba4-4575-9fc8-61cd672d7ee1.png"><br>项目存放位置：<img src="/images/1664167390409-5a92ff89-4959-4aed-84ee-2760a7b9e476.png"></p>
<h2 id="pycharm中删除已有的python解析器："><a href="#pycharm中删除已有的python解析器：" class="headerlink" title="pycharm中删除已有的python解析器："></a>pycharm中删除已有的python解析器：</h2><p>文件–&gt;设置–&gt;点击找到如下绿色框中的下拉列表框<img src="/images/1666246012986-4405cdc6-799f-4df9-8aad-0eea4210a803.png"><br>点击后有全部显示，选中所要删除的环境，点击红色框中’-‘减号：<img src="/images/1666246182269-b80a9f9f-8b8f-4a5d-8114-f2c9d111ca1a.png"></p>
<h2 id="python面向对象"><a href="#python面向对象" class="headerlink" title="python面向对象"></a>python面向对象</h2><h3 id="类定义"><a href="#类定义" class="headerlink" title="类定义"></a>类定义</h3><p>语法格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class ClassName:</span><br><span class="line">    &lt;statement-1&gt;</span><br><span class="line">    .</span><br><span class="line">    .</span><br><span class="line">    .</span><br><span class="line">    &lt;statement-N&gt;</span><br></pre></td></tr></table></figure>
<p>类实例化后，可以使用其属性，实际上，创建一个类之后，可以通过类名访问其属性。</p>
<h3 id="类对象和-init"><a href="#类对象和-init" class="headerlink" title="类对象和__init__"></a>类对象和__init__</h3><p>类对象支持两种操作：属性引用和示例化；属性引用使用和python中所有的属性引用一样的标准语法：obj.name；类对象创建后，类命名空间中所有的命名都是有效属性名。所以如果类定义是这样：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line"></span><br><span class="line">class MyClass:</span><br><span class="line">    #简单创建一个实例</span><br><span class="line">    i = 12345</span><br><span class="line">		#创建一个方法</span><br><span class="line">    def f(self):</span><br><span class="line">        return &quot;hello world&quot;</span><br><span class="line"></span><br><span class="line">#实例化类</span><br><span class="line">x = MyClass()</span><br><span class="line"></span><br><span class="line">#访问类的属性和方法</span><br><span class="line">print(&quot;MyClass类的属性 i 为：&quot;,x.i)</span><br><span class="line">print(&quot;MyClass类的方法f输出为：&quot;,x.f())</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MyClass类的属性 i 为： 12345</span><br><span class="line">MyClass类的方法f输出为： hello world</span><br></pre></td></tr></table></figure>
<p>以上创建一个新的类实例并将该对象赋给局部变量x，x为空的对象。</p>
<p>类有一个名为__init__()的特殊方法（构造方法），该方法在类实例化时会自动调用，像下面这样：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self):</span><br><span class="line">    self.data = [1,2]</span><br></pre></td></tr></table></figure>

<p>类定义了__init__()方法，类的实例化操作就会自动调用__init__()。如下实例化类MyClass，对应的__init__()方法就会被调用：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = MyClass()</span><br><span class="line">print(x.data)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1, 2]</span><br></pre></td></tr></table></figure>

<p>当然，<strong>init</strong>()方法可以有参数，参数通过__init__()传递到类的实例化操作上。例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line"></span><br><span class="line">class Complex:</span><br><span class="line">    def __init__(self,realpart,imagpart):</span><br><span class="line">        self.r = realpart</span><br><span class="line">        self.i = imagpart</span><br><span class="line">x = Complex(3.0, -4.5)</span><br><span class="line">print(x.r, x.i)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3.0 -4.5</span><br></pre></td></tr></table></figure>

<p>一定要用__init__()方法吗，不一定，例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class Rectangle():</span><br><span class="line">    def getPeri(self,a,b):</span><br><span class="line">        return (a + b)*2</span><br><span class="line"></span><br><span class="line">    def getArea(self,a,b):</span><br><span class="line">        return a*b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rect = Rectangle()</span><br><span class="line">print(rect.getPeri(3,4))</span><br><span class="line">print(rect.getArea(3,4))</span><br><span class="line">print(rect.__dict__)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">14</span><br><span class="line">12</span><br><span class="line">&#123;&#125;</span><br></pre></td></tr></table></figure>
<p>从上例中可以看到，类中并没有定义init()方法，但是也能够得到类似的要求，结果返回了矩形实例rect的周长及面积。但是，我们通过print(rect.<strong>dict</strong>)来看这个实例的属性，竟然是空的，我定义了一个矩形，按理来说它的属性应该是它的长、宽，但是它竟然没有，这就是没有定义init()的原因了。并且，在实例化对象的时候，rect &#x3D; Rectangle()参数为空，没有指定a、b的值，只有在调用函数的时候才指定了。且类中定义的每个方法的参数都有a、b，这显然浪费感情，在类中直接指定方法就可以了。因此，需要在类中定义init()方法，方便创建实例的时候，需要给实例绑定上属性，也方便类中方法（函数）的定义。上面例子用采用init()方法定义类，如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class Rectangle():</span><br><span class="line">    def __init__(self,a,b):</span><br><span class="line">        self.a = a</span><br><span class="line">        self.b = b</span><br><span class="line"></span><br><span class="line">    def getPeri(self):</span><br><span class="line">        return (self.a + self.b)*2</span><br><span class="line"></span><br><span class="line">    def getArea(self):</span><br><span class="line">        return self.a * self.b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rect = Rectangle(3,4)</span><br><span class="line">print(rect.getPeri())</span><br><span class="line">print(rect.getArea())</span><br><span class="line">print(rect.__dict__)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">14</span><br><span class="line">12</span><br><span class="line">&#123;&#x27;a&#x27;: 3, &#x27;b&#x27;: 4&#125;</span><br></pre></td></tr></table></figure>


<h3 id="self代表类的实例，而非类"><a href="#self代表类的实例，而非类" class="headerlink" title="self代表类的实例，而非类"></a>self代表类的实例，而非类</h3><p>类的方法与普通的函数只有一个特别的区别—-他们必须有一个额外的第一个参数名称，按照惯例他的名称是self。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line"></span><br><span class="line">class Test:</span><br><span class="line">    def prt(self):</span><br><span class="line">        print(self)</span><br><span class="line">        print(self.__class__)</span><br><span class="line"></span><br><span class="line">t = Test()</span><br><span class="line">t.prt()</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;__main__.Test object at 0x0000024D70959FD0&gt;</span><br><span class="line">&lt;class &#x27;__main__.Test&#x27;&gt;</span><br></pre></td></tr></table></figure>
<p>从执行结果可以很明显的看出，self代表的是类的实例，代表当前对象的地址，二self.class则转向类。</p>
<p>self不是python关键字，我们把它换成x也是可以正常执行的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line"></span><br><span class="line">class Test:</span><br><span class="line">    def prt(x):</span><br><span class="line">        print(x)</span><br><span class="line">        print(x.__class__)</span><br><span class="line"></span><br><span class="line">t = Test()</span><br><span class="line">t.prt()</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;__main__.Test object at 0x000001D097EA9FD0&gt;</span><br><span class="line">&lt;class &#x27;__main__.Test&#x27;&gt;</span><br></pre></td></tr></table></figure>


<h3 id="类的方法"><a href="#类的方法" class="headerlink" title="类的方法"></a>类的方法</h3><p>在类的内部，使用def关键字来定义一个方法，与一般函数定义不同，类方法必须包含参数self，且为第一个参数，self代表的是类的实例。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line"></span><br><span class="line">#类定义</span><br><span class="line">class people:</span><br><span class="line">    #定义基本属性</span><br><span class="line">    name = &#x27;&#x27;</span><br><span class="line">    age = 0</span><br><span class="line">    #定义私有属性，私有属性在类外部无法直接进行访问</span><br><span class="line">    __weight = 0</span><br><span class="line">    #定义构造方法</span><br><span class="line">    def __init__(self,n,a,w):</span><br><span class="line">        self.name = n</span><br><span class="line">        self.age = a</span><br><span class="line">        self.__weight = w</span><br><span class="line">    def speak(self):</span><br><span class="line">        print(&quot;%s 说：我 %d 岁。&quot; %(self.name,self.age))</span><br><span class="line"></span><br><span class="line">#实例化类</span><br><span class="line">p = people(&quot;runoob&quot;,10,30)</span><br><span class="line">p.speak()</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">runoob 说：我 10 岁。</span><br></pre></td></tr></table></figure>

<p>将上面代码进行如下更改</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python3</span><br><span class="line"></span><br><span class="line">#类定义</span><br><span class="line">class people:</span><br><span class="line">    # #定义基本属性</span><br><span class="line">    # name = &#x27;&#x27;</span><br><span class="line">    # age = 0</span><br><span class="line">    # #定义私有属性，私有属性在类外部无法直接进行访问</span><br><span class="line">    # __weight = 0</span><br><span class="line">    #定义构造方法</span><br><span class="line">    def __init__(self,n,a,w):</span><br><span class="line">        self.name = n</span><br><span class="line">        self.age = a</span><br><span class="line">        self.__weight = w</span><br><span class="line">    def speak(self):</span><br><span class="line">        print(&quot;%s 说：我 %d 岁，体重 %d 斤。&quot; %(self.name,self.age,self.__weight))</span><br><span class="line"></span><br><span class="line">#实例化类</span><br><span class="line">p = people(&quot;runoob&quot;,10,60)</span><br><span class="line">p.speak()</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">runoob 说：我 10 岁，体重 60 斤。</span><br></pre></td></tr></table></figure>


<h3 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h3><p>派生类(子类)的定义如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class DerivedClassName(BaseClassName):</span><br><span class="line">    &lt;statement-1&gt;</span><br><span class="line">    .</span><br><span class="line">    .</span><br><span class="line">    .</span><br><span class="line">    &lt;statement-N&gt;</span><br></pre></td></tr></table></figure>
<p>子类（派生类DeriveaClassName）会继承父类（基类BaseClassName）的属性和方法。BaseClassName(实例中的基类名)必须与派生类定义在一个作用域内。除了类，还可以用表达式，基类定义在另一个模块中时这一点非常有用：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class DerivedClassName(modname.BaseClassName):</span><br></pre></td></tr></table></figure>

<p>实例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#定义类</span><br><span class="line">class people:</span><br><span class="line">    #定义基本属性</span><br><span class="line">    name = &#x27;&#x27;</span><br><span class="line">    age = 0</span><br><span class="line">    #定义私有属性，私有属性在类外部无法直接进行访问</span><br><span class="line">    __weight = 0</span><br><span class="line">    #定义构造方法</span><br><span class="line">    def __init__(self,n,a,w):</span><br><span class="line">        self.name = n</span><br><span class="line">        self.age = a</span><br><span class="line">        self.__weight = w</span><br><span class="line">    def speak(self):</span><br><span class="line">        print(&quot;%s 说：我 %d 岁。&quot; %(self.name,self.age))</span><br><span class="line"></span><br><span class="line">#单继承示例</span><br><span class="line">class student(people):</span><br><span class="line">    grade = &#x27;&#x27;</span><br><span class="line">    def __init__(self,n,a,w,g):</span><br><span class="line">        #调用父类的构函</span><br><span class="line">        people.__init__(people,n,a,w)</span><br><span class="line">        self.grade = g</span><br><span class="line">    #覆写父类的方法</span><br><span class="line">    def speak(self):</span><br><span class="line">        print(&quot;%s 说：我 %d 岁了，我在读 %d 年级&quot;%(self.name,self.age,self.grade))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">s = student(&#x27;ken&#x27;,10,60,3)</span><br><span class="line">s.speak()</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ken 说：我 10 岁了，我在读 3 年级</span><br></pre></td></tr></table></figure>


<h3 id="多继承"><a href="#多继承" class="headerlink" title="多继承"></a>多继承</h3><p>python同样有限的支持多继承形式。多继承的类定义如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class DerivedClassName(Base1, Base2, Base3):</span><br><span class="line">    &lt;statement-1&gt;</span><br><span class="line">    .</span><br><span class="line">    .</span><br><span class="line">    .</span><br><span class="line">    &lt;statement-N&gt;</span><br></pre></td></tr></table></figure>
<p>需要注意圆括号中父类的顺序，若是父类中有相同的方法名，而在子类使用时未指定，python从左至右搜索，即方法在子类中未找到时，从左至右查找父类中是否包含方法。</p>
<p>实例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">#定义类</span><br><span class="line">class people:</span><br><span class="line">    #定义基本属性</span><br><span class="line">    name = &#x27;&#x27;</span><br><span class="line">    age = 0</span><br><span class="line">    #定义私有属性，私有属性在类外部无法直接进行访问</span><br><span class="line">    __weight = 0</span><br><span class="line">    #定义构造方法</span><br><span class="line">    def __init__(self,n,a,w):</span><br><span class="line">        self.name = n</span><br><span class="line">        self.age = a</span><br><span class="line">        self.__weight = w</span><br><span class="line">    def speak(self):</span><br><span class="line">        print(&quot;%s 说：我 %d 岁，体重 %d 斤。&quot; %(self.name,self.age,self.__weight))</span><br><span class="line"></span><br><span class="line">#单继承示例</span><br><span class="line">class student(people):</span><br><span class="line">    grade = &#x27;&#x27;</span><br><span class="line">    def __init__(self,n,a,w,g):</span><br><span class="line">        #调用父类的构函</span><br><span class="line">        people.__init__(people,n,a,w)</span><br><span class="line">        self.grade = g</span><br><span class="line">    #覆写父类的方法</span><br><span class="line">    def speak(self):</span><br><span class="line">        print(&quot;%s 说：我 %d 岁了，我在读 %d 年级&quot;%(self.name,self.age,self.grade))</span><br><span class="line"></span><br><span class="line">#另一个类，多重继承之前的准备</span><br><span class="line">class speaker():</span><br><span class="line">    topic = &#x27;&#x27;</span><br><span class="line">    name = &#x27;&#x27;</span><br><span class="line">    def __init__(self,n,t):</span><br><span class="line">        self.name = n</span><br><span class="line">        self.topic = t</span><br><span class="line">    def speak(self):</span><br><span class="line">        print(&quot;我叫 %s ，我是一个演说家，我演讲的主题是 %s&quot;%(self.name,self.topic))</span><br><span class="line"></span><br><span class="line">#多重继承</span><br><span class="line">class sample(speaker,student):</span><br><span class="line">    a = &#x27;&#x27;</span><br><span class="line">    def __init__(self,n,a,w,g,t):</span><br><span class="line">        student.__init__(self,n,a,w,g)</span><br><span class="line">        speaker.__init__(self,n,t)</span><br><span class="line"></span><br><span class="line">test = sample(&quot;Tim&quot;,25,80,4,&quot;Python&quot;)</span><br><span class="line">test.speak()    #方法名相同，默认调用的是在括号中参数位置排前父类的方法</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我叫 Tim ，我是一个演说家，我演讲的主题是 Python</span><br></pre></td></tr></table></figure>


<h3 id="方法重写"><a href="#方法重写" class="headerlink" title="方法重写"></a>方法重写</h3><p>如果父类方法不能满足你的需求，可以在子类重写父类的方法，实例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#定义父类</span><br><span class="line">class Parent:</span><br><span class="line">    def myMedthod(self):</span><br><span class="line">        print(&#x27;调用父类方法&#x27;)</span><br><span class="line"></span><br><span class="line">#定义子类</span><br><span class="line">class Child(Parent):</span><br><span class="line">    def myMedthod(self):</span><br><span class="line">        print(&#x27;调用子类方法&#x27;)</span><br><span class="line"></span><br><span class="line">#子类实例</span><br><span class="line">c = Child()</span><br><span class="line">#子类调用重写方法</span><br><span class="line">c.myMedthod()</span><br><span class="line">#用子类对象调用父类已被覆盖的方法</span><br><span class="line">super(Child,c).myMedthod()    #super()函数是用于调用父类（超类）的一个方法</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">调用子类方法</span><br><span class="line">调用父类方法</span><br></pre></td></tr></table></figure>


<h3 id="类属性与方法"><a href="#类属性与方法" class="headerlink" title="类属性与方法"></a>类属性与方法</h3><p>私有属性：__private_attrs：两个下划线开头，声明该属性为私有，不能在类的外部被使用或直接访问。类内部的方法中使用时self.__private_attrs。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class JustCounter:</span><br><span class="line">    __secretCount = 0   #私有变量</span><br><span class="line">    publicCount = 0    #公开变量</span><br><span class="line"></span><br><span class="line">    def count(self):</span><br><span class="line">        self.__secretCount += 1</span><br><span class="line">        self.publicCount += 1</span><br><span class="line">        print(self.__secretCount)</span><br><span class="line"></span><br><span class="line">counter = JustCounter()</span><br><span class="line">counter.count()</span><br><span class="line">counter.count()</span><br><span class="line">print(counter.publicCount)</span><br><span class="line">print(counter.__secretCount)    #报错，实例不能访问私有变量</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">2</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;D:\pytorch_demo\re_demo.py&quot;, line 183, in &lt;module&gt;</span><br><span class="line">    print(counter.__secretCount)    #报错，实例不能访问私有变量</span><br><span class="line">AttributeError: &#x27;JustCounter&#x27; object has no attribute &#x27;__secretCount&#x27;</span><br></pre></td></tr></table></figure>

<p>类的方法：在类的内部，使用def关键字来定义一个方法，与一般函数定义不同，类方法必须包含参数self，且为第一个参数，self代表的是类的实例。self的名字并不是规定死的，也可以用this，但是最好还是按照约定使用self.</p>
<p>类的私有方法：__private_method：两个下划线开头，声明该方法为私有方法，只能在类的内部调用，不能在类的外部调用。self.__private_method。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">class Site:</span><br><span class="line">    def __init__(self,name,url):</span><br><span class="line">        self.name = name    #public</span><br><span class="line">        self.__url = url    #private</span><br><span class="line"></span><br><span class="line">    def who(self):</span><br><span class="line">        print(&#x27;name: &#x27;,self.name)</span><br><span class="line">        print(&#x27;url: &#x27;,self.__url)</span><br><span class="line"></span><br><span class="line">    def __foo(self):    #私有方法</span><br><span class="line">        print(&#x27;这是私有方法&#x27;)</span><br><span class="line"></span><br><span class="line">    def foo(self):    #公有方法</span><br><span class="line">        print(&#x27;这是公有方法&#x27;)</span><br><span class="line">        self.__foo()</span><br><span class="line"></span><br><span class="line">x = Site(&#x27;菜鸟教程&#x27;,&#x27;www.runoob.com&#x27;)</span><br><span class="line">x.who()      #正常输出</span><br><span class="line">x.foo()      #正常输出</span><br><span class="line">x.__foo()    #报错，外部不能调用私有方法</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">name:  菜鸟教程</span><br><span class="line">url:  www.runoob.com</span><br><span class="line">这是公有方法</span><br><span class="line">这是私有方法</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;D:\pytorch_demo\re_demo.py&quot;, line 207, in &lt;module&gt;</span><br><span class="line">    x.__foo()    #报错，外部不能调用私有方法</span><br><span class="line">AttributeError: &#x27;Site&#x27; object has no attribute &#x27;__foo&#x27;</span><br></pre></td></tr></table></figure>


<h3 id="类的专有方法："><a href="#类的专有方法：" class="headerlink" title="类的专有方法："></a>类的专有方法：</h3><ul>
<li><strong><strong>init</strong> :</strong> 构造函数，在生成对象时调用</li>
<li><strong><strong>del</strong> :</strong> 析构函数，释放对象时使用</li>
<li><strong><strong>repr</strong> :</strong> 打印，转换</li>
<li><strong><strong>setitem</strong> :</strong> 按照索引赋值</li>
<li><strong><strong>getitem</strong>:</strong> 按照索引获取值</li>
<li><strong><strong>len</strong>:</strong> 获得长度</li>
<li><strong><strong>cmp</strong>:</strong> 比较运算</li>
<li><strong><strong>call</strong>:</strong> 函数调用</li>
<li><strong><strong>add</strong>:</strong> 加运算</li>
<li><strong><strong>sub</strong>:</strong> 减运算</li>
<li><strong><strong>mul</strong>:</strong> 乘运算</li>
<li><strong><strong>truediv</strong>:</strong> 除运算</li>
<li><strong><strong>mod</strong>:</strong> 求余运算</li>
<li><strong><strong>pow</strong>:</strong> 乘方</li>
</ul>
<h3 id="运算符重载"><a href="#运算符重载" class="headerlink" title="运算符重载"></a>运算符重载</h3><p>python同样支持运算符重载，可以对类的专有方法进行重载，实例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class Vector:</span><br><span class="line">    def __init__(self,a,b):</span><br><span class="line">        self.a = a</span><br><span class="line">        self.b = b</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">				#返回一个对象的描述信息</span><br><span class="line">        return &#x27;Vector (%d,%d)&#x27; % (self.a ,self.b)</span><br><span class="line"></span><br><span class="line">    def __add__(self, other):</span><br><span class="line">        return Vector(self.a + other.a, self.b + other.b)</span><br><span class="line"></span><br><span class="line">v1 = Vector(2,10)</span><br><span class="line">v2 = Vector(5,-2)</span><br><span class="line">print(v1 + v2)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Vector (7,8)</span><br></pre></td></tr></table></figure>


<h2 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h2><p>官网：<span class="exturl" data-url="aHR0cHM6Ly9tYXRwbG90bGliLm9yZy8=" title="https://matplotlib.org/">https://matplotlib.org/<i class="fa fa-external-link"></i></span></p>
<h3 id="matplotlib安装"><a href="#matplotlib安装" class="headerlink" title="matplotlib安装"></a>matplotlib安装</h3><p>一般安装Anaconda就会安装matplotlib，如果未安装好就用下面命令之一即可：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda install matplotlib</span><br><span class="line">pip install matplotlib</span><br></pre></td></tr></table></figure>

<p>安装完成后查看是否安装成功，输出版本号：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib</span><br><span class="line">print(matplotlib.__version__)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3.5.1</span><br></pre></td></tr></table></figure>


<h3 id="Matplotlib-Pyplot"><a href="#Matplotlib-Pyplot" class="headerlink" title="Matplotlib Pyplot"></a>Matplotlib Pyplot</h3><p>官网：<span class="exturl" data-url="aHR0cHM6Ly9tYXRwbG90bGliLm9yZy9zdGFibGUvYXBpL19hc19nZW4vbWF0cGxvdGxpYi5weXBsb3QucGxvdC5odG1s" title="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html">https://matplotlib.org/stable/api/_as_gen&#x2F;matplotlib.pyplot.plot.html<i class="fa fa-external-link"></i></span></p>
<p>pyplot是matplotlib的字库，是常用的绘图模块，能很方便让用户绘制2D图标，提供相应的API。用法：使用import导入pyplot库，并设置一个别名plt。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br></pre></td></tr></table></figure>

<p>实例：通过两点坐标（0，0）到（0，100）绘制一条线：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">xpoints = np.array([0,6])</span><br><span class="line">ypoints = np.array([0,100])</span><br><span class="line"></span><br><span class="line">plt.plot(xpoints,ypoints)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>输出结果：<img src="/images/1664508490222-2f0a191a-67e2-4f47-a46b-5ea101f8c4b3.png"><br>此实例中使用了Pyplot中绘制二维图形的最基本函数plot()。</p>
<p>plot()画图可以绘制点和线，语法格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 画单条线</span><br><span class="line">plot([x], y, [fmt], *, data=None, **kwargs)</span><br><span class="line"># 画多条线</span><br><span class="line">plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)</span><br></pre></td></tr></table></figure>
<p>参数说明：x,y：点或线的节点，x为x轴数据，y为y轴数据，数据可以列表或数组；fmt：可选，定义基本格式（如颜色、标记和线条样式）；**kwargs：可选，用在二维平面图上，设置指定属性，如标签、线的宽度等。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plot(x, y)        # 创建 y 中数据与 x 中对应值的二维线图，使用默认样式</span><br><span class="line">plot(x, y, &#x27;bo&#x27;)  # 创建 y 中数据与 x 中对应值的二维线图，使用蓝色实心圈绘制</span><br><span class="line">plot(y)           # x 的值为 0..N-1</span><br><span class="line">plot(y, &#x27;r+&#x27;)     # 使用红色 + 号</span><br></pre></td></tr></table></figure>
<p>颜色字符：’b’ 蓝色，’m’ 洋红色，’g’ 绿色，’y’ 黄色，’r’ 红色，’k’ 黑色，’w’ 白色，’c’ 青绿色，’#008000’ RGB 颜色符串。多条曲线不指定颜色时，会自动选择不同颜色。线形参数：’‐’ 实线，’‐‐’ 破折线，’‐.’ 点划线，’:’ 虚线。标记字符：’.’ 点标记，’,’ 像素标记(极小点)，’o’ 实心圈标记，’v’ 倒三角标记，’^’ 上三角标记，’&gt;’ 右三角标记，’&lt;’ 左三角标记…等等。</p>
<p>实例：绘制坐标(1,3)到(8,10)的线，我们就需要传递两个数据组[1, 8]和[3, 10]给plot函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">xpoints = np.array([1,8])</span><br><span class="line">ypoints = np.array([3,10])</span><br><span class="line"></span><br><span class="line">plt.plot(xpoints,ypoints)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>输出结果：<img src="/images/1664509552333-65437ee3-8980-4865-ad26-1898650c6da5.png"></p>
<p>如果只想绘制两个坐标点，而不是一条线，可以使用 o 参数，表示一个实心圈的标记：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">xpoints = np.array([1,8])</span><br><span class="line">ypoints = np.array([3,10])</span><br><span class="line"></span><br><span class="line">plt.plot(xpoints,ypoints,&#x27;o&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>输出结果：<img src="/images/1664516494012-96fdc55d-674f-48e4-90e7-795017d15dc0.png"></p>
<p>也可以绘制任意数量的点，只需确保两个轴上的点数相同即可。绘制一条不规则线，坐标为(1,3)、(2,8)、(6,1)、(8,10)，对应的两个数组为：[1, 2, 6, 8] 与 [3, 8, 1, 10]。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">xpoints = np.array([1, 2, 6, 8])</span><br><span class="line">ypoints = np.array([3, 8, 1, 10])</span><br><span class="line"></span><br><span class="line">plt.plot(xpoints,ypoints)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>输出结果：<img src="/images/1664516865522-42e48b64-8e34-4cf9-b9fe-860f3ecb140e.png"></p>
<p>若不指定x轴上的点，则x会根据y的值来设置为0，1，2，3 … N-1。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([3, 10])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>输出结果：<img src="/images/1664517767297-0eac88df-76d0-4a94-9b8d-085bb7307ddd.png"></p>
<p>绘制正弦和余弦图，在plt.plot()参数中包含两对x,y值，第一对是x,y，这对应于正弦函数；第二队是x,z，这对应于余弦函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.arange(0,4*np.pi,0.1)    # start,stop,step</span><br><span class="line">y = np.sin(x)</span><br><span class="line">z = np.cos(x)</span><br><span class="line">plt.plot(x,y,x,z)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>输出结果：<img src="/images/1664518328048-9de8182e-574b-40e5-b5a1-247ae8f9f90a.png"></p>
<h3 id="Matplotlib绘图标记"><a href="#Matplotlib绘图标记" class="headerlink" title="Matplotlib绘图标记"></a>Matplotlib绘图标记</h3><p>绘图过程如果我们想要给坐标自定义一些不一样的标记，就可以使用plot()方法的marker参数来定义。</p>
<p>实例：实心圆标记</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([1,3,4,5,8,9,6,1,3,4,5,2,4])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints,marker = &#x27;o&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>输出结果：<img src="/images/1664519221519-470ddf93-291c-493a-ae31-056aff5f71e7.png"></p>
<p>marker可以定义的符号如下：</p>
<table>
<thead>
<tr>
<th>标记</th>
<th>符号</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>“.”</td>
<td><img src="/images/1664519277899-51a0f06e-bf0d-48fc-9179-55224c5afba8.png"></td>
<td></td>
</tr>
<tr>
<td>点</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“,”</td>
<td><img src="/images/1664519277909-0d26bba1-a89b-4e78-a8c8-30479e27f696.png"></td>
<td></td>
</tr>
<tr>
<td>像素点</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“o”</td>
<td><img src="/images/1664519277977-4f01b2b4-606e-4d79-9350-51ee44bcfe3f.png"></td>
<td></td>
</tr>
<tr>
<td>实心圆</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“v”</td>
<td><img src="/images/1664519277887-dded2e8a-3176-484d-be8e-8845e74570d8.png"></td>
<td></td>
</tr>
<tr>
<td>下三角</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“^”</td>
<td><img src="/images/1664519277885-590a404e-577d-4b4b-a67f-25e4d5e03f96.png"></td>
<td></td>
</tr>
<tr>
<td>上三角</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“&lt;”</td>
<td><img src="/images/1664519278197-953c0942-2cd8-43c3-b91b-8423ce4ccda9.png"></td>
<td></td>
</tr>
<tr>
<td>左三角</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“&gt;”</td>
<td><img src="/images/1664519278214-8c63902d-8656-4705-b6fa-f914916a0e61.png"></td>
<td></td>
</tr>
<tr>
<td>右三角</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“1”</td>
<td><img src="/images/1664519278260-d5fae855-e4c8-48f2-8727-ad99f63b94d2.png"></td>
<td></td>
</tr>
<tr>
<td>下三叉</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“2”</td>
<td><img src="/images/1664519278267-55bb3a79-b62e-445d-aeda-cb030aceb4d5.png"></td>
<td></td>
</tr>
<tr>
<td>上三叉</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“3”</td>
<td><img src="/images/1664519278301-f14fbc95-fb33-4bf1-86cf-119165b2837e.png"></td>
<td></td>
</tr>
<tr>
<td>左三叉</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“4”</td>
<td><img src="/images/1664519278496-7262ee29-3eb6-40ff-a8ae-e4c91e040df3.png"></td>
<td></td>
</tr>
<tr>
<td>右三叉</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“8”</td>
<td><img src="/images/1664519278555-3b35f18b-fc42-4dd9-963c-e0aed5ff9010.png"></td>
<td></td>
</tr>
<tr>
<td>八角形</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“s”</td>
<td><img src="/images/1664519278509-c42e1196-2b00-4c1c-8810-9c9c763c1518.png"></td>
<td></td>
</tr>
<tr>
<td>正方形</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“p”</td>
<td><img src="/images/1664519278577-560d6d53-756f-457b-a377-69933c39d278.png"></td>
<td></td>
</tr>
<tr>
<td>五边形</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“P”</td>
<td><img src="/images/1664519278660-1fc8479d-e031-408c-afa3-78610ce349e6.png"></td>
<td></td>
</tr>
<tr>
<td>加号（填充）</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“*”</td>
<td><img src="/images/1664519278827-b7a0c098-9b15-4f70-bea7-861df56fb0df.png"></td>
<td></td>
</tr>
<tr>
<td>星号</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“h”</td>
<td><img src="/images/1664519278853-55cfaf77-930d-43ca-8d03-ba0d50ac1c70.png"></td>
<td></td>
</tr>
<tr>
<td>六边形 1</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“H”</td>
<td><img src="/images/1664519278903-14a0288d-2a4c-4599-b7e9-e215c135ce76.png"></td>
<td></td>
</tr>
<tr>
<td>六边形 2</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“+”</td>
<td><img src="/images/1664519278933-dccf2030-0e8f-4768-ac53-9b418250087b.png"></td>
<td></td>
</tr>
<tr>
<td>加号</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“x”</td>
<td><img src="/images/1664519278950-f4f617dd-376b-4512-a20e-c95b7ff511f8.png"></td>
<td></td>
</tr>
<tr>
<td>乘号 x</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“X”</td>
<td><img src="/images/1664519279074-6137d31f-c734-4eec-a397-c77295550202.png"></td>
<td></td>
</tr>
<tr>
<td>乘号 x (填充)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“D”</td>
<td><img src="/images/1664519279116-668d0faf-f9e6-4d9d-8bcd-84174e965e81.png"></td>
<td></td>
</tr>
<tr>
<td>菱形</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“d”</td>
<td><img src="/images/1664519279185-57d099ae-5aa6-47df-991d-8b89dca5aba1.png"></td>
<td></td>
</tr>
<tr>
<td>瘦菱形</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“&#124;”</td>
<td><img src="/images/1664519279274-f7c3e437-bb1c-4e3f-9a74-2eaf3734f45b.png"></td>
<td></td>
</tr>
<tr>
<td>竖线</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“_”</td>
<td><img src="/images/1664519279320-bb253098-8634-4c1a-a885-778cb148c419.png"></td>
<td></td>
</tr>
<tr>
<td>横线</td>
<td></td>
<td></td>
</tr>
<tr>
<td>0 (TICKLEFT)</td>
<td><img src="/images/1664519279354-8bf6a458-09f0-488b-b805-553f6d17f4b1.png"></td>
<td></td>
</tr>
<tr>
<td>左横线</td>
<td></td>
<td></td>
</tr>
<tr>
<td>1 (TICKRIGHT)</td>
<td><img src="/images/1664519279384-674869f3-b82b-410b-901f-becdde3056b0.png"></td>
<td></td>
</tr>
<tr>
<td>右横线</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2 (TICKUP)</td>
<td><img src="/images/1664519279527-0a7772be-59a9-4485-8648-0bab201927da.png"></td>
<td></td>
</tr>
<tr>
<td>上竖线</td>
<td></td>
<td></td>
</tr>
<tr>
<td>3 (TICKDOWN)</td>
<td><img src="/images/1664519279571-f294468e-5c2c-43b7-965d-65cdcb3edc53.png"></td>
<td></td>
</tr>
<tr>
<td>下竖线</td>
<td></td>
<td></td>
</tr>
<tr>
<td>4 (CARETLEFT)</td>
<td><img src="/images/1664519279674-8acd7f58-363d-4217-8ce2-d03231aba307.png"></td>
<td></td>
</tr>
<tr>
<td>左箭头</td>
<td></td>
<td></td>
</tr>
<tr>
<td>5 (CARETRIGHT)</td>
<td><img src="/images/1664519279632-8215efed-9de7-42c6-981f-99da56008521.png"></td>
<td></td>
</tr>
<tr>
<td>右箭头</td>
<td></td>
<td></td>
</tr>
<tr>
<td>6 (CARETUP)</td>
<td><img src="/images/1664519279777-dd5bd72c-cf04-4440-9216-44ee73611588.png"></td>
<td></td>
</tr>
<tr>
<td>上箭头</td>
<td></td>
<td></td>
</tr>
<tr>
<td>7 (CARETDOWN)</td>
<td><img src="/images/1664519279821-7c7b1e2c-210c-477c-baf5-b28f357b2bc3.png"></td>
<td></td>
</tr>
<tr>
<td>下箭头</td>
<td></td>
<td></td>
</tr>
<tr>
<td>8 (CARETLEFTBASE)</td>
<td><img src="/images/1664519279888-1419609c-0366-49cd-be2b-f8e7ff32e0b9.png"></td>
<td></td>
</tr>
<tr>
<td>左箭头 (中间点为基准)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>9 (CARETRIGHTBASE)</td>
<td><img src="/images/1664519279904-2b54d6ce-7c34-422e-a1c2-334d637a0b52.png"></td>
<td></td>
</tr>
<tr>
<td>右箭头 (中间点为基准)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>10 (CARETUPBASE)</td>
<td><img src="/images/1664519280046-9c1f6097-e42d-4416-8352-5186b438a78d.png"></td>
<td></td>
</tr>
<tr>
<td>上箭头 (中间点为基准)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>11 (CARETDOWNBASE)</td>
<td><img src="/images/1664519280060-2b10f080-862a-4f80-9f0a-d8bfbed53796.png"></td>
<td></td>
</tr>
<tr>
<td>下箭头 (中间点为基准)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>“None”, “ “ or “”</td>
<td></td>
<td>没有任何标记</td>
</tr>
<tr>
<td>‘$…$’</td>
<td><img src="/images/1664519280176-2a9ee32e-096f-4d62-a9f0-87730f553518.png"></td>
<td></td>
</tr>
<tr>
<td>渲染指定的字符。例如 “$f$” 以字母 f 为标记。</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>实例：*标记</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([1,3,4,5,8,9,6,1,3,4,5,2,4])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints,marker = &#x27;*&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>输出结果：<img src="/images/1664519535629-62d4862a-4222-4951-a283-51e937f002b4.png"></p>
<p>实例：箭头标记</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import matplotlib.markers</span><br><span class="line">plt.plot([1,2,3],marker = matplotlib.markers.CARETDOWNBASE)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>结果输出：<img src="/images/1664519806466-4596a647-cfe3-48f6-8b1f-d1929a7a508d.png"></p>
<h4 id="fmt参数"><a href="#fmt参数" class="headerlink" title="fmt参数"></a>fmt参数</h4><p>fmt参数定义了基本格式，如标记、线条样式和颜色。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fmt = &#x27;[marker][line][color]&#x27;</span><br></pre></td></tr></table></figure>

<p>实例：o:r，o表示实心圆标记，：表示虚线，r表示颜色为红色。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([6,2,13,10])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints,&#x27;o:r&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>输出结果：<img src="/images/1664520343079-77487b7c-39e7-408f-964f-e94202f1df41.png"></p>
<p>线类型：</p>
<table>
<thead>
<tr>
<th>线类型标记</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>‘-‘</td>
<td>实线</td>
</tr>
<tr>
<td>‘:’</td>
<td>虚线</td>
</tr>
<tr>
<td>‘–’</td>
<td>破折线</td>
</tr>
<tr>
<td>‘-.’</td>
<td>点划线</td>
</tr>
</tbody></table>
<p>颜色类型：</p>
<table>
<thead>
<tr>
<th>颜色标记</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>‘r’</td>
<td>红色</td>
</tr>
<tr>
<td>‘g’</td>
<td>绿色</td>
</tr>
<tr>
<td>‘b’</td>
<td>蓝色</td>
</tr>
<tr>
<td>‘c’</td>
<td>青色</td>
</tr>
<tr>
<td>‘m’</td>
<td>品红</td>
</tr>
<tr>
<td>‘y’</td>
<td>黄色</td>
</tr>
<tr>
<td>‘k’</td>
<td>黑色</td>
</tr>
<tr>
<td>‘w’</td>
<td>白色</td>
</tr>
</tbody></table>
<p>标记大小与颜色：自定义标记的大小与颜色，使用的参数分别是：</p>
<ul>
<li>markersize，简写为ms：定义标记的大小；</li>
<li>markerfacecolor，简写为mfc：定义标记内部的颜色；</li>
<li>maekerdgecolor，简写为mec：定义标记边框的颜色。</li>
</ul>
<p>实例：设置标记大小ms&#x3D;20、标记外边框颜色mec&#x3D;’r’、标记内部颜色mfc&#x3D;’y’</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([6,2,13,10])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints,marker = &#x27;o&#x27;,ms = 20, mec = &#x27;r&#x27;, mfc = &#x27;y&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>结果如下：<img src="/images/1664521184614-1804774b-6e3e-43aa-b859-4ebd7d1315ee.png"></p>
<p>自定义标记内部与边框的颜色：SeaGreen、#8FBC8F 等，完整样式可以参考 <span class="exturl" data-url="aHR0cHM6Ly93d3cucnVub29iLmNvbS9odG1sL2h0bWwtY29sb3J2YWx1ZXMuaHRtbA==" title="https://www.runoob.com/html/html-colorvalues.html">HTML 颜色值<i class="fa fa-external-link"></i></span>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([6,2,13,10])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints,marker = &#x27;o&#x27;,ms = 20, mec = &#x27;#4CAF50&#x27;, mfc = &#x27;#4CAF50&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>输出结果：<img src="/images/1664521636164-14143440-7a23-4a84-87e1-75e50ab4946f.png"></p>
<h3 id="Matplotlib绘图线"><a href="#Matplotlib绘图线" class="headerlink" title="Matplotlib绘图线"></a>Matplotlib绘图线</h3><p>绘图过程需要自定义线的样式，包括线的类型、颜色和大小等。</p>
<h4 id="线的类型"><a href="#线的类型" class="headerlink" title="线的类型"></a>线的类型</h4><p>线的类型可以使用linestyle参数来定义，简写为ls。</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>简写</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>‘solid’ (默认)</td>
<td>‘-‘</td>
<td>实线</td>
</tr>
<tr>
<td>‘dotted’</td>
<td>‘:’</td>
<td>点虚线</td>
</tr>
<tr>
<td>‘dashed’</td>
<td>‘–’</td>
<td>破折线</td>
</tr>
<tr>
<td>‘dashdot’</td>
<td>‘-.’</td>
<td>点划线</td>
</tr>
<tr>
<td>‘None’</td>
<td>‘’ 或 ‘ ‘</td>
<td>不画线</td>
</tr>
</tbody></table>
<p>实例：线类型全写linestyle &#x3D; ‘dotted’</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([6,2,13,10])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints,linestyle = &#x27;dotted&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>输出结果：<img src="/images/1664522453024-58bfaa74-4546-4b58-98d1-2ce2d2fd9518.png"></p>
<p>实例：线类型简写ls &#x3D; ‘-.’</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">ypoints = np.array([6,2,13,10])</span><br><span class="line"></span><br><span class="line">plt.plot(ypoints,ls = &#x27;-.&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>输出结果：<img src="/images/1664522621707-c1f427bf-2ae9-4f41-9e59-82508c283e6b.png"></p>
<h4 id="线的颜色"><a href="#线的颜色" class="headerlink" title="线的颜色"></a>线的颜色</h4><p>线的颜色可以使用color参数来定义，简写为c。颜色类型：</p>
<table>
<thead>
<tr>
<th>颜色标记</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>‘r’</td>
<td>红色</td>
</tr>
<tr>
<td>‘g’</td>
<td>绿色</td>
</tr>
<tr>
<td>‘b’</td>
<td>蓝色</td>
</tr>
<tr>
<td>‘c’</td>
<td>青色</td>
</tr>
<tr>
<td>‘m’</td>
<td>品红</td>
</tr>
<tr>
<td>‘y’</td>
<td>黄色</td>
</tr>
<tr>
<td>‘k’</td>
<td>黑色</td>
</tr>
<tr>
<td>‘w’</td>
<td>白色</td>
</tr>
</tbody></table>
<p>当然也可以自定义颜色类型，例如：SeaGreen、#8FBC8F 等，完整样式可以参考 <span class="exturl" data-url="aHR0cHM6Ly93d3cucnVub29iLmNvbS9odG1sL2h0bWwtY29sb3J2YWx1ZXMuaHRtbA==" title="https://www.runoob.com/html/html-colorvalues.html">HTML 颜色值<i class="fa fa-external-link"></i></span>。代码格式：如红色的线color &#x3D; ‘r’</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(ypoints,ls = &#x27;-.&#x27;,color = &#x27;r&#x27;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(ypoints, c = &#x27;#8FBC8F&#x27;)</span><br></pre></td></tr></table></figure>


<h2 id="安装pytorch"><a href="#安装pytorch" class="headerlink" title="安装pytorch"></a>安装pytorch</h2><h3 id="检查是否有合适的GPU"><a href="#检查是否有合适的GPU" class="headerlink" title="检查是否有合适的GPU"></a>检查是否有合适的GPU</h3><p>在桌面上右击如果能找到NVIDA控制面板，则说明该电脑有GPU。本机显然没有：<img src="/images/1664155645002-196ca169-d256-4ac3-b3c7-701bc320a4af.png"></p>
<h3 id="安装pytorch-1"><a href="#安装pytorch-1" class="headerlink" title="安装pytorch"></a>安装pytorch</h3><p>官网：<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy8=" title="https://pytorch.org/">https://pytorch.org/<i class="fa fa-external-link"></i></span>，安装教程在首页的Install&gt;<img src="/images/1664160246563-932bb215-da7e-4b61-b96f-9608a79c9617.png"><br>，进入后会有详细的介绍：<img src="/images/1664157380937-284c9da3-66fa-4a85-a883-b4a4f0cfdf9e.png"><br>由于本机没有GPU，所以选择上图蓝色框内的配置，即在命令行进入指定的环境base（可以新建个环境）内，输入绿色框内命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch</span><br></pre></td></tr></table></figure>
<p>安装的过程会比较慢。安装完成后进入python，输入以下命令，如果没有报错证明安装成功：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(base) PS C:\Users\sy&gt; python</span><br><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; x = torch.rand(5, 3)</span><br><span class="line">&gt;&gt;&gt; print(x)</span><br><span class="line">tensor([[0.5715, 0.8384, 0.3018],</span><br><span class="line">        [0.6275, 0.1263, 0.2403],</span><br><span class="line">        [0.3079, 0.2989, 0.6742],</span><br><span class="line">        [0.9151, 0.4709, 0.1775],</span><br><span class="line">        [0.9304, 0.6655, 0.2846]])</span><br></pre></td></tr></table></figure>

<p>也可以在pycharm里面装，之后执行上面代码不报错即可。</p>
<h1 id="1-pytorch简介"><a href="#1-pytorch简介" class="headerlink" title="1 pytorch简介"></a>1 pytorch简介</h1><h2 id="1-1-张量"><a href="#1-1-张量" class="headerlink" title="1.1 张量"></a>1.1 张量</h2><p>张量是一种特殊的数据结构，与数组和矩阵非常相似。pytorch用张量来编码模型的输入、输出及其模型的参数。</p>
<p>张量与<span class="exturl" data-url="aHR0cHM6Ly9udW1weS5vcmcv" title="https://numpy.org/">NumPy<i class="fa fa-external-link"></i></span>的ndarrays类似，只是张量可以在GPU或其他硬件加速器上运行。实际上，张量和Numpy数组通常可以共享相同的底层内存，消除了复制数据的需要** **(查阅 <span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvYmxpdHovdGVuc29yX3R1dG9yaWFsLmh0bWwjYnJpZGdlLXRvLW5wLWxhYmVs" title="https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label">Bridge with NumPy<i class="fa fa-external-link"></i></span>)，tensors也优化了自动微分（更多请参考<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvYmFzaWNzL2F1dG9ncmFkcXNfdHV0b3JpYWwuaHRtbA==" title="https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html">Autograd<i class="fa fa-external-link"></i></span>后面部分），如果熟悉ndarrays，那么你对张量API已经很熟悉了，如果没有，那就跟着做！</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br></pre></td></tr></table></figure>


<h3 id="初始化张量"><a href="#初始化张量" class="headerlink" title="初始化张量"></a>初始化张量</h3><p>张量可以用各种方式初始化，看看下面这些例子。</p>
<h4 id="直接从数据中创建"><a href="#直接从数据中创建" class="headerlink" title="直接从数据中创建"></a>直接从数据中创建</h4><p>张量可以从数据中创建，数据类型被自动推断出来：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = [[1,2],[3,4]]</span><br><span class="line">x_data = torch.tensor(data)</span><br><span class="line">print(x_data)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1, 2],</span><br><span class="line">        [3, 4]])</span><br></pre></td></tr></table></figure>


<h4 id="从Numpy数组中创建"><a href="#从Numpy数组中创建" class="headerlink" title="从Numpy数组中创建"></a>从Numpy数组中创建</h4><p>张量可以从Numpy数组中创建（反之亦然，参考<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvYmxpdHovdGVuc29yX3R1dG9yaWFsLmh0bWwjYnJpZGdlLXRvLW5wLWxhYmVs" title="https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label">Bridge with NumPy<i class="fa fa-external-link"></i></span>）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np_array = np.array(data)</span><br><span class="line">x_np = torch.from_numpy(np_array)</span><br><span class="line">print(x_np)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1, 2],</span><br><span class="line">        [3, 4]], dtype=torch.int32)</span><br></pre></td></tr></table></figure>


<h4 id="从其他张量创建"><a href="#从其他张量创建" class="headerlink" title="从其他张量创建"></a>从其他张量创建</h4><p>新张量保留了参数张量的属性（形状、数据类型），除非被明确重写。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_ones = torch.ones_like(x_data)    #保留x_data属性</span><br><span class="line">print(f&quot;one tensor: \n &#123;x_ones&#125; \n&quot;)</span><br><span class="line"></span><br><span class="line">x_rand = torch.rand_like(x_data,dtype=torch.float)    #覆盖x_data属性</span><br><span class="line">print(f&quot;random tensor: \n &#123;x_rand&#125; \n&quot;)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">one tensor: </span><br><span class="line"> tensor([[1, 1],</span><br><span class="line">        [1, 1]]) </span><br><span class="line"></span><br><span class="line">random tensor: </span><br><span class="line"> tensor([[0.4668, 0.3876],</span><br><span class="line">        [0.0113, 0.6029]]) </span><br></pre></td></tr></table></figure>


<h4 id="随机或常量值"><a href="#随机或常量值" class="headerlink" title="随机或常量值"></a>随机或常量值</h4><p>形状是张量维度的元组，在下面函数中，它决定了输出张量的维度：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">shape = (2,3)</span><br><span class="line">rand_tensor = torch.rand(shape)</span><br><span class="line">ones_tensor = torch.ones(shape)</span><br><span class="line">zeros_tensor = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line">print(f&quot;random tensor: \n &#123;rand_tensor&#125;&quot;)</span><br><span class="line">print(f&quot;ones tensor: \n &#123;ones_tensor&#125;&quot;)</span><br><span class="line">print(f&quot;zeros tensor: \n &#123;zeros_tensor&#125;&quot;)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">random tensor: </span><br><span class="line"> tensor([[0.1024, 0.2078, 0.6793],</span><br><span class="line">        [0.8625, 0.4195, 0.1272]])</span><br><span class="line">ones tensor: </span><br><span class="line"> tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]])</span><br><span class="line">zeros tensor: </span><br><span class="line"> tensor([[0., 0., 0.],</span><br><span class="line">        [0., 0., 0.]])</span><br></pre></td></tr></table></figure>


<h3 id="张量属性"><a href="#张量属性" class="headerlink" title="张量属性"></a>张量属性</h3><p>张量属性描述了他们的形状、数据类型和存储他们的设备。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.rand(3,4)</span><br><span class="line">print(f&quot;shape of tensor: &#123;tensor.shape&#125;&quot;)</span><br><span class="line">print(f&quot;datatype of tensor: &#123;tensor.dtype&#125;&quot;)</span><br><span class="line">print(f&quot;device tensor is stored on: &#123;tensor.device&#125;&quot;)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">shape of tensor: torch.Size([3, 4])</span><br><span class="line">datatype of tensor: torch.float32</span><br><span class="line">device tensor is stored on: cpu</span><br></pre></td></tr></table></figure>


<h3 id="张量运算"><a href="#张量运算" class="headerlink" title="张量运算"></a>张量运算</h3><p>张量运算超过100种，包括包括算术、线性代数、矩阵操作(转置，索引，切片) 及抽样，更多描述请参考** **<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS90b3JjaC5odG1s" title="https://pytorch.org/docs/stable/torch.html">here<i class="fa fa-external-link"></i></span>。</p>
<p>通常这些运算可以在GPU上运行（通常比CPU上运行速度更快），如果你使用的是Colab，可以分配出一个GPU，步骤： Runtime &gt; Change runtime type &gt; GPU</p>
<p>默认情况下，张量是在CPU上创建的，我们用 .to 方法直接将张量转移到GPU上(确保GPU可用)，跨设备复制大量的张量是非常耗时间和内存的！</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># We move our tensor to the GPU if available</span><br><span class="line">if torch.cuda.is_available():</span><br><span class="line">    tensor = tensor.to(&quot;cuda&quot;)</span><br></pre></td></tr></table></figure>


<h4 id="标准化的numpy类索引和切片"><a href="#标准化的numpy类索引和切片" class="headerlink" title="标准化的numpy类索引和切片"></a>标准化的numpy类索引和切片</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.ones(4,4)</span><br><span class="line">print(f&quot;first row: &#123;tensor[0]&#125;&quot;)</span><br><span class="line">print(f&quot;first column: &#123;tensor[:,0]&#125;&quot;)</span><br><span class="line">print(f&quot;last column: &#123;tensor[...,-1]&#125;&quot;)</span><br><span class="line">tensor[:,1] = 0</span><br><span class="line">print(tensor)</span><br></pre></td></tr></table></figure>
<p>输出结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">first row: tensor([1., 1., 1., 1.])</span><br><span class="line">first column: tensor([1., 1., 1., 1.])</span><br><span class="line">last column: tensor([1., 1., 1., 1.])</span><br><span class="line">tensor([[1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.],</span><br><span class="line">        [1., 0., 1., 1.]])</span><br></pre></td></tr></table></figure>

<p>torch.cat可以将一系列张量顺着给定的维度连接起来。参考<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9nZW5lcmF0ZWQvdG9yY2guc3RhY2suaHRtbA==" title="https://pytorch.org/docs/stable/generated/torch.stack.html">torch.stack<i class="fa fa-external-link"></i></span>，它是另一个张量连接op，与torch.cat稍有不同。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1 = torch.cat([tensor,tensor,tensor],dim=1)</span><br><span class="line">print(t1)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.],</span><br><span class="line">        [0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.],</span><br><span class="line">        [0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.],</span><br><span class="line">        [0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.]])</span><br></pre></td></tr></table></figure>


<h4 id="Arithmetic-operations算数运算"><a href="#Arithmetic-operations算数运算" class="headerlink" title="Arithmetic operations算数运算"></a>Arithmetic operations算数运算</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">tensor = tensor = torch.ones(4,4)</span><br><span class="line">tensor[0] = 0</span><br><span class="line">tensor[2] = 2</span><br><span class="line">tensor[3] = 3</span><br><span class="line">print(&#x27;tensor输出结果:\n&#x27;,tensor,)</span><br><span class="line"></span><br><span class="line">#两个矩阵相乘，y1,y2,y3得出相同的值，其中官网对y3的写法如下：</span><br><span class="line">#y3 = torch.rand_like(y1)</span><br><span class="line">#torch.matmul(tensor, tensor.T, out=y3)</span><br><span class="line">y1 = tensor @ tensor.T</span><br><span class="line">y2 = tensor.matmul(tensor.T)</span><br><span class="line">y3 = torch.matmul(tensor,tensor.T)</span><br><span class="line">print(&#x27;y3输出结果:\n&#x27;,y3)</span><br><span class="line"></span><br><span class="line">#两个矩阵对应元素相乘，z1,z2,z3得出相同的值，其中官网对z3的写法如下：</span><br><span class="line">#z3 = torch.rand_like(tensor)</span><br><span class="line">#torch.mul(tensor, tensor, out=z3)</span><br><span class="line">z1 = tensor * tensor</span><br><span class="line">z2 = tensor.mul(tensor)</span><br><span class="line">z3 = torch.mul(tensor,tensor)</span><br><span class="line">print(&#x27;z3输出结果:\n&#x27;,z3)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">tensor输出结果:</span><br><span class="line"> tensor([[0., 0., 0., 0.],</span><br><span class="line">        [1., 1., 1., 1.],</span><br><span class="line">        [2., 2., 2., 2.],</span><br><span class="line">        [3., 3., 3., 3.]])</span><br><span class="line">y3输出结果:</span><br><span class="line"> tensor([[ 0.,  0.,  0.,  0.],</span><br><span class="line">        [ 0.,  4.,  8., 12.],</span><br><span class="line">        [ 0.,  8., 16., 24.],</span><br><span class="line">        [ 0., 12., 24., 36.]])</span><br><span class="line">z3输出结果:</span><br><span class="line"> tensor([[0., 0., 0., 0.],</span><br><span class="line">        [1., 1., 1., 1.],</span><br><span class="line">        [4., 4., 4., 4.],</span><br><span class="line">        [9., 9., 9., 9.]])</span><br></pre></td></tr></table></figure>


<h4 id="Single-element-tensors单元张量"><a href="#Single-element-tensors单元张量" class="headerlink" title="Single-element tensors单元张量"></a>Single-element tensors单元张量</h4><p>如果有一个单元张量，例如将一个张量的所有值聚合成一个值，可以使用item()将其转换为python数值：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor = tensor = torch.ones(4,4)</span><br><span class="line">agg = tensor.sum()</span><br><span class="line">agg_item = agg.item()</span><br><span class="line">print(agg_item,type(agg_item))</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">16.0 &lt;class &#x27;float&#x27;&gt;</span><br></pre></td></tr></table></figure>


<h4 id="In-place-operations就地操作"><a href="#In-place-operations就地操作" class="headerlink" title="In-place operations就地操作"></a>In-place operations就地操作</h4><p>将结果储存到操作对象中的操作称为就地操作，以 _ 为后缀。例如：x.copy_(y)，x.t_()，将改变 x。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor = tensor = torch.ones(4,4)</span><br><span class="line">print(f&quot;&#123;tensor&#125; \n&quot;)</span><br><span class="line">tensor.add_(5)</span><br><span class="line">print(tensor)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 1., 1., 1.],</span><br><span class="line">        [1., 1., 1., 1.],</span><br><span class="line">        [1., 1., 1., 1.],</span><br><span class="line">        [1., 1., 1., 1.]]) </span><br><span class="line"></span><br><span class="line">tensor([[6., 6., 6., 6.],</span><br><span class="line">        [6., 6., 6., 6.],</span><br><span class="line">        [6., 6., 6., 6.],</span><br><span class="line">        [6., 6., 6., 6.]])</span><br></pre></td></tr></table></figure>
<p>就地操作会节省一些内存，但可能会有问题出现，因为在计算机导数时会立即丢失历史记录，因此不提倡使用。</p>
<h4 id="Bridge-with-NumPy"><a href="#Bridge-with-NumPy" class="headerlink" title="Bridge with NumPy"></a>Bridge with NumPy</h4><p>CPU上的张量和numpy数组可以共享他们的底层内存位置，改变其中一个另一个也随之改变。</p>
<p>Tensor 到 NumPy array：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t = torch.ones(5)</span><br><span class="line">print(f&quot;t: &#123;t&#125;&quot;)</span><br><span class="line">n = t.numpy()</span><br><span class="line">print(f&quot;n: &#123;n&#125;&quot;)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([1., 1., 1., 1., 1.])</span><br><span class="line">n: [1. 1. 1. 1. 1.]</span><br></pre></td></tr></table></figure>

<p>改变张量会映射到numpy数组中：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">t = torch.ones(5)</span><br><span class="line">n = t.numpy()</span><br><span class="line">t.add_(1)</span><br><span class="line">print(f&quot;t: &#123;t&#125;&quot;)</span><br><span class="line">print(f&quot;n: &#123;n&#125;&quot;)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t: tensor([2., 2., 2., 2., 2.])</span><br><span class="line">n: [2. 2. 2. 2. 2.]</span><br></pre></td></tr></table></figure>


<h2 id="1-2-数据集和数据加载器"><a href="#1-2-数据集和数据加载器" class="headerlink" title="1.2 数据集和数据加载器"></a>1.2 数据集和数据加载器</h2><p>英文名称：datasets &amp; dataloaders</p>
<p>处理数据样本的代码可能很混乱且难以维护，理想情况下是希望数据集代码与模型训练代码解耦，以获得更好的可读性和模块化。pytorch提供了两个数据基元：torch.utils.data.DataLoader 和 torch.utils.data.Dataset，它允许使用预加载的数据以及计算机本地数据（也就是你自己的数据）。Dataset储存样品及其相应的标签，DataLoader封装了迭代器(iterable)，以便访问Dataset储存的数据。</p>
<p>pytorch提供了许多预加载的数据集（如FashionMNIST），这些数据集对于特定数据的函数进行了子类化torch.utils.data.Dataset和实现化，可以用于原型和测试模型。数据集链接： <span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy92aXNpb24vc3RhYmxlL2RhdGFzZXRzLmh0bWw=" title="https://pytorch.org/vision/stable/datasets.html">Image Datasets<i class="fa fa-external-link"></i></span>, <span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy90ZXh0L3N0YWJsZS9kYXRhc2V0cy5odG1s" title="https://pytorch.org/text/stable/datasets.html">Text Datasets<i class="fa fa-external-link"></i></span> 和 <span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9hdWRpby9zdGFibGUvZGF0YXNldHMuaHRtbA==" title="https://pytorch.org/audio/stable/datasets.html">Audio Datasets<i class="fa fa-external-link"></i></span>。</p>
<h3 id="数据集加载"><a href="#数据集加载" class="headerlink" title="数据集加载"></a>数据集加载</h3><p>下面是如何从TorchVision加载FashionMNIST数据集的示例。Fashion MNIST是Zalando文章图像的数据集，包含6万个示例的训练集和万个示例的测试集。每个示例都是一个28x28灰度图像，与10个类中的一个标签相关联。使用以下参数加载FashionMNIST 数据集：</p>
<ul>
<li>root是存储训练&#x2F;测试数据的路径；</li>
<li>train指定训练或测试数据集；</li>
<li>download&#x3D;True从互联网上下载数据，如果无法在root上找到；</li>
<li>transform和target_transform是指定特征和标签转换。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.utils.data import Dataset</span><br><span class="line">from torchvision import datasets</span><br><span class="line">from torchvision.transforms import ToTensor</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root=&quot;data&quot;,</span><br><span class="line">    train=True,</span><br><span class="line">    download=True,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root=&quot;data&quot;,</span><br><span class="line">    train=False,</span><br><span class="line">    download=True,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="迭代和可视化数据集"><a href="#迭代和可视化数据集" class="headerlink" title="迭代和可视化数据集"></a>迭代和可视化数据集</h3><p>我们可以像列表一样手动索引数据集:training_data[index]。我们使用matplotlib将训练数据中的一些样本可视化。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">labels_map = &#123;</span><br><span class="line">    0: &quot;T-Shirt&quot;,</span><br><span class="line">    1: &quot;Trouser&quot;,</span><br><span class="line">    2: &quot;Pullover&quot;,</span><br><span class="line">    3: &quot;Dress&quot;,</span><br><span class="line">    4: &quot;Coat&quot;,</span><br><span class="line">    5: &quot;Sandal&quot;,</span><br><span class="line">    6: &quot;Shirt&quot;,</span><br><span class="line">    7: &quot;Sneaker&quot;,</span><br><span class="line">    8: &quot;Bag&quot;,</span><br><span class="line">    9: &quot;Ankle Boot&quot;,</span><br><span class="line">&#125;</span><br><span class="line">figure = plt.figure(figsize=(8, 8))</span><br><span class="line">cols, rows = 3, 3</span><br><span class="line">for i in range(1, cols * rows + 1):</span><br><span class="line">    sample_idx = torch.randint(len(training_data), size=(1,)).item()</span><br><span class="line">    img, label = training_data[sample_idx]</span><br><span class="line">    figure.add_subplot(rows, cols, i)</span><br><span class="line">    plt.title(labels_map[label])</span><br><span class="line">    plt.axis(&quot;off&quot;)</span><br><span class="line">    plt.imshow(img.squeeze(), cmap=&quot;gray&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>运行结果：<img src="/images/1664353494073-451a286a-423f-463e-bb99-c4c331f3c5f5.png"></p>
<h3 id="自定义数据集"><a href="#自定义数据集" class="headerlink" title="自定义数据集"></a>自定义数据集</h3><p>自定义Dataset类必须满足三个函数： <strong>init</strong>, <strong>len</strong>, and __getitem__。看看这个实现；FashionMNIST图像存储在目录img_dir中，它们的标签单独存储在CSV文件annotations_file中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import pandas as pd</span><br><span class="line">from torchvision.io import read_image</span><br><span class="line">from torch.utils.data import Dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class CustomImageDataset(Dataset):</span><br><span class="line">    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):</span><br><span class="line">        self.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">        self.img_dir = img_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.target_transform = target_transform</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.img_labels)</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, idx):</span><br><span class="line">        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])</span><br><span class="line">        image = read_image(img_path)</span><br><span class="line">        label = self.img_labels.iloc[idx, 1]</span><br><span class="line">        if self.transform:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">        if self.target_transform:</span><br><span class="line">            label = self.target_transform(label)</span><br><span class="line">        return image, label</span><br></pre></td></tr></table></figure>


<h4 id="init"><a href="#init" class="headerlink" title="init"></a><strong>init</strong></h4><p>当实例化Dataset对象时__init__函数就运行一次。我们初始化包含图像、注释文件和两个转换的目录(下一节将详细介绍)。labels.csv文件如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tshirt1.jpg, 0</span><br><span class="line">tshirt2.jpg, 0</span><br><span class="line">......</span><br><span class="line">ankleboot999.jpg, 9</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):</span><br><span class="line">    self.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">    self.img_dir = img_dir</span><br><span class="line">    self.transform = transform</span><br><span class="line">    self.target_transform = target_transform</span><br></pre></td></tr></table></figure>


<h4 id="len"><a href="#len" class="headerlink" title="len"></a><strong>len</strong></h4><p>__len__函数返回数据集中的样本数量。示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def __len__(self):</span><br><span class="line">    return len(self.img_labels)</span><br></pre></td></tr></table></figure>


<h4 id="getitem"><a href="#getitem" class="headerlink" title="getitem"></a><strong>getitem</strong></h4><p>__getitem__函数从给定索引 idx 处的数据集加载并返回一个示例。基于索引，它识别图像在磁盘上的位置，使用read_image将其转换为一个张量，从self.Img_labels中的csv数据中检索相应的标签，调用其转换函数(如果适用的话)，并返回一个元组中的张量图像和相应的标签。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def __getitem__(self, idx):</span><br><span class="line">    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])</span><br><span class="line">    image = read_image(img_path)</span><br><span class="line">    label = self.img_labels.iloc[idx, 1]</span><br><span class="line">    if self.transform:</span><br><span class="line">        image = self.transform(image)</span><br><span class="line">    if self.target_transform:</span><br><span class="line">        label = self.target_transform(label)</span><br><span class="line">    return image, label</span><br></pre></td></tr></table></figure>


<h3 id="用数据加载器数据进行训练"><a href="#用数据加载器数据进行训练" class="headerlink" title="用数据加载器数据进行训练"></a>用数据加载器数据进行训练</h3><p>数据集每次检索一个样本的数据集的特征和标签。在训练模型时，我们通常希望以“小批量”传递样本，在每个阶段重新打乱数据以减少模型过拟合，并使用Python的多进程处理来加速数据检索。DataLoader是一个可迭代对象，它通过一个简单的API为我们抽象了这种复杂性。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.data import DataLoader</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)</span><br></pre></td></tr></table></figure>


<h3 id="循环访问数据加载器"><a href="#循环访问数据加载器" class="headerlink" title="循环访问数据加载器"></a>循环访问数据加载器</h3><p>上面已经将该数据集加载到DataLoader中，并可以根据需要遍历该数据集。下面的每次迭代都返回一批train_features和train_labels(分别包含batch_size&#x3D;64个特性和标签)。因为我们指定了shuffle&#x3D;True，所以在我们遍历所有批次之后，数据就会被打乱(对于更细粒度的数据加载顺序的控制，请查看<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9kYXRhLmh0bWwjZGF0YS1sb2FkaW5nLW9yZGVyLWFuZC1zYW1wbGVy" title="https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler">Samplers<i class="fa fa-external-link"></i></span>)。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from torchvision import datasets</span><br><span class="line">from torchvision.transforms import ToTensor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root=&quot;data&quot;,</span><br><span class="line">    train=True,</span><br><span class="line">    download=True,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root=&quot;data&quot;,</span><br><span class="line">    train=False,</span><br><span class="line">    download=True,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)</span><br><span class="line"># Display image and label.</span><br><span class="line">train_features, train_labels = next(iter(train_dataloader))</span><br><span class="line">print(f&quot;Feature batch shape: &#123;train_features.size()&#125;&quot;)</span><br><span class="line">print(f&quot;Labels batch shape: &#123;train_labels.size()&#125;&quot;)</span><br><span class="line">img = train_features[0].squeeze()</span><br><span class="line">label = train_labels[0]</span><br><span class="line">plt.imshow(img, cmap=&quot;gray&quot;)</span><br><span class="line">plt.show()</span><br><span class="line">print(f&quot;Label: &#123;label&#125;&quot;)</span><br></pre></td></tr></table></figure>
<p>输出结果（代码+图片）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Feature batch shape: torch.Size([64, 1, 28, 28])</span><br><span class="line">Labels batch shape: torch.Size([64])</span><br><span class="line">Label: 3</span><br></pre></td></tr></table></figure>
<p><img src="/images/1664358122608-2c8dbc03-91bf-4caa-8579-17219b05c365.png"></p>
<h3 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h3><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9kYXRhLmh0bWw=" title="https://pytorch.org/docs/stable/data.html">torch.utils.data API<i class="fa fa-external-link"></i></span></li>
</ul>
<h2 id="1-3-Transform"><a href="#1-3-Transform" class="headerlink" title="1.3 Transform"></a>1.3 Transform</h2><p>数据并不总是以训练机器学习算法所需的最终处理形式出现，用Transform对数据进行一些处理，使其适合于训练。</p>
<p>所有的 TorchVision 数据集都有两个参数——transform用来修改特征，target_transform用来修改标签——它们接受包含转换逻辑的可调用对象。torchvision.transforms模块提供了几个常用的开箱即用转换。</p>
<p>FashionMNIST特征采用PIL Image格式，标签为整数。在训练中，我们需要特征作为标准化张量，标签是one-hot编码张量，本次用ToTensor和Lambda做这些变换。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torchvision import datasets</span><br><span class="line">from torchvision.transforms import ToTensor,Lambda</span><br><span class="line"></span><br><span class="line">ds = datasets.FashionMNIST(</span><br><span class="line">    root = &quot;data&quot;,</span><br><span class="line">    train = True,</span><br><span class="line">    download = True,</span><br><span class="line">    transform = ToTensor(),</span><br><span class="line">    target_transform = Lambda(lambda y: torch.zeros(10,dtype=torch.float).scatter_(0,torch.trnsor(y),value=1))</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>输出结果：之前没有下载过数据集会输出downloading进度，已经下载过，运行之后没有任何反应。</p>
<h3 id="ToTensor"><a href="#ToTensor" class="headerlink" title="ToTensor"></a>ToTensor</h3><p><span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy92aXNpb24vc3RhYmxlL3RyYW5zZm9ybXMuaHRtbCN0b3JjaHZpc2lvbi50cmFuc2Zvcm1zLlRvVGVuc29y" title="https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor">ToTensor<i class="fa fa-external-link"></i></span>将PIL图像或NumPy ndarray转换为FloatTensor，并缩放图像的像素强度值在范围[0. , 1.]内。</p>
<h3 id="Lambda-Transforms"><a href="#Lambda-Transforms" class="headerlink" title="Lambda Transforms"></a>Lambda Transforms</h3><p>λ-转换适用于任何用户定义的Lambda函数，定义一个函数将整数转换one-hot编码张量。首先创建一个大小为10（数据集中标签的数量）的零张量，调用scatter_把 value&#x3D;1 分配给索引对应的标签y上：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">target_transform = Lambda(lambda y: torch.zeros(10,dtype=torch.float).scatter_(dim=0,index=torch.trnsor(y),value=1))</span><br></pre></td></tr></table></figure>


<h3 id="延伸阅读-1"><a href="#延伸阅读-1" class="headerlink" title="延伸阅读"></a>延伸阅读</h3><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy92aXNpb24vc3RhYmxlL3RyYW5zZm9ybXMuaHRtbA==" title="https://pytorch.org/vision/stable/transforms.html">torchvision.transforms API<i class="fa fa-external-link"></i></span></li>
</ul>
<h2 id="1-4-构建模型"><a href="#1-4-构建模型" class="headerlink" title="1.4 构建模型"></a>1.4 构建模型</h2><h3 id="构建神经网络"><a href="#构建神经网络" class="headerlink" title="构建神经网络"></a>构建神经网络</h3><p>神经网络是由对数据进行处理的层&#x2F;块组成。torch.nn命名空间提供了构建神经网络所需的构建块。pytorch中每个模型都是<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9nZW5lcmF0ZWQvdG9yY2gubm4uTW9kdWxlLmh0bWw=" title="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">nn.Module<i class="fa fa-external-link"></i></span>的子类。神经网络本身就是由其他模块(层)组成的模块，这种嵌套结构可以轻松地构建和管理复杂的架构体系。</p>
<p>构建一个对FashionMNIST数据集中的图像进行分类的神经网络。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torchvision import datasets,transforms</span><br></pre></td></tr></table></figure>

<p>获取训练资源：如果有cpu之类的硬件加速器，就在加速器上训练模型，查看torch.cuda是否可用，不可用则用cpu:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;</span><br><span class="line">print(device)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cuda</span><br></pre></td></tr></table></figure>

<p>定义类：通过继承nn.Module来定义新的神经网络，并在__init__中初始化神经网络层。每一个nn.Module子类在forward方法中实现对输入数据的操作。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;</span><br><span class="line">class NeuralNetwork(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(NeuralNetwork,self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(28*28,512),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(512,512),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(512,10)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self,x):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack()</span><br><span class="line">        return logits</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">NeuralNetwork(</span><br><span class="line">  (flatten): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (linear_relu_stack): Sequential(</span><br><span class="line">    (0): Linear(in_features=784, out_features=512, bias=True)</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">    (3): ReLU()</span><br><span class="line">    (4): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>将数据输入模型，这将执行模型的forward以及一些<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3B5dG9yY2gvcHl0b3JjaC9ibG9iLzI3MDExMWI3YjYxMWQxNzQ5NjdlZDIwNDc3Njk4NWNlZmNhOWMxNDQvdG9yY2gvbm4vbW9kdWxlcy9tb2R1bGUucHkjTDg2Ng==" title="https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866">后台操作<i class="fa fa-external-link"></i></span>。不要直接调用model.forward()</p>
<p>在输入上调用模型将返回一个二维张量，其中 dim&#x3D;0 对应于每个类的 10 个原始预测值的每个输出，dim&#x3D;1 对应于每个输出的单个值。我们通过nn.Softmax模块的实例传递预测概率来获得预测概率。</p>
<h3 id="Model-Layers"><a href="#Model-Layers" class="headerlink" title="Model Layers"></a>Model Layers</h3><p>在FashionMNIST模型中分解图层，小批量处理3张28*28的图片样本，观察通过网络时的样子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_image = torch.rand(3,28,28)</span><br><span class="line">print(input_image.size())</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 28, 28])</span><br></pre></td></tr></table></figure>


<h3 id="nn-Flatten"><a href="#nn-Flatten" class="headerlink" title="nn.Flatten"></a>nn.Flatten</h3><p>flatten()是对多维数据的降维函数，默认缺省参数为0，也就是说flatten()和flatte(0)效果一样。python里的flatten(dim)表示，从第dim个维度开始展开，将后面的维度转化为一维.也就是说，只保留dim之前的维度，其他维度的数据全都挤在dim这一维。比如一个数据的维度是( S 0 , S 1 , S 2……… , S n ) (S0,S1,S2………,Sn)(S0,S1,S2………,Sn), flatten(m)后的数据为( S 0 ， S 1 ， S 2 ， . . . ， S m − 2 ， S m − 1 ， S m ∗ S m + 1 ∗ S m + 2 ∗ . . . ∗ S n ) (S0，S1，S2，…，Sm-2，Sm-1，Sm<em>Sm+1</em>Sm+2*…*Sn)(S0，S1，S2，…，Sm−2，Sm−1，Sm∗Sm+1∗Sm+2∗…∗Sn)</p>
<p>初始化 nn.Flatten图层，用来将每个 2D 28x28 图像转换为包含 784 个像素值的连续数组（（在 dim&#x3D;0 时）保持小批量维度）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flatten = nn.Flatten()</span><br><span class="line">flat_image = flatten(input_image)</span><br><span class="line">print(flat_image.size())</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 28, 28])</span><br><span class="line">torch.Size([3, 784])</span><br></pre></td></tr></table></figure>


<h3 id="nn-Linear"><a href="#nn-Linear" class="headerlink" title="nn.Linear"></a>nn.Linear</h3><p>线性层是一个模块，它使用存储好的权重和偏差对输入进行线性转换。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">layer1 = nn.Linear(in_features=28*28, out_features=20)</span><br><span class="line">hidden1 = layer1(flat_image)</span><br><span class="line">print(hidden1.size())</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 20])   #表示列表中有3个元素，每个元素是一个新的列表，元素列表中有20个元素</span><br></pre></td></tr></table></figure>


<h3 id="nn-ReLU"><a href="#nn-ReLU" class="headerlink" title="nn.ReLU"></a>nn.ReLU</h3><p>非线性激活是在模型的输入和输出之间创建的复杂映射。它们应用于线性变换后引入非线性，帮助神经网络学习各种各样的现象。</p>
<p>此模型中，线性层之间使用nn.ReLU，非线性中引入其他激活量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(f&quot;Before ReLU: &#123;hidden1&#125;\n\n&quot;)</span><br><span class="line">hidden1 = nn.ReLU()(hidden1)</span><br><span class="line">print(f&quot;After ReLU: &#123;hidden1&#125;&quot;)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Before ReLU: tensor([[ 0.1426, -0.3470, -0.1418, -0.3554, -0.1413, -0.1681, -0.1939, -0.0601,</span><br><span class="line">          0.2604,  0.1082,  0.2209,  0.2681,  0.1723,  0.1945, -0.1162,  0.1421,</span><br><span class="line">         -0.0883,  0.0151, -0.2167, -0.3203],</span><br><span class="line">        [ 0.0153, -0.5945, -0.1802, -0.3600,  0.1283,  0.0200,  0.1102, -0.0895,</span><br><span class="line">          0.3346,  0.1966,  0.2081,  0.1350,  0.4364,  0.3082,  0.1694, -0.1802,</span><br><span class="line">         -0.5276,  0.3391,  0.2126, -0.1366],</span><br><span class="line">        [ 0.0145, -0.2145, -0.4439, -0.1665, -0.1340,  0.0364, -0.5288, -0.1765,</span><br><span class="line">          0.2297, -0.1475,  0.0147,  0.0021,  0.1454,  0.4573, -0.3557, -0.2232,</span><br><span class="line">         -0.4160,  0.1634, -0.0264, -0.4376]], grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">After ReLU: tensor([[0.1426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2604,</span><br><span class="line">         0.1082, 0.2209, 0.2681, 0.1723, 0.1945, 0.0000, 0.1421, 0.0000, 0.0151,</span><br><span class="line">         0.0000, 0.0000],</span><br><span class="line">        [0.0153, 0.0000, 0.0000, 0.0000, 0.1283, 0.0200, 0.1102, 0.0000, 0.3346,</span><br><span class="line">         0.1966, 0.2081, 0.1350, 0.4364, 0.3082, 0.1694, 0.0000, 0.0000, 0.3391,</span><br><span class="line">         0.2126, 0.0000],</span><br><span class="line">        [0.0145, 0.0000, 0.0000, 0.0000, 0.0000, 0.0364, 0.0000, 0.0000, 0.2297,</span><br><span class="line">         0.0000, 0.0147, 0.0021, 0.1454, 0.4573, 0.0000, 0.0000, 0.0000, 0.1634,</span><br><span class="line">         0.0000, 0.0000]], grad_fn=&lt;ReluBackward0&gt;)</span><br><span class="line"></span><br><span class="line">进程已结束,退出代码0</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h3 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h3><p>nn.Sequential是模块的序列容器，数据按照定义的顺序传递给所有模块。可以使用序列容器把类似seq_modules的快速网络组合在一起。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">seq_modules = nn.Sequential(</span><br><span class="line">    flatten,</span><br><span class="line">    layer1,</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(20, 10)</span><br><span class="line">)</span><br><span class="line">input_image = torch.rand(3,28,28)</span><br><span class="line">logits = seq_modules(input_image)</span><br><span class="line">print(&#x27;logits:&#x27;,&#x27;\n&#x27;,logits)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">logits: </span><br><span class="line"> tensor([[ 0.1616,  0.0817,  0.0735, -0.2260,  0.1975, -0.1653,  0.1895,  0.0527,</span><br><span class="line">          0.0308, -0.0972],</span><br><span class="line">        [ 0.1359,  0.1092,  0.1909, -0.1869,  0.3001, -0.1487,  0.2278,  0.1496,</span><br><span class="line">         -0.0421, -0.0847],</span><br><span class="line">        [ 0.1871,  0.0593,  0.1264, -0.1700,  0.3118, -0.2164,  0.1416,  0.1205,</span><br><span class="line">         -0.0370, -0.0457]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>


<h3 id="nn-Softmax"><a href="#nn-Softmax" class="headerlink" title="nn.Softmax"></a>nn.Softmax</h3><p>神经网络的最后一个线性层返回logits 的原始值在区间[-infty, infty]内，这些值被传递给nn.Softmax模块。logits值被缩放在区间[0,1]内，表示模型对每个类的预测概率。Dim参数表示值之和为1的维度。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">softmax = nn.Softmax(dim=1)</span><br><span class="line">pred_probab = softmax(logits)</span><br><span class="line">print(&#x27;pred_probab:&#x27;,&#x27;\n&#x27;,pred_probab)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pred_probab: </span><br><span class="line"> tensor([[0.1160, 0.0935, 0.1411, 0.0945, 0.1039, 0.0819, 0.1050, 0.0800, 0.0721,</span><br><span class="line">         0.1120],</span><br><span class="line">        [0.1140, 0.0928, 0.1494, 0.1022, 0.0992, 0.0822, 0.0972, 0.0789, 0.0727,</span><br><span class="line">         0.1114],</span><br><span class="line">        [0.1249, 0.0926, 0.1434, 0.0946, 0.1088, 0.0728, 0.0986, 0.0874, 0.0711,</span><br><span class="line">         0.1058]], grad_fn=&lt;SoftmaxBackward0&gt;)</span><br></pre></td></tr></table></figure>


<h3 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h3><p>神经网络中是许多层都是参数化的，即在训练过程中有相关的权重和偏差被优化。子类化nn.Module模块自动跟踪模型对象中定义的所有字段，并使用模型的parameters()或named_parameters()方法访问所有参数。</p>
<p>在这个例子中，我们迭代每个参数，并打印它的大小和预览其值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(f&quot;Model structure: &#123;model&#125;\n\n&quot;)</span><br><span class="line">for name, param in model.named_parameters():</span><br><span class="line">    print(f&quot;Layer: &#123;name&#125; | Size: &#123;param.size()&#125; | Values : &#123;param[:2]&#125; \n&quot;)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Model structure: NeuralNetwork(</span><br><span class="line">  (flatten): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (linear_relu_stack): Sequential(</span><br><span class="line">    (0): Linear(in_features=784, out_features=512, bias=True)</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">    (3): ReLU()</span><br><span class="line">    (4): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0185,  0.0094, -0.0099,  ...,  0.0073,  0.0120, -0.0207],</span><br><span class="line">        [ 0.0332,  0.0094,  0.0302,  ..., -0.0288, -0.0147, -0.0188]],</span><br><span class="line">       device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0175, 0.0353], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0042, -0.0256, -0.0213,  ..., -0.0243,  0.0143,  0.0336],</span><br><span class="line">        [-0.0410,  0.0155, -0.0331,  ..., -0.0383,  0.0384,  0.0250]],</span><br><span class="line">       device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0219,  0.0169], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0372, -0.0264,  0.0106,  ...,  0.0389,  0.0076,  0.0132],</span><br><span class="line">        [ 0.0309,  0.0214,  0.0044,  ..., -0.0207, -0.0020, -0.0263]],</span><br><span class="line">       device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0196,  0.0350], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;) </span><br></pre></td></tr></table></figure>


<h3 id="延伸阅读-2"><a href="#延伸阅读-2" class="headerlink" title="延伸阅读"></a>延伸阅读</h3><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9ubi5odG1s" title="https://pytorch.org/docs/stable/nn.html">torch.nn API<i class="fa fa-external-link"></i></span></li>
</ul>
<h2 id="1-5-自动求导Autograd"><a href="#1-5-自动求导Autograd" class="headerlink" title="1.5 自动求导Autograd"></a>1.5 自动求导Autograd</h2><p>在训练神经网络时，最常用的算法是反向传播。在该算法中，参数(模型权重)根据损失函数相对于给定参数的梯度进行调整。为了计算这些梯度，PyTorch内置了一个名为torch.autograd的微分引擎。它支持自动计算任何计算图的梯度。</p>
<p>考虑最简单的单层神经网络，输入x，参数w和b，以及一些损失函数。它可以在PyTorch中以如下方式定义：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">x = torch.ones(5) #input tensor</span><br><span class="line">y = torch.zeros(3) #expected output</span><br><span class="line">w = torch.randn(5,3,requires_grad=True)</span><br><span class="line">b = torch.randn(3,requires_grad=True)</span><br><span class="line">z = torch.matmul(x,w)+b</span><br><span class="line">loss = torch.nn.functional.binary_cross_entropy_with_logits(z,y)</span><br></pre></td></tr></table></figure>


<h3 id="Tensors-Functions-and-Computational-graph"><a href="#Tensors-Functions-and-Computational-graph" class="headerlink" title="Tensors, Functions and Computational graph"></a>Tensors, Functions and Computational graph</h3><p>这段代码定义了以下计算图：</p>
<p><img src="/images/1665286572199-29ead83d-9bd2-47b5-b9b9-8ba46e6fbc1a.jpeg"><br>在这个网络中，优化参数w和b，需要计算损失函数的梯度，为了做到这一点，我们设置这些张量的requires_grad属性。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">您可以在创建张量时设置requires_grad的值，或者稍后使用x.requires_grad_(True)方法。</span><br></pre></td></tr></table></figure>
<p>应用在张量上构造计算图的函数实际上是一个函数类的对象。该对象知道如何在正方向计算函数，也知道在反向传播过程中如何计算其导数。反向传播函数的引用存储在张量的grad_fn属性中，更多信息请参考function<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9hdXRvZ3JhZC5odG1sI2Z1bmN0aW9u" title="https://pytorch.org/docs/stable/autograd.html#function">文档<i class="fa fa-external-link"></i></span>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(f&quot;Gradient function for z = &#123;z.grad_fn&#125;&quot;)</span><br><span class="line">print(f&quot;Gradient funxtion for loss = &#123;loss.grad_fn&#125;&quot;)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Gradient function for z = &lt;AddBackward0 object at 0x000001FA3EAB56D0&gt;</span><br><span class="line">Gradient funxtion for loss = &lt;BinaryCrossEntropyWithLogitsBackward0 object at 0x000001FA3EAB56D0&gt;</span><br></pre></td></tr></table></figure>


<h3 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h3><p>优化神经网络参数的权重，需要计算损失函数对参数的导数，即计算固定x和y值的和，调用loss.backward()来计算这些导数，然后从w.grad 和 b.grad检索值：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line">print(w.grad)</span><br><span class="line">print(b.grad)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.0405, 0.0236, 0.1284],</span><br><span class="line">        [0.0405, 0.0236, 0.1284],</span><br><span class="line">        [0.0405, 0.0236, 0.1284],</span><br><span class="line">        [0.0405, 0.0236, 0.1284],</span><br><span class="line">        [0.0405, 0.0236, 0.1284]])</span><br><span class="line">tensor([0.0405, 0.0236, 0.1284])</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.只能获取计算图的叶节点的grad属性，它们的requires_grad属性设置为True，图中的其他节点找不到梯度。</span><br><span class="line"></span><br><span class="line">2.由于性能原因，只能在给定的图上使用一次向后梯度计算。如果需要对同一个图进行几个向后调用，则需要将retain_graph=True传递给向后调用。</span><br></pre></td></tr></table></figure>


<h3 id="禁用梯度跟踪Disabling-Gradient-Tracking"><a href="#禁用梯度跟踪Disabling-Gradient-Tracking" class="headerlink" title="禁用梯度跟踪Disabling Gradient Tracking"></a>禁用梯度跟踪Disabling Gradient Tracking</h3><p>默认情况下，所有require_grad &#x3D;True的张量都会跟踪其计算历史，支持梯度计算。但是，在某些情况下不需要这样做。例如，训练的模型只是想把它应用到一些输入数据上，即只想通过网络进行正向计算。通过使用torch.no_grad()块包围需要计算的代码来停止跟踪计算：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">z = torch.matmul(x,w)+b</span><br><span class="line">print(z.requires_grad)</span><br><span class="line"></span><br><span class="line">with torch.no_grad():</span><br><span class="line">    z = torch.matmul(x,w)+b</span><br><span class="line">print(z.requires_grad)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">True</span><br><span class="line">False</span><br></pre></td></tr></table></figure>

<p>另一种实现相同结果的方法是对张量使用detach()方法:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z = torch.matmul(x,w)+b</span><br><span class="line">z_det = z.detach()</span><br><span class="line">print(z_det.requires_grad)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">False</span><br></pre></td></tr></table></figure>

<p>禁用梯度跟踪的原因如下：</p>
<ul>
<li>将神经网络中的一些参数标记为冻结参数，这是一个微调预训练网络的常见方案；</li>
<li>在只向正向传递时加快计算速度，因为在不跟踪梯度张量上的计算将更加高效。</li>
</ul>
<h3 id="有关计算图的更多信息"><a href="#有关计算图的更多信息" class="headerlink" title="有关计算图的更多信息"></a>有关计算图的更多信息</h3><p>从概念上讲，autograd 在由 Function 对象组成的有向无环图(DAG)中保留数据(张量)和所有已执行操作(以及生成的新张量)的记录。在此 DAG 中，叶子是输入张量，根是输出张量。通过从根到叶跟踪此图，可以使用链式规则自动计算梯度。</p>
<p>在正向传递中，autograd同时执行两项操作：</p>
<ul>
<li>运行请求操作来计算产生的张量** ；**</li>
<li>在 DAG 中维护运算的梯度函数。</li>
</ul>
<p>当在DAG root. autograd上调用.backward()时，向后传递开始，然后:</p>
<ul>
<li>计算每个.grad_fn的梯度；</li>
<li>将它们累积在相应张量的.grad属性中；</li>
<li>使用链规则，一直传播到叶张量。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在PyTorch中，DAG是动态的。在每次.backward()调用之后，autograd开始填充一个新的图。这正是允许在模型中使用控制流语句的原因;如果需要，可以在每次迭代中更改形状、大小和操作。</span><br></pre></td></tr></table></figure>


<h3 id="选读：张量梯度和雅可比积"><a href="#选读：张量梯度和雅可比积" class="headerlink" title="选读：张量梯度和雅可比积"></a>选读：张量梯度和雅可比积</h3><p>在许多情况下，我们有一个标量损失函数，我们需要计算相对于某些参数的梯度。但是，在某些情况下，输出函数是任意张量。在这种情况下，PyTorch 允许计算所谓的雅可比积，而不是实际的梯度。</p>
<p>向量函数,其中,。对的梯度由雅可比矩阵给出:</p>
<p>对于给定的输入向量， PyTorch允许计算雅可比矩阵，而不是计算雅可比矩阵本身。这是通过反向调用作为参数来实现的,的大小应该和原始张量的大小相同，我们要根据它来计算乘积:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">inp = torch.eye(4,5,requires_grad=True)</span><br><span class="line">out = (inp+1).pow(2).t()  #pow()幂函数，t()转置</span><br><span class="line">out.backward(torch.ones_like(out),retain_graph=True)</span><br><span class="line">print(f&quot;First call \n &#123;inp.grad&#125;&quot;)</span><br><span class="line">out.backward(torch.ones_like(out),retain_graph=True)</span><br><span class="line">print(f&quot;\nSensond call \n &#123;inp.grad&#125;&quot;)</span><br><span class="line">out.backward(torch.ones_like(out),retain_graph=True)</span><br><span class="line">print(f&quot;\n Call after zeroing gradient \n &#123;inp.grad&#125;&quot;)</span><br></pre></td></tr></table></figure>
<p>其中，torch.eye()函数主要是为了生成对角线全为1，其余部分全为0的二维数组；</p>
<ul>
<li>函数原型： torch.eye(n，m&#x3D;None，out&#x3D;None)；</li>
<li>参数解释：n行数，m列数，out输出类型。</li>
</ul>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">First call </span><br><span class="line"> tensor([[4., 2., 2., 2., 2.],</span><br><span class="line">        [2., 4., 2., 2., 2.],</span><br><span class="line">        [2., 2., 4., 2., 2.],</span><br><span class="line">        [2., 2., 2., 4., 2.]])</span><br><span class="line"></span><br><span class="line">Sensond call </span><br><span class="line"> tensor([[8., 4., 4., 4., 4.],</span><br><span class="line">        [4., 8., 4., 4., 4.],</span><br><span class="line">        [4., 4., 8., 4., 4.],</span><br><span class="line">        [4., 4., 4., 8., 4.]])</span><br><span class="line"></span><br><span class="line"> Call after zeroing gradient </span><br><span class="line"> tensor([[12.,  6.,  6.,  6.,  6.],</span><br><span class="line">        [ 6., 12.,  6.,  6.,  6.],</span><br><span class="line">        [ 6.,  6., 12.,  6.,  6.],</span><br><span class="line">        [ 6.,  6.,  6., 12.,  6.]])</span><br></pre></td></tr></table></figure>

<p>注意，当使用相同的参数第二次向后调用时，梯度的值是不同的。这是因为在做反向传播时，PyTorch会对梯度进行累加，即计算出的梯度的值被添加到计算图的所有叶子节点的grad属性中。如果想计算正确的梯度，需要在此之前将grad属性归零。在现实训练中，优化器可以帮助我们做到这一点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以前调用的backward()函数不带参数，这本质上相当于向后调用(torch.tensor(1.0))，对于标量值函数是一种计算梯度的有效方法，如神经网络训练期间的损耗。</span><br></pre></td></tr></table></figure>


<h3 id="延伸阅读-3"><a href="#延伸阅读-3" class="headerlink" title="延伸阅读"></a>延伸阅读</h3><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9ub3Rlcy9hdXRvZ3JhZC5odG1s" title="https://pytorch.org/docs/stable/notes/autograd.html">Autograd Mechanics<i class="fa fa-external-link"></i></span></li>
</ul>
<h2 id="1-6-优化Optimization"><a href="#1-6-优化Optimization" class="headerlink" title="1.6 优化Optimization"></a>1.6 优化Optimization</h2><h3 id="优化模型参数"><a href="#优化模型参数" class="headerlink" title="优化模型参数"></a>优化模型参数</h3><p>现在我们有了一个模型和数据，通过在数据上优化其参数来训练、验证和测试模型。训练一个模型是一个迭代的过程；在每次迭代(称为epoch)中，模型对输出进行猜测，计算猜测中的误差(损失)，收集误差对其参数的导数(正如我们在前一节中看到的)，并使用梯度下降优化这些参数。关于这个过程的更详细的演练，请查看来自<span class="exturl" data-url="aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj10SWVITG5qczVVOA==" title="https://www.youtube.com/watch?v=tIeHLnjs5U8">backpropagation from 3Blue1Brown<i class="fa fa-external-link"></i></span>。</p>
<h3 id="前提代码"><a href="#前提代码" class="headerlink" title="前提代码"></a>前提代码</h3><p>我们将从前面的章节数据集和数据载入器以及构建模型中加载部分代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torchvision import datasets</span><br><span class="line">from torchvision.transforms import ToTensor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root=&#x27;data&#x27;,</span><br><span class="line">    train=True,</span><br><span class="line">    download=True,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root=&#x27;data&#x27;,</span><br><span class="line">    train=True,</span><br><span class="line">    download=True,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data,batch_size=64)</span><br><span class="line">test_dataloader = DataLoader(training_data,batch_size=64)</span><br><span class="line"></span><br><span class="line">class NeuralNetwork(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(NeuralNetwork,self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(28*28,512),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(512,512),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(512,10),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self,x):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        return logits</span><br><span class="line"></span><br><span class="line">model = NeuralNetwork()</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">NeuralNetwork(</span><br><span class="line">  (flatten): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (linear_relu_stack): Sequential(</span><br><span class="line">    (0): Linear(in_features=784, out_features=512, bias=True)</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">    (3): ReLU()</span><br><span class="line">    (4): Linear(in_features=512, out_features=10, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<h3 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h3><p>超参数是可调整的参数，可用于控制模型优化过程。不同的超参数值会影响模型训练和收敛速率（<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvaHlwZXJwYXJhbWV0ZXJfdHVuaW5nX3R1dG9yaWFsLmh0bWw=" title="https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html">阅读更多<i class="fa fa-external-link"></i></span>有关超参数优化信息）。</p>
<p>为训练定义以下超参数：</p>
<ul>
<li>纪元(epoch)数量 - 在数据上迭代的次数；</li>
<li>批量大小 - 在更新参数之前通过网络传播的数据样本数；</li>
<li>学习率 - 每个批次&#x2F;epoch更新模型参数的多少。较小的值会导致学习速度较慢，而较大的值可能会导致训练期间不可预测的行为。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = 1e-3</span><br><span class="line">batch_size = 64</span><br><span class="line">epochs = 5</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="优化循环"><a href="#优化循环" class="headerlink" title="优化循环"></a>优化循环</h3><p>设置了超参数，可以使用优化循环来训练和优化模型，优化循环的每次迭代称为一个纪元(epoch)。每个纪元由两部分组成：</p>
<ul>
<li>训练循环 - 遍历训练数据集，试图收敛到最优参数。</li>
<li>验证&#x2F;测试循环-对测试数据集进行迭代，以检查模型性能是否正在改善。</li>
</ul>
<p>地熟悉一下训练循环中使用的一些概念，跳到前面，查看优化循环的<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvYmFzaWNzL29wdGltaXphdGlvbl90dXRvcmlhbC5odG1sI2Z1bGwtaW1wbC1sYWJlbA==" title="https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#full-impl-label">Full Implementation<i class="fa fa-external-link"></i></span>。</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>当面对一些训练数据时，我们未经训练的网络很可能不会给出正确的答案。损失函数衡量所获得的结果与目标值的相似程度，是在训练过程中想要最小化的损失函数。为了计算损失，我们使用输入的数据样本进行预测，并将其与真实的数据标签值进行比较。</p>
<p>常用的损失函数包括nn.MSELoss(均方差)用于回归；nn.NLLLoss(负对数似然)用于分类。nn.CrossEntropyLoss结合了nn.LogSoftmax 和 nn.NLLLoss。</p>
<p>将模型的输出对数传递给nn.CrossEntropyLoss，将对数归一化并计算预测误差。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>


<h4 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h4><p>优化是调整模型参数以减少每个训练步骤中的模型误差的过程。优化算法定义了如何执行此过程(在此示例中，使用随机梯度下降法)。所有优化逻辑都封装在优化器对象中，这里使用SGD优化器；此外，PyTorch中还有许多<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9vcHRpbS5odG1s" title="https://pytorch.org/docs/stable/optim.html">优化器<i class="fa fa-external-link"></i></span>，比如ADAM和RMSProp，它们可以更好地处理不同类型的模型和数据。</p>
<p>通过自动记录需要训练的模型参数，并传入学习速率超参数来初始化优化器。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)</span><br></pre></td></tr></table></figure>

<p>在训练循环中，优化分为三个步骤：</p>
<ul>
<li>调用optimizer.zero_grad()重置模型参数的梯度。梯度默认情况下是叠加的；为了防止重复计算，在每次迭代时将它们归零。</li>
<li>通过调用loss.backward()反向传播预测损失。PyTorch保存每个参数的损耗w.r.t.的梯度。</li>
<li>有了梯度，调用optimizer.step()来通过反向传递中收集的梯度来调整参数。</li>
</ul>
<h3 id="全面实施"><a href="#全面实施" class="headerlink" title="全面实施"></a>全面实施</h3><p>在优化代码上定义循环train_loop，并根据测试数据评估模型性能。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def train_loop(dataloader,model,loss_fn,opimiizer):</span><br><span class="line">    size = len(dataloader.dataset)</span><br><span class="line">    for batch, (X,y) in enumerate(dataloader):</span><br><span class="line">        #计算预测和损失compute prediction and loss</span><br><span class="line">        pred = model(X)</span><br><span class="line">        loss = loss_fn(pred,y)</span><br><span class="line"></span><br><span class="line">        #反向传播Backpropagetion</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        if batch % 100 == 0:</span><br><span class="line">            loss ,current = loss.item(), batch * len(X)</span><br><span class="line">            print(f&quot;loss: &#123;loss:&gt;7f&#125; [&#123;current:&gt;5d&#125;/&#123;size:&gt;5d&#125;]&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def mytest_loop(dataloader,model,loss_fn):</span><br><span class="line">    size = len(dataloader.dataset)</span><br><span class="line">    num_batches = len(dataloader)</span><br><span class="line">    test_loss,correct = 0,0</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        for X,y in dataloader:</span><br><span class="line">            pred = model(X)</span><br><span class="line">            test_loss += loss_fn(pred, y).item()</span><br><span class="line">            correct += (pred.argmax(1) == y).type(torch.float).sum().item()</span><br><span class="line">    test_loss /= num_batches</span><br><span class="line">    correct /= size</span><br><span class="line">    print(f&quot;Test Error: \n Accuracy: &#123;(100*correct):&gt;0.1f&#125;%, Avg loss: &#123;test_loss:&gt;8f&#125; \n&quot;)</span><br></pre></td></tr></table></figure>

<p>初始化损失函数和优化器，并将其传递给train_loop和test_loop。还可以随意增加epoch的数量，以跟踪模型的改进性能。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)</span><br><span class="line"></span><br><span class="line">epochs = 10</span><br><span class="line">for t in range(epochs):</span><br><span class="line">    print(f&quot;Epoch &#123;t+1&#125;\n------------------------------------&quot;)</span><br><span class="line">    train_loop(train_dataloader,model,loss_fn,optimizer)</span><br><span class="line">    mytest_loop(test_dataloader,model,loss_fn)</span><br><span class="line">print(&quot;Done!&quot;)</span><br></pre></td></tr></table></figure>

<p>整体输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1</span><br><span class="line">------------------------------------</span><br><span class="line">loss: 2.298616 [    0/60000]</span><br><span class="line">loss: 2.286485 [ 6400/60000]</span><br><span class="line">loss: 2.270025 [12800/60000]</span><br><span class="line">loss: 2.269643 [19200/60000]</span><br><span class="line">loss: 2.253848 [25600/60000]</span><br><span class="line">loss: 2.225894 [32000/60000]</span><br><span class="line">loss: 2.233021 [38400/60000]</span><br><span class="line">loss: 2.200334 [44800/60000]</span><br><span class="line">loss: 2.193990 [51200/60000]</span><br><span class="line">loss: 2.175176 [57600/60000]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: 55.8%, Avg loss: 2.162668 </span><br><span class="line"></span><br><span class="line">Epoch 2</span><br><span class="line">------------------------------------</span><br><span class="line">loss: 2.167829 [    0/60000]</span><br><span class="line">loss: 2.157712 [ 6400/60000]</span><br><span class="line">loss: 2.098750 [12800/60000]</span><br><span class="line">loss: 2.122153 [19200/60000]</span><br><span class="line">loss: 2.079582 [25600/60000]</span><br><span class="line">loss: 2.016336 [32000/60000]</span><br><span class="line">loss: 2.051298 [38400/60000]</span><br><span class="line">loss: 1.968452 [44800/60000]</span><br><span class="line">loss: 1.977741 [51200/60000]</span><br><span class="line">loss: 1.917104 [57600/60000]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: 58.1%, Avg loss: 1.903896 </span><br><span class="line"></span><br><span class="line">Epoch 3</span><br><span class="line">------------------------------------</span><br><span class="line">loss: 1.933278 [    0/60000]</span><br><span class="line">loss: 1.899875 [ 6400/60000]</span><br><span class="line">loss: 1.783205 [12800/60000]</span><br><span class="line">loss: 1.834392 [19200/60000]</span><br><span class="line">loss: 1.734482 [25600/60000]</span><br><span class="line">loss: 1.675408 [32000/60000]</span><br><span class="line">loss: 1.713700 [38400/60000]</span><br><span class="line">loss: 1.606300 [44800/60000]</span><br><span class="line">loss: 1.643087 [51200/60000]</span><br><span class="line">loss: 1.542788 [57600/60000]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: 62.6%, Avg loss: 1.543185 </span><br><span class="line"></span><br><span class="line">Epoch 4</span><br><span class="line">------------------------------------</span><br><span class="line">loss: 1.609218 [    0/60000]</span><br><span class="line">loss: 1.570532 [ 6400/60000]</span><br><span class="line">loss: 1.422559 [12800/60000]</span><br><span class="line">loss: 1.495395 [19200/60000]</span><br><span class="line">loss: 1.385326 [25600/60000]</span><br><span class="line">loss: 1.368906 [32000/60000]</span><br><span class="line">loss: 1.391647 [38400/60000]</span><br><span class="line">loss: 1.310771 [44800/60000]</span><br><span class="line">loss: 1.353835 [51200/60000]</span><br><span class="line">loss: 1.252852 [57600/60000]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: 64.8%, Avg loss: 1.263495 </span><br><span class="line"></span><br><span class="line">Epoch 5</span><br><span class="line">------------------------------------</span><br><span class="line">loss: 1.341740 [    0/60000]</span><br><span class="line">loss: 1.325438 [ 6400/60000]</span><br><span class="line">loss: 1.159720 [12800/60000]</span><br><span class="line">loss: 1.261750 [19200/60000]</span><br><span class="line">loss: 1.144259 [25600/60000]</span><br><span class="line">loss: 1.157326 [32000/60000]</span><br><span class="line">loss: 1.183641 [38400/60000]</span><br><span class="line">loss: 1.119133 [44800/60000]</span><br><span class="line">loss: 1.165480 [51200/60000]</span><br><span class="line">loss: 1.080092 [57600/60000]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: 66.2%, Avg loss: 1.084698 </span><br><span class="line"></span><br><span class="line">Epoch 6</span><br><span class="line">------------------------------------</span><br><span class="line">loss: 1.155534 [    0/60000]</span><br><span class="line">loss: 1.166295 [ 6400/60000]</span><br><span class="line">loss: 0.982190 [12800/60000]</span><br><span class="line">loss: 1.115493 [19200/60000]</span><br><span class="line">loss: 0.992656 [25600/60000]</span><br><span class="line">loss: 1.013594 [32000/60000]</span><br><span class="line">loss: 1.055350 [38400/60000]</span><br><span class="line">loss: 0.997410 [44800/60000]</span><br><span class="line">loss: 1.044053 [51200/60000]</span><br><span class="line">loss: 0.973711 [57600/60000]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: 67.4%, Avg loss: 0.970132 </span><br><span class="line"></span><br><span class="line">Epoch 7</span><br><span class="line">------------------------------------</span><br><span class="line">loss: 1.027112 [    0/60000]</span><br><span class="line">loss: 1.064346 [ 6400/60000]</span><br><span class="line">loss: 0.861960 [12800/60000]</span><br><span class="line">loss: 1.019788 [19200/60000]</span><br><span class="line">loss: 0.897487 [25600/60000]</span><br><span class="line">loss: 0.914093 [32000/60000]</span><br><span class="line">loss: 0.973150 [38400/60000]</span><br><span class="line">loss: 0.919831 [44800/60000]</span><br><span class="line">loss: 0.962260 [51200/60000]</span><br><span class="line">loss: 0.904603 [57600/60000]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: 68.6%, Avg loss: 0.893350 </span><br><span class="line"></span><br><span class="line">Epoch 8</span><br><span class="line">------------------------------------</span><br><span class="line">loss: 0.934642 [    0/60000]</span><br><span class="line">loss: 0.994998 [ 6400/60000]</span><br><span class="line">loss: 0.776983 [12800/60000]</span><br><span class="line">loss: 0.953107 [19200/60000]</span><br><span class="line">loss: 0.834409 [25600/60000]</span><br><span class="line">loss: 0.842692 [32000/60000]</span><br><span class="line">loss: 0.916326 [38400/60000]</span><br><span class="line">loss: 0.868921 [44800/60000]</span><br><span class="line">loss: 0.904816 [51200/60000]</span><br><span class="line">loss: 0.856013 [57600/60000]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: 69.8%, Avg loss: 0.838920 </span><br><span class="line"></span><br><span class="line">Epoch 9</span><br><span class="line">------------------------------------</span><br><span class="line">loss: 0.865317 [    0/60000]</span><br><span class="line">loss: 0.943758 [ 6400/60000]</span><br><span class="line">loss: 0.714329 [12800/60000]</span><br><span class="line">loss: 0.903987 [19200/60000]</span><br><span class="line">loss: 0.789997 [25600/60000]</span><br><span class="line">loss: 0.789787 [32000/60000]</span><br><span class="line">loss: 0.874063 [38400/60000]</span><br><span class="line">loss: 0.833949 [44800/60000]</span><br><span class="line">loss: 0.862678 [51200/60000]</span><br><span class="line">loss: 0.819333 [57600/60000]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: 71.0%, Avg loss: 0.798254 </span><br><span class="line"></span><br><span class="line">Epoch 10</span><br><span class="line">------------------------------------</span><br><span class="line">loss: 0.811143 [    0/60000]</span><br><span class="line">loss: 0.902883 [ 6400/60000]</span><br><span class="line">loss: 0.666282 [12800/60000]</span><br><span class="line">loss: 0.866518 [19200/60000]</span><br><span class="line">loss: 0.756904 [25600/60000]</span><br><span class="line">loss: 0.749717 [32000/60000]</span><br><span class="line">loss: 0.840520 [38400/60000]</span><br><span class="line">loss: 0.808478 [44800/60000]</span><br><span class="line">loss: 0.830349 [51200/60000]</span><br><span class="line">loss: 0.790129 [57600/60000]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: 72.3%, Avg loss: 0.766204 </span><br><span class="line"></span><br><span class="line">Done!</span><br></pre></td></tr></table></figure>


<h3 id="延伸阅读-4"><a href="#延伸阅读-4" class="headerlink" title="延伸阅读"></a>延伸阅读</h3><ul>
<li><span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9ubi5odG1sI2xvc3MtZnVuY3Rpb25z" title="https://pytorch.org/docs/stable/nn.html#loss-functions">损失函数<i class="fa fa-external-link"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9vcHRpbS5odG1s" title="https://pytorch.org/docs/stable/optim.html">torch.optim<i class="fa fa-external-link"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy90dXRvcmlhbHMvcmVjaXBlcy9yZWNpcGVzL3dhcm1zdGFydGluZ19tb2RlbF91c2luZ19wYXJhbWV0ZXJzX2Zyb21fYV9kaWZmZXJlbnRfbW9kZWwuaHRtbA==" title="https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html">Warmstart Training a Model<i class="fa fa-external-link"></i></span></li>
</ul>
<h2 id="1-7-保存-加载模型"><a href="#1-7-保存-加载模型" class="headerlink" title="1.7 保存&amp;加载模型"></a>1.7 保存&amp;加载模型</h2><h3 id="保存并加载模型"><a href="#保存并加载模型" class="headerlink" title="保存并加载模型"></a>保存并加载模型</h3><p>本节介绍如何通过保存、加载、运行模型预测来保持模型状态。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision.models as models</span><br></pre></td></tr></table></figure>


<h3 id="保存和加载模型权重"><a href="#保存和加载模型权重" class="headerlink" title="保存和加载模型权重"></a>保存和加载模型权重</h3><p>PyTorch模型将学习到的参数存储在一个名为state_dict的内部状态字典中，这些可以通过torch.save方法持久化:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = models.vgg16(pretrained=True)</span><br><span class="line">torch.save(model.state_dict(),&#x27;model_weights.pth&#x27;)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">C:\ProgramData\Anaconda3\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter &#x27;pretrained&#x27; is deprecated since 0.13 and will be removed in 0.15, please use &#x27;weights&#x27; instead.</span><br><span class="line">  warnings.warn(</span><br><span class="line">C:\ProgramData\Anaconda3\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for &#x27;weights&#x27; are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.</span><br><span class="line">  warnings.warn(msg)</span><br><span class="line">Downloading: &quot;https://download.pytorch.org/models/vgg16-397923af.pth&quot; to C:\Users\sy/.cache\torch\hub\checkpoints\vgg16-397923af.pth</span><br><span class="line">100%|██████████| 528M/528M [00:28&lt;00:00, 19.2MB/s]</span><br></pre></td></tr></table></figure>

<p>要加载模型权重，需要先创建同一个模型的实例，然后使用load_state_dict()方法加载参数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = models.vgg16()    #不指定pretrained=True,是不加载默认权重</span><br><span class="line">model.load_state_dict(torch.load(&#x27;model_weights.pth&#x27;))</span><br><span class="line">model.eval()</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">请务必在推理之前调用方法model.eval()，以将丢弃层和批处理归一化层设置为评估模式。如果不这样做，将产生不一致的推理结果。</span><br></pre></td></tr></table></figure>


<h3 id="Saving-and-Loading-Models-with-Shapes"><a href="#Saving-and-Loading-Models-with-Shapes" class="headerlink" title="Saving and Loading Models with Shapes"></a>Saving and Loading Models with Shapes</h3><p>加载模型权重时，需要先实例化模型类，因为该类定义了网络的结构。如果将此类的结构与模型一起保存，在这种情况下，可以将model (而不是model.state_dict()) 传递给save函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model,&#x27;model.pth&#x27;)</span><br></pre></td></tr></table></figure>
<p>然后可以这样加载模型:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.load(&#x27;model.pth&#x27;)</span><br></pre></td></tr></table></figure>


<h3 id="延伸阅读-5"><a href="#延伸阅读-5" class="headerlink" title="延伸阅读"></a>延伸阅读</h3><p><span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy90dXRvcmlhbHMvcmVjaXBlcy9yZWNpcGVzL3NhdmluZ19hbmRfbG9hZGluZ19hX2dlbmVyYWxfY2hlY2twb2ludC5odG1s" title="https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html">Saving and Loading a General Checkpoint in PyTorch<i class="fa fa-external-link"></i></span></p>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="SindreYang 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="SindreYang 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>SindreYang
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://blog.mviai.com/2025/1._%E7%86%9F%E6%82%89pytorch_----__1_%E7%AE%80%E4%BB%8B/" title="1">http://blog.mviai.com/2025/1._熟悉pytorch_----__1_简介/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/images/wechat.png">
            <span class="icon">
              <i class="fa fa-wechat"></i>
            </span>

            <span class="label">WeChat</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/1._%E7%86%9F%E6%82%89pytorch_----__2_pytorch%E7%A4%BA%E4%BE%8B/" rel="prev" title="1">
      <i class="fa fa-chevron-left"></i> 1
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/1._%E7%86%9F%E6%82%89Markdown%E8%AF%AD%E6%B3%95/" rel="next" title="1">
      1 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="SOHUCS"></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#0-%E9%A2%84%E5%A4%87"><span class="nav-number">1.</span> <span class="nav-text">0 预备</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Anaconda%E5%AE%89%E8%A3%85"><span class="nav-number">1.1.</span> <span class="nav-text">Anaconda安装</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#conda%E8%BF%9B%E5%85%A5base%E5%9F%BA%E6%9C%AC%E7%8E%AF%E5%A2%83"><span class="nav-number">1.1.1.</span> <span class="nav-text">conda进入base基本环境</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#conda%E9%80%80%E5%87%BAbase%E7%8E%AF%E5%A2%83%E5%8F%8A%E5%85%B6%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4"><span class="nav-number">1.1.2.</span> <span class="nav-text">conda退出base环境及其常用命令</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%A1%E7%90%86%E5%91%98windows-powershell%E8%BF%9B%E5%85%A5base%E7%8E%AF%E5%A2%83"><span class="nav-number">1.2.</span> <span class="nav-text">管理员windows powershell进入base环境</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%96%E6%B6%88%E6%89%93%E5%BC%80%E7%AE%A1%E7%90%86%E5%91%98%E5%B0%B1%E8%BF%9B%E5%85%A5conda-base%E7%8E%AF%E5%A2%83%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="nav-number">1.3.</span> <span class="nav-text">取消打开管理员就进入conda base环境的方法：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sudo%E5%AE%89%E8%A3%85"><span class="nav-number">1.4.</span> <span class="nav-text">sudo安装</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pycharm%E9%85%8D%E7%BD%AE%E8%AE%BE%E7%BD%AE"><span class="nav-number">1.5.</span> <span class="nav-text">pycharm配置设置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pycharm%E4%B8%AD%E5%88%A0%E9%99%A4%E5%B7%B2%E6%9C%89%E7%9A%84python%E8%A7%A3%E6%9E%90%E5%99%A8%EF%BC%9A"><span class="nav-number">1.6.</span> <span class="nav-text">pycharm中删除已有的python解析器：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#python%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1"><span class="nav-number">1.7.</span> <span class="nav-text">python面向对象</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B1%BB%E5%AE%9A%E4%B9%89"><span class="nav-number">1.7.1.</span> <span class="nav-text">类定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B1%BB%E5%AF%B9%E8%B1%A1%E5%92%8C-init"><span class="nav-number">1.7.2.</span> <span class="nav-text">类对象和__init__</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#self%E4%BB%A3%E8%A1%A8%E7%B1%BB%E7%9A%84%E5%AE%9E%E4%BE%8B%EF%BC%8C%E8%80%8C%E9%9D%9E%E7%B1%BB"><span class="nav-number">1.7.3.</span> <span class="nav-text">self代表类的实例，而非类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B1%BB%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">1.7.4.</span> <span class="nav-text">类的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%A7%E6%89%BF"><span class="nav-number">1.7.5.</span> <span class="nav-text">继承</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E7%BB%A7%E6%89%BF"><span class="nav-number">1.7.6.</span> <span class="nav-text">多继承</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E9%87%8D%E5%86%99"><span class="nav-number">1.7.7.</span> <span class="nav-text">方法重写</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B1%BB%E5%B1%9E%E6%80%A7%E4%B8%8E%E6%96%B9%E6%B3%95"><span class="nav-number">1.7.8.</span> <span class="nav-text">类属性与方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B1%BB%E7%9A%84%E4%B8%93%E6%9C%89%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="nav-number">1.7.9.</span> <span class="nav-text">类的专有方法：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%90%E7%AE%97%E7%AC%A6%E9%87%8D%E8%BD%BD"><span class="nav-number">1.7.10.</span> <span class="nav-text">运算符重载</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Matplotlib"><span class="nav-number">1.8.</span> <span class="nav-text">Matplotlib</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#matplotlib%E5%AE%89%E8%A3%85"><span class="nav-number">1.8.1.</span> <span class="nav-text">matplotlib安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Matplotlib-Pyplot"><span class="nav-number">1.8.2.</span> <span class="nav-text">Matplotlib Pyplot</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Matplotlib%E7%BB%98%E5%9B%BE%E6%A0%87%E8%AE%B0"><span class="nav-number">1.8.3.</span> <span class="nav-text">Matplotlib绘图标记</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#fmt%E5%8F%82%E6%95%B0"><span class="nav-number">1.8.3.1.</span> <span class="nav-text">fmt参数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Matplotlib%E7%BB%98%E5%9B%BE%E7%BA%BF"><span class="nav-number">1.8.4.</span> <span class="nav-text">Matplotlib绘图线</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="nav-number">1.8.4.1.</span> <span class="nav-text">线的类型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E7%9A%84%E9%A2%9C%E8%89%B2"><span class="nav-number">1.8.4.2.</span> <span class="nav-text">线的颜色</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85pytorch"><span class="nav-number">1.9.</span> <span class="nav-text">安装pytorch</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A3%80%E6%9F%A5%E6%98%AF%E5%90%A6%E6%9C%89%E5%90%88%E9%80%82%E7%9A%84GPU"><span class="nav-number">1.9.1.</span> <span class="nav-text">检查是否有合适的GPU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85pytorch-1"><span class="nav-number">1.9.2.</span> <span class="nav-text">安装pytorch</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-pytorch%E7%AE%80%E4%BB%8B"><span class="nav-number">2.</span> <span class="nav-text">1 pytorch简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-%E5%BC%A0%E9%87%8F"><span class="nav-number">2.1.</span> <span class="nav-text">1.1 张量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%BC%A0%E9%87%8F"><span class="nav-number">2.1.1.</span> <span class="nav-text">初始化张量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%B4%E6%8E%A5%E4%BB%8E%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%88%9B%E5%BB%BA"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">直接从数据中创建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8ENumpy%E6%95%B0%E7%BB%84%E4%B8%AD%E5%88%9B%E5%BB%BA"><span class="nav-number">2.1.1.2.</span> <span class="nav-text">从Numpy数组中创建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8E%E5%85%B6%E4%BB%96%E5%BC%A0%E9%87%8F%E5%88%9B%E5%BB%BA"><span class="nav-number">2.1.1.3.</span> <span class="nav-text">从其他张量创建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%88%96%E5%B8%B8%E9%87%8F%E5%80%BC"><span class="nav-number">2.1.1.4.</span> <span class="nav-text">随机或常量值</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E5%B1%9E%E6%80%A7"><span class="nav-number">2.1.2.</span> <span class="nav-text">张量属性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E8%BF%90%E7%AE%97"><span class="nav-number">2.1.3.</span> <span class="nav-text">张量运算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E5%8C%96%E7%9A%84numpy%E7%B1%BB%E7%B4%A2%E5%BC%95%E5%92%8C%E5%88%87%E7%89%87"><span class="nav-number">2.1.3.1.</span> <span class="nav-text">标准化的numpy类索引和切片</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Arithmetic-operations%E7%AE%97%E6%95%B0%E8%BF%90%E7%AE%97"><span class="nav-number">2.1.3.2.</span> <span class="nav-text">Arithmetic operations算数运算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Single-element-tensors%E5%8D%95%E5%85%83%E5%BC%A0%E9%87%8F"><span class="nav-number">2.1.3.3.</span> <span class="nav-text">Single-element tensors单元张量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#In-place-operations%E5%B0%B1%E5%9C%B0%E6%93%8D%E4%BD%9C"><span class="nav-number">2.1.3.4.</span> <span class="nav-text">In-place operations就地操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bridge-with-NumPy"><span class="nav-number">2.1.3.5.</span> <span class="nav-text">Bridge with NumPy</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%99%A8"><span class="nav-number">2.2.</span> <span class="nav-text">1.2 数据集和数据加载器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8A%A0%E8%BD%BD"><span class="nav-number">2.2.1.</span> <span class="nav-text">数据集加载</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%AD%E4%BB%A3%E5%92%8C%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">2.2.2.</span> <span class="nav-text">迭代和可视化数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">2.2.3.</span> <span class="nav-text">自定义数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#init"><span class="nav-number">2.2.3.1.</span> <span class="nav-text">init</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#len"><span class="nav-number">2.2.3.2.</span> <span class="nav-text">len</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#getitem"><span class="nav-number">2.2.3.3.</span> <span class="nav-text">getitem</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%A8%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%99%A8%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83"><span class="nav-number">2.2.4.</span> <span class="nav-text">用数据加载器数据进行训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E8%AE%BF%E9%97%AE%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%99%A8"><span class="nav-number">2.2.5.</span> <span class="nav-text">循环访问数据加载器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB"><span class="nav-number">2.2.6.</span> <span class="nav-text">延伸阅读</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-Transform"><span class="nav-number">2.3.</span> <span class="nav-text">1.3 Transform</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ToTensor"><span class="nav-number">2.3.1.</span> <span class="nav-text">ToTensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lambda-Transforms"><span class="nav-number">2.3.2.</span> <span class="nav-text">Lambda Transforms</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-1"><span class="nav-number">2.3.3.</span> <span class="nav-text">延伸阅读</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.4.</span> <span class="nav-text">1.4 构建模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.4.1.</span> <span class="nav-text">构建神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-Layers"><span class="nav-number">2.4.2.</span> <span class="nav-text">Model Layers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nn-Flatten"><span class="nav-number">2.4.3.</span> <span class="nav-text">nn.Flatten</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nn-Linear"><span class="nav-number">2.4.4.</span> <span class="nav-text">nn.Linear</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nn-ReLU"><span class="nav-number">2.4.5.</span> <span class="nav-text">nn.ReLU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nn-Sequential"><span class="nav-number">2.4.6.</span> <span class="nav-text">nn.Sequential</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nn-Softmax"><span class="nav-number">2.4.7.</span> <span class="nav-text">nn.Softmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="nav-number">2.4.8.</span> <span class="nav-text">模型参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-2"><span class="nav-number">2.4.9.</span> <span class="nav-text">延伸阅读</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BCAutograd"><span class="nav-number">2.5.</span> <span class="nav-text">1.5 自动求导Autograd</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensors-Functions-and-Computational-graph"><span class="nav-number">2.5.1.</span> <span class="nav-text">Tensors, Functions and Computational graph</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="nav-number">2.5.2.</span> <span class="nav-text">梯度计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A6%81%E7%94%A8%E6%A2%AF%E5%BA%A6%E8%B7%9F%E8%B8%AADisabling-Gradient-Tracking"><span class="nav-number">2.5.3.</span> <span class="nav-text">禁用梯度跟踪Disabling Gradient Tracking</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%89%E5%85%B3%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%9A%84%E6%9B%B4%E5%A4%9A%E4%BF%A1%E6%81%AF"><span class="nav-number">2.5.4.</span> <span class="nav-text">有关计算图的更多信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%89%E8%AF%BB%EF%BC%9A%E5%BC%A0%E9%87%8F%E6%A2%AF%E5%BA%A6%E5%92%8C%E9%9B%85%E5%8F%AF%E6%AF%94%E7%A7%AF"><span class="nav-number">2.5.5.</span> <span class="nav-text">选读：张量梯度和雅可比积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-3"><span class="nav-number">2.5.6.</span> <span class="nav-text">延伸阅读</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-6-%E4%BC%98%E5%8C%96Optimization"><span class="nav-number">2.6.</span> <span class="nav-text">1.6 优化Optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="nav-number">2.6.1.</span> <span class="nav-text">优化模型参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E6%8F%90%E4%BB%A3%E7%A0%81"><span class="nav-number">2.6.2.</span> <span class="nav-text">前提代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-number">2.6.3.</span> <span class="nav-text">超参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%BE%AA%E7%8E%AF"><span class="nav-number">2.6.4.</span> <span class="nav-text">优化循环</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">2.6.4.1.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="nav-number">2.6.4.2.</span> <span class="nav-text">优化器</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%A8%E9%9D%A2%E5%AE%9E%E6%96%BD"><span class="nav-number">2.6.5.</span> <span class="nav-text">全面实施</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-4"><span class="nav-number">2.6.6.</span> <span class="nav-text">延伸阅读</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-7-%E4%BF%9D%E5%AD%98-%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.7.</span> <span class="nav-text">1.7 保存&amp;加载模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%9D%E5%AD%98%E5%B9%B6%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.7.1.</span> <span class="nav-text">保存并加载模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D"><span class="nav-number">2.7.2.</span> <span class="nav-text">保存和加载模型权重</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Saving-and-Loading-Models-with-Shapes"><span class="nav-number">2.7.3.</span> <span class="nav-text">Saving and Loading Models with Shapes</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-5"><span class="nav-number">2.7.4.</span> <span class="nav-text">延伸阅读</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SindreYang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">SindreYang</p>
  <div class="site-description" itemprop="description">沉淀后我愿意做一个温暖的人。有自己的喜好，有自己的原则，有自己的信仰，不急功近利，不浮夸轻薄，宠辱不惊，淡定安逸，心静如水。------不忘初心，方得始终</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">321</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1NpbmRyZVlhbmc=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;SindreYang"><i class="fa fa-fw fa-github"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnl4QG12aWFpLmNvbQ==" title="E-Mail → mailto:yx@mviai.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        
  <div class="languages">
    <label class="lang-select-label">
      <i class="fa fa-language"></i>
      <span>简体中文</span>
      <i class="fa fa-angle-up" aria-hidden="true"></i>
    </label>
    <select class="lang-select" data-canonical="">
      
        <option value="zh-CN" data-href="/2025/1._%E7%86%9F%E6%82%89pytorch_----__1_%E7%AE%80%E4%BB%8B/" selected="">
          简体中文
        </option>
      
        <option value="en" data-href="/en/2025/1._%E7%86%9F%E6%82%89pytorch_----__1_%E7%AE%80%E4%BB%8B/" selected="">
          English
        </option>
      
    </select>
  </div>

        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SindreYang</span>
</div><!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/love.js"></script>
<!-- 背景波浪 -->
<script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>


<!-- 腾讯企业邮箱 -->
<style>
.bizmail_loginpanel {
    font-size: 12px;
    width: 300px;
    height: auto;
    background: transparent;
    margin-left: auto;
    margin-right: auto;
}

.bizmail_LoginBox {
    padding: 10px 15px;
}


.bizmail_loginpanel form {
    margin: 0;
    padding: 0;
}

.bizmail_loginpanel input.text {
    font-size: 12px;
    width: 100px;
    height: 20px;
    margin: 0 2px;
    background-color: transparent;
    border:1px solid transparent;
    box-shadow: none;
    color: black;
}

.bizmail_loginpanel .bizmail_column {
    height: 28px;
}

.bizmail_loginpanel .bizmail_column label {
    display: block;
    float: left;
    width: 30px;
    height: 24px;
    line-height: 24px;
    font-size: 12px;
}

.bizmail_loginpanel .bizmail_column .bizmail_inputArea {
    float: left;
    width: 240px;
}

.bizmail_loginpanel .bizmail_column span {
    font-size: 12px;
    word-wrap: break-word;
    margin-left: 2px;
    line-height: 200%;
}

.bizmail_loginpanel .bizmail_SubmitArea {
    margin-left: 30px;
    clear: both;
}

.bizmail_loginpanel .bizmail_SubmitArea a {
    font-size: 12px;
    margin-left: 5px;
}

.bizmail_loginpanel select {
    width: 110px;
    height: 20px;
    margin: 0 2px;
}
.bizmail_loginpanel input {

    background-color: rgba(83, 126, 236, 0.562);
}


</style>

<script type="text/javascript">
function checkInput() {
    var e = document.form1.uin,
        i = document.form1.pwd;
    return 0 == e.value.length ? e.focus() : 0 == i.value.length ? i.focus() : (document.form1.submit(), setTimeout(" document.form1.pwd.value = '' ", 500)), !1
}

function writeLoginPanel(e) {
    if (e && e.domainlist && -1 != e.domainlist.indexOf(".")) {
        var a = "return checkInput()",
            t = '<div id="divLoginpanelHor" class="bizmail_loginpanel" style="width:550px;"><div class="bizmail_LoginBox"><form name="form1" action="https://exmail.qq.com/cgi-bin/login" target="_blank" method="post" onsubmit="' + a + '"><input type="hidden" name="firstlogin" value="false" /><input type="hidden" name="errtemplate" value="dm_loginpage" /><input type="hidden" name="aliastype" value="other" /><input type="hidden" name="dmtype" value="bizmail" /><input type="hidden" name="p" value="" /><label>\u8d26\u53f7:</label><input type="text" name="uin" class="text" value="" />@#domainlist#<label>&nbsp&nbsp&nbsp;\u5bc6\u7801:</label><input type="password" name="pwd" class="text" value="" /><input type="submit" class="" name="" value="\u767b\u5f55" />&nbsp;<a href="https://exmail.qq.com/cgi-bin/readtemplate?check=false&t=biz_rf_portal#recovery" target="_blank">\u5fd8\u8bb0\u5bc6\u7801\uff1f</a></form></div></div>',
            n = '<div id="divLoginpanelVer" class="bizmail_loginpanel"><div class="bizmail_LoginBox"><form name="form1" action="https://exmail.qq.com/cgi-bin/login" target="_blank" method="post" onsubmit="' + a + '"><input type="hidden" name="firstlogin" value="false" /><input type="hidden" name="errtemplate" value="dm_loginpage" /><input type="hidden" name="aliastype" value="other" /><input type="hidden" name="dmtype" value="bizmail" /><input type="hidden" name="p" value="" /><div class="bizmail_column"><label>\u8d26\u53f7:</label><div class="bizmail_inputArea"><input type="text" name="uin" class="text" value="" />@#domainlist#</div></div><div class="bizmail_column"><label>\u5bc6\u7801:</label><div class="bizmail_inputArea"><input type="password" name="pwd" class="text" value="" /></div></div><div class="bizmail_SubmitArea"><input type="submit" class="" name="" style="width:66px;" value="\u767b\u5f55" /><a href="https://exmail.qq.com/cgi-bin/readtemplate?check=false&t=biz_rf_portal#recovery" target="_blank">\u5fd8\u8bb0\u5bc6\u7801\uff1f</a></div></form></div></div>',
            l = e.domainlist.split(";");
        if (1 == l.length) var m = '<span>#domain#</span><input type="hidden" name="domain" value="#domain#" />'.replace(/#domain#/g, l[0]);
        else {
            m = '<select name="domain">';
            for (i = 0; i < l.length; i++) m += '<option value="' + l[i] + '">' + l[i] + "</option>";
            m += "</select>"
        }
        e.mode && "vertical" != e.mode && "both" != e.mode || document.write(n.replace(/#domainlist#/g, m)), "horizontal" != e.mode && "both" != e.mode || document.write(t.replace(/#domainlist#/g, m))
    }
}

</script>      

<script type="text/javascript"> writeLoginPanel({domainlist:"mviai.com", mode:"horizontal"});</script>      


        








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>


  




  
<script src="/js/local-search.js"></script>









<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  

  <script>
  NexT.utils.loadComments(document.querySelector('#SOHUCS'), () => {
    var appid = 'cyxmItxjS';
    var conf = 'e5e71132d9086bb54aeeba6e88e87df9';
    var width = window.innerWidth || document.documentElement.clientWidth;
    if (width < 960) {
      window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://cy-cdn.kuaizhan.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>');
    } else {
      var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:appid,conf:conf})});
    }
  });
  </script>
  <script src="https://cy-cdn.kuaizhan.com/upload/plugins/plugins.count.js"></script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"left","width":75,"height":150},"mobile":{"show":true},"log":false});</script></body>
</html>




