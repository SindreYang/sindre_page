<!DOCTYPE html>
<html lang="zh-CN,en,default">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.mviai.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":true,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="自动摘要: 	链接：	DeepLearningfor3DPointClouds:ASurvey[https:&#x2F;&#x2F;gith ……..">
<meta property="og:type" content="article">
<meta property="og:title" content="论文翻译__3D点云深度学习综述-2020年">
<meta property="og:url" content="http://blog.mviai.com/2025/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91__3D%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0-2020%E5%B9%B4/index.html">
<meta property="og:site_name" content="落叶无痕">
<meta property="og:description" content="自动摘要: 	链接：	DeepLearningfor3DPointClouds:ASurvey[https:&#x2F;&#x2F;gith ……..">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://blog.mviai.com/images/1701073208220-2fb6ece3-cf94-4ce7-bf54-84f0debe8fa3.jpeg">
<meta property="og:image" content="http://blog.mviai.com/images/1701073314650-5ebd16e5-2136-44da-8e3d-eee75da97e0a.png">
<meta property="og:image" content="http://blog.mviai.com/images/1701075279419-109e5411-947f-4b14-8a75-f1749fa9cdca.png">
<meta property="og:image" content="http://blog.mviai.com/images/1701075728911-31b00d7e-aa28-4215-acf6-baf45ed2e694.png">
<meta property="og:image" content="http://blog.mviai.com/images/1701076204216-e723b6c6-3f47-4e11-8425-8badf95a98c7.png">
<meta property="og:image" content="http://blog.mviai.com/images/1701138227438-d0e116e7-0189-4d75-a355-2638b140f6b0.png">
<meta property="og:image" content="http://blog.mviai.com/images/1701142595696-d6953899-9f3d-4a17-bd66-033b0c7b1c28.png">
<meta property="og:image" content="http://blog.mviai.com/images/1701142654670-71e17a04-aaa1-4a73-99de-3da7c17d2a03.png">
<meta property="og:image" content="http://blog.mviai.com/images/1701310839390-70a95e97-7e7b-439b-9fdf-d6b2041c3624.png">
<meta property="og:image" content="http://blog.mviai.com/images/1701154025889-ee9bd3d9-b476-4ccf-8efb-1e48b92626d3.png">
<meta property="og:image" content="http://blog.mviai.com/images/94e79ad0c1aabeafef9e2fc4af6adf66.svg">
<meta property="og:image" content="http://blog.mviai.com/images/6204886f5cc39a4b860ea98a7e95af1d.svg">
<meta property="og:image" content="http://blog.mviai.com/images/6204886f5cc39a4b860ea98a7e95af1d.svg">
<meta property="og:image" content="http://blog.mviai.com/images/04272d36bac122a9e96b2ca42116dca4.svg">
<meta property="og:image" content="http://blog.mviai.com/images/1701243339739-524c9664-1203-430b-97b9-3ad2a08c877f.png">
<meta property="og:image" content="http://blog.mviai.com/images/1701243836433-ee84fa7f-4b94-43d3-8a30-468250593e35.png">
<meta property="og:image" content="http://blog.mviai.com/images/1701243857825-ab0293b6-e0fe-46ce-a0e1-0ebea7f4fc3b.png">
<meta property="og:image" content="http://blog.mviai.com/images/1701245931607-54e4b3be-f89a-4555-b238-a29f512807fb.png">
<meta property="og:image" content="http://blog.mviai.com/images/1701310801343-6bb38215-4a6c-481b-a922-9c1480150799.png">
<meta property="article:published_time" content="2025-01-22T04:37:39.000Z">
<meta property="article:modified_time" content="2025-01-22T12:37:39.940Z">
<meta property="article:author" content="SindreYang">
<meta property="article:tag" content="生活">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.mviai.com/images/1701073208220-2fb6ece3-cf94-4ce7-bf54-84f0debe8fa3.jpeg">

<link rel="canonical" href="http://blog.mviai.com/2025/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91__3D%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0-2020%E5%B9%B4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>论文翻译__3D点云深度学习综述-2020年 | 落叶无痕</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">落叶无痕</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">72</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">321</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <span class="exturl github-corner" data-url="aHR0cHM6Ly9naXRodWIuY29tL1NpbmRyZVlhbmc=" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></span>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.mviai.com/2025/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91__3D%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0-2020%E5%B9%B4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="SindreYang">
      <meta itemprop="description" content="沉淀后我愿意做一个温暖的人。有自己的喜好，有自己的原则，有自己的信仰，不急功近利，不浮夸轻薄，宠辱不惊，淡定安逸，心静如水。------不忘初心，方得始终">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="落叶无痕">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          论文翻译__3D点云深度学习综述-2020年
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-01-22 12:37:39 / 修改时间：20:37:39" itemprop="dateCreated datePublished" datetime="2025-01-22T12:37:39+08:00">2025-01-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%82%B9%E4%BA%91%E7%B3%BB%E5%88%97/" itemprop="url" rel="index"><span itemprop="name">点云系列</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Changyan：</span>
    
    
      <a title="changyan" href="/2025/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91__3D%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0-2020%E5%B9%B4/#SOHUCS" itemprop="discussionUrl">
        <span id="changyan_count_unit" class="post-comments-count hc-comment-count" data-xid="2025/论文翻译__3D点云深度学习综述-2020年/" itemprop="commentCount"></span>
      </a>
    
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>自动摘要: 	链接：	<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MTIuMTIwMzM=" title="https://arxiv.org/abs/1912.12033">DeepLearningfor3DPointClouds:ASurvey<i class="fa fa-external-link"></i></span>[<span class="exturl" data-url="aHR0cHM6Ly9naXRoLw==" title="https://gith/">https://gith<i class="fa fa-external-link"></i></span> ……..</p>
<span id="more"></span>

<h1 id="链接："><a href="#链接：" class="headerlink" title="链接："></a>链接：</h1><p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MTIuMTIwMzM=" title="https://arxiv.org/abs/1912.12033">Deep Learning for 3D Point Clouds: A Survey<i class="fa fa-external-link"></i></span><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1Fpbmd5b25nSHUvU29UQS1Qb2ludC1DbG91ZA==" title="https://github.com/QingyongHu/SoTA-Point-Cloud">https://github.com/QingyongHu/SoTA-Point-Cloud<i class="fa fa-external-link"></i></span></p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>随着3D采集技术的快速发展，3D传感器越来越容易获得，价格也越来越低，包括各种类型的3D扫描仪、激光雷达和RGB-D相机（如Kinect、RealSense和苹果深度相机）[1]。这些传感器采集的3D数据可以提供丰富的几何、形状和尺度信息[2]，[3]。3D 数据与 2D 图像相辅相成，为机器更好地了解周围环境提供了机会。3D数据在自动驾驶、机器人、遥感和医疗等不同领域有着广泛的应用[4]。3D 数据通常可以用不同的格式表示，包括深度图像、点云、网格和体素网格。作为一种常用的格式，点云表示在 3D 空间中保留了原始几何信息，而无需任何离散化。因此，它是许多场景理解相关应用（如自动驾驶和机器人）的首选表示。最近，深度学习技术在许多研究领域占据主导地位，例如计算机视觉、语音识别和自然语言处理。然而，基于3D点云的深度学习仍面临数据集规模小、三维点云高维、非结构化等重大挑战[5]。在此基础上，本文重点分析了用于处理三维点云的深度学习方法。点云上的深度学习越来越受到关注，尤其是在过去五年中。还发布了几个公开可用的数据集，例如ModelNet [6], ScanObjectNN [7], ShapeNet [8], PartNet [9],S3DIS [10], ScanNet [11], Semantic3D [12], ApolloCar3D[13],和KITTI  视觉基准套件 [14], [15]. 这些数据集进一步推动了3D点云深度学习的研究，越来越多的方法被提出来解决与点云处理相关的各种问题，包括3D形状分类、3D目标检测和跟踪、3D点云分割、3D点云配准、6自由度位姿估计和3D重建[16]。 [17], [18].关于3D数据深度学习的调查也很少，如[19]、[20]、[21]、[22]。然而，我们的论文是第一个专门关注点云理解的深度学习方法的论文。图 1 显示了现有 3D 点云深度学习方法的分类。</p>
<p><img src="/images/1701073208220-2fb6ece3-cf94-4ce7-bf54-84f0debe8fa3.jpeg"><br><img src="/images/1701073314650-5ebd16e5-2136-44da-8e3d-eee75da97e0a.png"><br>图1：3D点云的深度学习方法分类</p>
<p>与现有文献相比，这项工作的主要贡献可以总结如下：</p>
<ol>
<li>据我们所知，这是第一篇全面涵盖几个重要点云理解任务的深度学习方法的调查论文，包括 3D 形状分类、3D 目标检测和跟踪以及 3D 点云分割。</li>
<li>与现有的综述[19]，[20]相反，我们特别关注3D点云的深度学习方法，而不是所有类型的3D数据。</li>
<li>本文介绍了点云深度学习的最新进展。因此，它为读者提供了最先进的方法。</li>
<li>对现有方法的综合比较，提供了几个公开可用的数据集（例如，在表2、3、4、5中），并提供了简短的总结和有见地的讨论。</li>
</ol>
<p>本文的结构如下。 第 2 节介绍了相应任务的数据集和评估指标。第 3 节回顾了 3D 形状分类的方法。第 4 节介绍了用于 3D 目标检测和跟踪的现有方法。第5节回顾了点云分割的方法，包括语义分割、实例分割和部分分割。最后，第6节结束了本文。我们还提供定期更新的项目页面：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1Fpbmd5b25nSHUvU29UQS1Qb2ludC1DbG91ZCVFMyU4MCU4Mg==" title="https://github.com/QingyongHu/SoTA-Point-Cloud%E3%80%82">https://github.com/QingyongHu/SoTA-Point-Cloud。<i class="fa fa-external-link"></i></span></p>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>已经收集了大量的数据集来评估深度学习算法在不同3D点云应用中的性能。表 1 列出了一些用于 3D 形状分类、3D 目标检测和跟踪以及 3D 点云分割的典型数据集。特别是，还总结了这些数据集的属性。</p>
<ol>
<li>对于3D形状分类，有两种类型的数据集：<ol>
<li>synthetic datasets（合成数据集） [6], [8] and real-world datasets（真实世界数据集）[7]、[11]。<ol>
<li>synthetic datasets中的对象是完整的，没有任何遮挡和背景。相比之下，and real-world datasets中的对象在不同级别被遮挡，并且某些对象被背景噪声污染。</li>
</ol>
</li>
</ol>
</li>
<li>对于3D目标检测和跟踪，有两种类型的数据集：<ol>
<li>indoor scenes （室内场景）[11], [25] 和 outdoor urban scene（室外城市场景）[14]、[28]、[30]、[31]<ol>
<li>indoor scenes 的点云要么从密集深度图转换或从 3D 网格采样。outdoor urban scene专为自动驾驶而设计，其中物体在空间上分离良好，这些点云稀疏。</li>
</ol>
</li>
</ol>
</li>
<li>对于3D点云分割，这些数据集由不同类型的传感器获取，<ol>
<li>包括 Mobile Laser Scanners (MLS)（移动激光扫描仪）[15]、[34]、[36]、 Aerial Laser Scanners (ALS) （航空激光扫描仪）[33]、[38]、static Terrestrial Laser Scanners (TLS)（静态地面激光扫描仪）[12]、RGB-D cameras（RGBD相机）[11]和 other 3D scanners （其他3D扫描仪）[10]。</li>
</ol>
</li>
</ol>
<p>这些数据集可用于开发应对各种挑战的算法，包括相似的干扰项、形状不完整和类不平衡。</p>
<h3 id="用于3D形状分类的数据集"><a href="#用于3D形状分类的数据集" class="headerlink" title="用于3D形状分类的数据集"></a>用于3D形状分类的数据集</h3><table>
<thead>
<tr>
<th>名称和参考</th>
<th>年</th>
<th>样本</th>
<th>类</th>
<th>训练集</th>
<th>测试集</th>
<th>类型</th>
<th>Representation</th>
</tr>
</thead>
<tbody><tr>
<td>McGill Benchmark [23]</td>
<td>2008</td>
<td>456</td>
<td>19</td>
<td>304</td>
<td>152</td>
<td>Synthetic</td>
<td>Mesh</td>
</tr>
<tr>
<td>Sydney Urban Objects [24]</td>
<td>2013</td>
<td>588</td>
<td>14</td>
<td>-</td>
<td>-</td>
<td>Real-World</td>
<td>Point Clouds</td>
</tr>
<tr>
<td>ModelNet10 [6]</td>
<td>2015</td>
<td>4899</td>
<td>10</td>
<td>3991</td>
<td>605</td>
<td>Synthetic</td>
<td>Mesh</td>
</tr>
<tr>
<td>ModelNet40 [6]</td>
<td>2015</td>
<td>12311</td>
<td>40</td>
<td>9843</td>
<td>2468</td>
<td>Synthetic</td>
<td>Mesh</td>
</tr>
<tr>
<td>ShapeNet [8]</td>
<td>2015</td>
<td>51190</td>
<td>55</td>
<td>-</td>
<td>-</td>
<td>Synthetic</td>
<td>Mesh</td>
</tr>
<tr>
<td>ScanNet [11]</td>
<td>2017</td>
<td>12283</td>
<td>17</td>
<td>9677</td>
<td>2606</td>
<td>Real-World</td>
<td>RGB-D</td>
</tr>
<tr>
<td>ScanObjectNN [7]</td>
<td>2019</td>
<td>2902</td>
<td>15</td>
<td>2321</td>
<td>581</td>
<td>Real-World</td>
<td>Point Clouds</td>
</tr>
</tbody></table>
<h3 id="用于3D物体检测和跟踪的数据集"><a href="#用于3D物体检测和跟踪的数据集" class="headerlink" title="用于3D物体检测和跟踪的数据集"></a>用于3D物体检测和跟踪的数据集</h3><table>
<thead>
<tr>
<th>名称和参考</th>
<th>年</th>
<th>场景</th>
<th>类别</th>
<th>注释的框架</th>
<th>3D Boxes</th>
<th>场景类型</th>
<th>传感器</th>
</tr>
</thead>
<tbody><tr>
<td>KITTI [14]</td>
<td>2012</td>
<td>22</td>
<td>8</td>
<td>15K</td>
<td>200K</td>
<td>Urban (Driving)</td>
<td>RGB &amp; LiDAR</td>
</tr>
<tr>
<td>SUN RGB-D [25]</td>
<td>2015</td>
<td>47</td>
<td>37</td>
<td>5K</td>
<td>65K</td>
<td>Indoor</td>
<td>RGB-D</td>
</tr>
<tr>
<td>ScanNetV2 [11]</td>
<td>2018</td>
<td>1.5K</td>
<td>18</td>
<td>-</td>
<td>-</td>
<td>Indoor</td>
<td>RGB-D &amp; Mesh</td>
</tr>
<tr>
<td>H3D [26]</td>
<td>2019</td>
<td>160</td>
<td>8</td>
<td>27K</td>
<td>1.1M</td>
<td>Urban (Driving)</td>
<td>RGB &amp; LiDAR</td>
</tr>
<tr>
<td>Argoverse [27]</td>
<td>2019</td>
<td>113</td>
<td>15</td>
<td>44K</td>
<td>993K</td>
<td>Urban (Driving)</td>
<td>RGB &amp; LiDAR</td>
</tr>
<tr>
<td>Lyft L5 [28]</td>
<td>2019</td>
<td>366</td>
<td>9</td>
<td>46K</td>
<td>1.3M</td>
<td>Urban (Driving)</td>
<td>RGB &amp; LiDAR</td>
</tr>
<tr>
<td>A*3D [29]</td>
<td>2019</td>
<td>-</td>
<td>7</td>
<td>39K</td>
<td>230K</td>
<td>Urban (Driving)</td>
<td>RGB &amp; LiDAR</td>
</tr>
<tr>
<td>Waymo Open [30]</td>
<td>2020</td>
<td>1K</td>
<td>4</td>
<td>200K</td>
<td>12M</td>
<td>Urban (Driving)</td>
<td>RGB &amp; LiDAR</td>
</tr>
<tr>
<td>nuScenes [31]</td>
<td>2020</td>
<td>1K</td>
<td>23</td>
<td>40K</td>
<td>1.4M</td>
<td>Urban (Driving)</td>
<td>RGB &amp; LiDAR</td>
</tr>
</tbody></table>
<h3 id="用于3D点云分割的数据集"><a href="#用于3D点云分割的数据集" class="headerlink" title="用于3D点云分割的数据集"></a>用于3D点云分割的数据集</h3><table>
<thead>
<tr>
<th>名称和参考</th>
<th>年</th>
<th>顶点</th>
<th>类别</th>
<th>#Scans</th>
<th>空间大小</th>
<th>RGB</th>
<th>传感器</th>
</tr>
</thead>
<tbody><tr>
<td>Oakland [32]</td>
<td>2009</td>
<td>1.6M</td>
<td>5(44)</td>
<td>17</td>
<td>-</td>
<td>N&#x2F;A</td>
<td>MLS</td>
</tr>
<tr>
<td>ISPRS [33]</td>
<td>2012</td>
<td>1.2M</td>
<td>9</td>
<td>-</td>
<td>-</td>
<td>N&#x2F;A</td>
<td>ALS</td>
</tr>
<tr>
<td>Paris-rue-Madame [34]</td>
<td>2014</td>
<td>20M</td>
<td>17</td>
<td>2</td>
<td>-</td>
<td>N&#x2F;A</td>
<td>MLS</td>
</tr>
<tr>
<td>IQmulus [35]</td>
<td>2015</td>
<td>300M</td>
<td>8(22)</td>
<td>10</td>
<td>-</td>
<td>N&#x2F;A</td>
<td>MLS</td>
</tr>
<tr>
<td>ScanNet [11]</td>
<td>2017</td>
<td>-</td>
<td>20(20)</td>
<td>1513</td>
<td>8×4×4</td>
<td>Yes</td>
<td>RGB-D</td>
</tr>
<tr>
<td>S3DIS [10]</td>
<td>2017</td>
<td>273M</td>
<td>13(13)</td>
<td>272</td>
<td>10×5×5</td>
<td>Yes</td>
<td>Matterport</td>
</tr>
<tr>
<td>Semantic3D [12]</td>
<td>2017</td>
<td>4000M</td>
<td>8(9)</td>
<td>15&#x2F;15</td>
<td>250×260×80</td>
<td>Yes</td>
<td>TLS</td>
</tr>
<tr>
<td>Paris-Lille-3D [36]</td>
<td>2018</td>
<td>143M</td>
<td>9(50)</td>
<td>3</td>
<td>200×280×30</td>
<td>N&#x2F;A</td>
<td>MLS</td>
</tr>
<tr>
<td>SemanticKITTI [15]</td>
<td>2019</td>
<td>4549M</td>
<td>25(28)</td>
<td>23201&#x2F;20351</td>
<td>150×100×10</td>
<td>N&#x2F;A</td>
<td>MLS</td>
</tr>
<tr>
<td>Toronto-3D [37]</td>
<td>2020</td>
<td>78.3M</td>
<td>8(9)</td>
<td>4</td>
<td>260×350×40</td>
<td>Yes</td>
<td>MLS</td>
</tr>
<tr>
<td>DALES [38]</td>
<td>2020</td>
<td>505M</td>
<td>8(9)</td>
<td>40</td>
<td>500×500×65</td>
<td>N&#x2F;A</td>
<td>ALS</td>
</tr>
</tbody></table>
<p>表 1：用于 3D 形状分类、3D 对象检测和跟踪以及 3D 点云分割的现有数据集摘要。 用于评估的类数和带注释的类数（如括号所示）。</p>
<h2 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h2><p>已经提出了不同的评估指标来测试这些方法是否适用于各种点云理解任务。</p>
<ol>
<li>对于 3D 形状分类，总体精度 （OA） 和平均等级精度 （mAcc） 是最常用的性能标准。<ol>
<li>“OA”表示所有测试实例的平均精度，“mAcc”表示所有形状类的平均精度。</li>
</ol>
</li>
<li>对于 3D 目标检测，平均精度 （AP） 是最常用的标准。它被计算为精确召回率曲线下的面积。</li>
<li>精确度（Precision）、成功率（Success Rate）通常用于评估 3D 单对象跟踪器的整体性能。平均</li>
<li>目标跟踪精度 （AMOTA） 和平均多目标跟踪精度 （AMOTP） 是评估 3D 多目标跟踪最常用的标准。</li>
<li>对于 3D 点云分割，OA、平均交并比 （mIoU） 和平均类精度 （mAcc） [10]、[12]、[15]、[36]、[37] 是最常用的性能评估标准。特别是，平均精度（mAP）[39]也用于3D点云的实例分割。</li>
</ol>
<h1 id="3D形状分类"><a href="#3D形状分类" class="headerlink" title="3D形状分类"></a>3D形状分类</h1><p>此任务的方法通常首先学习每个点的嵌入，然后使用聚合方法从整个点云中提取全局形状嵌入。最终通过将全局嵌入馈送到几个全连接层中来实现分类。根据神经网络输入的数据类型，现有的三维形状分类方法可以分为基于多视图的方法、基于体积的方法和基于点的方法。图 2 说明了几种里程碑方法。基于多视图的方法将非结构化点云投影到 2D 图像中，而基于体积的方法将点云转换为 3D 体积表示。然后，利用完善的2D或3D卷积网络实现形状分类。相比之下，基于点的方法直接处理原始点云，无需任何体素化或投影。基于点的方法则不引入显性信息丢失，并变得越来越流行。需要注意的是，本文主要关注基于点的方法，但也包括一些基于多视图和基于体积的完整性方法。<img src="/images/1701075279419-109e5411-947f-4b14-8a75-f1749fa9cdca.png"><br>图 2：最相关的基于深度学习的 3D 形状分类方法的时间顺序概述。</p>
<h2 id="基于多视图的方法"><a href="#基于多视图的方法" class="headerlink" title="基于多视图的方法"></a>基于多视图的方法</h2><p>这些方法首先将 3D 形状投影到多个视图中并提取视图特征，然后融合这些特征以进行准确的形状分类。如何将多个视图特征聚合为可判别的全局表示是这些方法面临的关键挑战。MVCNN[40]是一项开创性的工作，它只是将多视图特征最大化为一个全局描述符。但是，max-pooling 仅保留特定视图中的最大元素，从而导致信息丢失。MHBN [41]通过协调的双线性池化来整合局部卷积特征，以产生一个紧凑的全局描述符。Yang 等.[42] 首先利用关系网络来利用一组视图的相互关系（例如，区域-区域关系和视图-视图关系），然后聚合这些视图以获得可判别的 3D 对象表示。此外，还提出了其他几种方法[43]、[44]、[45]、[46]来提高识别精度。与之前的方法不同，Wei等[47]在View-GCN中使用了有向图，将多个视图视为grpah节点。然后，将局部图卷积、非局部消息传递和选择性视图采样组成的核心层应用于构建的图中，最后，使用所有级别的最大池化节点特征的串联来形成全局形状描述符。</p>
<h2 id="基于体素的方法"><a href="#基于体素的方法" class="headerlink" title="基于体素的方法"></a>基于体素的方法</h2><p>这些方法通常将点云体素化为 3D 网格，然后在体积表示上应用 3D 卷积神经网络 （CNN） 进行形状分类。Maturana等[48]引入了一种名为VoxNet的体积占用网络来实现鲁棒的3D物体识别。Wu 等.[6] 提出了一种基于卷积深度信念的 3D ShapeNets，用于学习各种 3D 形状的点分布（由体素网格上二元变量的概率分布表示）。尽管已经取得了令人鼓舞的性能，但这些方法无法很好地扩展到密集的 3D 数据，因为计算和内存占用会随着分辨率的增加而增加。为此，引入了分层和紧凑的结构（如八叉树），以降低这些方法的计算和内存成本。OctNet [49]首先使用混合网格-八叉树结构对点云进行分层划分，该结构表示沿规则网格具有多个浅八叉树的场景。使用位字符串表示对八叉树的结构进行高效编码，并通过简单的算术对每个体素的特征向量进行索引。Wang等[50]提出了一种基于Octree的CNN用于3D形状分类。在最细叶八分频数中采样的 3D 模型的平均法向量被输入网络，并将 3D-CNN应用于3D形状表面占据的八分之一空间。与基于密集输入网格的基准网络相比，OctNet在处理高分辨率点云时需要更少的内存和运行时间。Le等人提出了一种名为PointGrid的混合网络，它将点和网格表示集成在一起，以实现高效的点云处理。在每个嵌入体素网格单元内采样了一个恒定数量的点，这使得网络能够通过使用3D卷积提取几何细节。Ben-Shabat等人将输入点云转换为3D网格，并通过3D改进的Fisher向量（3DmFV）方法来表示这些网格，然后通过传统的CNN架构学习全局表示。</p>
<h2 id="基于点的方法"><a href="#基于点的方法" class="headerlink" title="基于点的方法"></a>基于点的方法</h2><p>根据用于各点特征学习的网络架构，该类别中的方法可分为逐点MLP、基于卷积、基于图、基于分层数据结构的方法和其他典型方法。</p>
<h3 id="逐点MLP方法"><a href="#逐点MLP方法" class="headerlink" title="逐点MLP方法"></a>逐点MLP方法</h3><p>这些方法使用多个共享的多层感知器（MLP）对每个点进行独立建模，然后使用对称聚合函数聚合全局特征，如图3所示。</p>
<p><img src="/images/1701075728911-31b00d7e-aa28-4215-acf6-baf45ed2e694.png"><br> 图 3：PointNet 的轻量级架构。n 表示输入点的数量，M 表示每个点的学习特征的维度</p>
<p>典型的 2D 图像深度学习方法由于其固有的数据不规则性而无法直接应用于 3D 点云。作为一项开创性的工作，PointNet[5] 直接将点云作为其输入，并实现了具有对称函数的排列不变性。具体来说，PointNet 使用多个 MLP 层独立学习逐点特征，并使用最大池化层提取全局特征。通过对所有深度集合[53]表示求和并应用非线性变换来实现排列不变性。由于PointNet[5]中每个点的特征都是独立学习的，因此无法捕获点之间的局部结构信息。	因此，Qi等[54]提出了一种分层网络PointNet++，用于捕获每个点的邻域的精细几何结构。作为PointNet++层次结构的核心，其抽象级别集合层由三层组成：采样层、分组层和基于PointNet的学习层。通过堆叠多个设置的抽象级别，PointNet++ 从局部几何结构中学习特征，并逐层抽象局部特征。由于其简单性和较强的表示能力，许多网络都是基于PointNet开发的[5]。Mo-Net [55] 的架构类似于 PointNet [5]，但它需要一组有限的矩作为其输入。Point Attention Transformers（点注意力变换器）（PAT）[56]通过每个点自身的绝对位置和相对于相邻点的相对位置来表示每个点，并通过MLP学习高维特征。然后，利用群体随机注意力（GSA）捕获点间关系，开发排列不变、可微分、可训练的端到端Gumbel子集采样（GSS）层来学习层次特征。PointWeb [57] 基于 PointNet++ [54]，利用局部邻域的上下文，通过自适应特征调整 （AFA） 来改进点特征。Duan 等.[58] 提出了一种结构关系网络（SRN），以使用MLP学习不同局部结构之间的结构关系特征。Lin等[59]通过为PointNet学习的输入空间和函数空间构建查找表来加速推理过程。ModelNet 和 ShapeNet 数据集的推理时间比中等计算机上的 PointNet 快 1.5 ms，快 32 倍。SRINet[60]首先投射点云以获得旋转不变表示，然后利用基于PointNet的骨干网提取全局特征，利用基于图的聚合提取局部特征。在PointASNL中，Yan等人。[61]利用自适应采样（AS）模块自适应调整最远点采样（FPS）算法采样点的坐标和特征，并提出一个局部-非局部（L-NL）模块来捕获这些采样点的局部和长距离依赖关系。</p>
<h3 id="基于卷积的方法"><a href="#基于卷积的方法" class="headerlink" title="基于卷积的方法"></a>基于卷积的方法</h3><p>与在二维网格结构（例如图像）上定义的核相比，由于点云的不规则性，三维点云的卷积核难以设计。根据卷积核的类型，目前的3D卷积方法可以分为连续卷积方法和离散卷积方法，如图4所示。<img src="/images/1701076204216-e723b6c6-3f47-4e11-8425-8badf95a98c7.png"><br>图 4：点的局部邻居的连续和离散卷积图示。（a） 表示在点 p 处以 qcenter 为中心的局部邻域;（b） 和 （c） 分别表示 3D 连续卷积和离散卷积。</p>
<h4 id="3D-连续卷积方法："><a href="#3D-连续卷积方法：" class="headerlink" title="3D 连续卷积方法："></a>3D 连续卷积方法：</h4><p>这些方法在连续空间上定义卷积核，其中相邻点的权重与相对于中心点的空间分布相关。3D 卷积可以解释为给定子集的加权和。RSConv作为RS-CNN的核心层[62]，以某点周围的局部点子集作为输入，通过学习从低级关系（如欧几里得距离和相对位置）到局部子集中点间高级关系的映射，使用MLP实现卷积。在[63]中，核元素是在单位球体中随机选择的。然后使用基于MLP的连续函数在核元素的位置和点云之间建立关系。在DensePoint[64]中，卷积被定义为具有非线性激活子的单层感知器（SLP）。特征是通过连接所有先前层的特征来学习的，以充分利用上下文信息。Thomas等[65]提出了使用一组可学习核点的3D点云的刚性和可变形核点卷积（KPConv）算子。ConvPoint[66]将卷积核分为空间部分和特征部分。从单位球体中随机选择空间部分的位置，并通过简单的MLP学习加权函数。一些方法还使用现有算法来执行卷积。在PointConv[67]中，卷积被定义为连续3D卷积相对于重要性采样的蒙特卡罗估计。卷积核由加权函数（通过 MLP 层学习）和密度函数（通过核密度估计和 MLP 层学习）组成。为了提高内存和计算效率，将三维卷积进一步简化为矩阵乘法和二维卷积两种运算。在相同的参数设置下，其内存消耗可减少约64倍。在MCCNN [68]中，卷积被认为是一种蒙特卡罗估计过程，依赖于样本的密度函数（通过MLP实现）。然后使用泊松盘采样来构建点云层次结构。该卷积算子可用于在两种或多种采样方法之间执行卷积，并且可以处理不同的采样密度。在SpiderCNN[69]中，SpiderConv被提议将卷积定义为阶跃函数和在k个最近邻上定义的泰勒展开的乘积。阶跃函数通过对局部测地线距离进行编码来捕获粗略几何，泰勒展开通过在立方体的顶点处插值任意值来捕获固有的局部几何变化。此外，还提出了一种基于径向基函数的三维点云卷积网络PCNN[70]。已经提出了几种方法来解决三维卷积网络面临的旋转等变问题。Esteves等[71]提出了3D球面CNN来学习3D形状的旋转等变表示，它以多值球函数为输入。通过在球谐域中锚点对频谱进行参数化来获得局域卷积滤波器。张量场网络[72]被提出将点卷积运算定义为可学习的径向函数和球谐波的乘积，球谐波与三维旋转、平移和排列局部等变。文献[73]中的卷积基于球面互相关定义，并使用广义快速傅里叶变换（FFT）算法实现。SPHNet[74]基于PCNN，通过在体积函数卷积过程中加入球谐核来实现旋转不变性。为了加快计算速度，Flex-Convolution[75]将卷积核的权重定义为k个最近邻的标准标量乘积，可以使用CUDA进行加速。实验结果表明，该算法在参数较少、内存消耗较低的小数据集上具有较强的竞争力。</p>
<h4 id="离散卷积方法："><a href="#离散卷积方法：" class="headerlink" title="离散卷积方法："></a>离散卷积方法：</h4><p>这些方法在正则网格上定义卷积核，其中相邻点的权重与相对于中心点的偏移量相关。Hua等[76]将非均匀的三维点云转换为均匀网格，并在每个网格上定义卷积核。其中 3D 内核为落入同一网格的所有点分配相同的权重。对于给定点，将根据上一图层计算位于同一格网上的所有相邻点的平均要素。然后，对所有格网的平均要素进行加权和求和，以产生当前图层的输出。Lei 等人。[77] 通过将一个 3D 球形相邻区域划分为多个体积箱，并将每个箱与可学习的加权矩阵相关联，定义了一个球形卷积核。一个点的球面卷积核的输出由其相邻点的加权激活值平均值的非线性激活决定。在GeoConv[78]中，基于六个基础，明确地建模了点与其怕邻点之间的几何关系，每个基础方向上的边缘持征由与该方向相关的可字习矩阵独立加仅。然后，根据拾笔点与其邻点形成的角度，聚合这些与方向相关的待征。对于给定点，其当前层的待征被定义为给定点及其前一相邻边缘持征的总和。</p>
<p>PointCNN [79]通过χ-conv变换（通过MLP实现）将输入点转换为潜在的规范顺序，然后对变换后的特征应用典型的卷积算子。Mao等[80]通过对相邻离散卷积核权坐标进行点特征插值，提出了一种插值卷积算子InterpConv来测量输入点云与核权坐标之间的几何关系。Zhang等. [81]提出了一种RIConv算子来实现旋转不变性，该算子将低级旋转不变几何特征作为输入，然后通过简单的分箱方法将卷积转换为一维。A-CNN [82]通过循环查询点每个环上的邻域数组来定义环形卷积，并学习局部子集中相邻点之间的关系。为了降低3D CNN的计算和内存成本，Kumawat等[83]提出了一种基于3D短期傅里叶变换（STFT）的整流局部相位体积（ReLPV）模块来提取3D局部邻域中的相位，从而显著减少了参数的数量。在SFCNN[84]中，点云被投射到具有对齐球坐标的规则二十面体晶格上。然后，通过卷积-最大池化-卷积结构对球形晶格顶点及其相邻点连接的特征进行卷积。SFCNN 可抵抗旋转和扰动。</p>
<h3 id="基于图的方法"><a href="#基于图的方法" class="headerlink" title="基于图的方法"></a>基于图的方法</h3><p>基于图的网络将点云中的每个点视为图的一个顶点，并根据每个点的相邻点为图生成有向边。然后在空间域或光谱域中进行特征学习[85]。典型的基于图的网络如图所示。<img src="/images/1701138227438-d0e116e7-0189-4d75-a355-2638b140f6b0.png"><br>图 5：基于图的网络图示。</p>
<h4 id="基于空间域的图方法"><a href="#基于空间域的图方法" class="headerlink" title="基于空间域的图方法"></a>基于空间域的图方法</h4><p>这些方法定义空间域的操作（例如卷积和池化）。具体而言，卷积通常通过MLP在空间邻居上实现，并采用池化，通过聚合每个点邻居的信息来生成新的粗图。每个折点处的要素通常被分配有坐标、激光强度或颜色，而每条边上的要素通常被分配两个连接点之间的几何属性。</p>
<p>Simonovsky等[85]将每个点视为图的一个顶点，并通过有向边将每个顶点与其所有相邻点连接起来。然后，使用滤波生成网络（例如MLP）提出EdgeConditioned卷积（ECC）。采用最大池化来聚合邻域信息，并基于VoxelGrid[86]实现图粗化。在DGCNN[87]中，图是在特征空间中构建的，并在网络的每一层之后动态更新。作为EdgeConv的核心层，MLP作为每条边的特征学习函数，将通道对称聚合应用于与每个点的相邻边缘特征相关联。此外，LDGCNN [88]去除了变换网络，并将DGCNN [87]中不同层的分层特征联系起来，以提高其性能并减小模型大小。该文还提出了一种端到端的无监督深度自编码器网络（即FoldingNet [89]），该网络使用矢量化局部协方差矩阵和点坐标的串联作为其输入。受Inception [90]和DGCNN [87]的启发，Hassani和Haley [91]提出了一种无监督的多任务自动编码器来学习点和形状特征。编码器是基于多尺度图构造的。该解码器采用聚类、自监督分类和重构三种无监督任务构建，这些任务与多任务损失联合训练。Liu等. [92]提出了一种基于图卷积的动态点集聚模块（DPAM），将点集聚过程（采样、分组和池化）简化为一个简单的步骤，通过集聚矩阵和点特征矩阵的乘法来实现。基于PointNet架构，通过堆叠多个DPAM构建分层学习架构。与PointNet++[54]的层次策略相比，DPAM动态地利用了语义空间中的点之间的关系和点的聚集。为了利用局部几何结构，KCNet[93]基于核相关性学习特征。具体来说，一组表征局部结构几何类型的可学习点被定义为核。然后，计算核与给定点的邻域之间的亲和力。在G3D[94]中，卷积被定义为邻接矩阵多项式的变体，池化被定义为拉普拉斯矩阵和顶点矩阵乘以粗化矩阵。ClusterNet[95]利用严格的旋转不变模块从每个点的k个最近邻中提取旋转不变特征，并基于无监督的团聚分层聚类方法构造点云的层次结构[96]。每个子集群中的特征首先通过 EdgeConv 块学习，然后通过 max pooling 进行聚合。针对当前数据结构化方法（如FPS、邻点查询等）耗时的问题，Xu等[97]提出将基于体素的方法和基于点的方法的优点结合起来，以提高计算效率。在ModelNet分类任务上的实验表明，所提出的Grid-GCN网络的计算效率平均比其他模型快5X。</p>
<h4 id="基于光谱域的图方法"><a href="#基于光谱域的图方法" class="headerlink" title="基于光谱域的图方法"></a>基于光谱域的图方法</h4><p>这些方法方法将卷积定义为谱滤波，其实现为图上的信号与图拉普拉斯矩阵的特征向量相乘[98]，[99]。RGCNN [100] 通过将每个点与点云中的所有其他点连接起来来构建图形并更新每层中的拉普拉斯矩阵图。为了使相邻顶点的特征更加相似，在损失函数中添加了图信号平滑性先验。为了解决数据图拓扑结构多样化带来的挑战，AGCN [101]中的SGC-LL层利用可学习的距离度量来参数化图上两个顶点之间的相似性。从图中获得的邻接矩阵使用高斯核和学习距离进行归一化。HGNN [102]通过在超图上应用谱卷积来构建超边缘卷积层。上述方法对完整的图形进行操作。为了利用局部结构信息，Wang等[103]提出了一个端到端的谱卷积网络LocalSpecGCN，用于处理局部图（由k个最近邻构建）。该方法不需要对图拉普拉斯矩阵和图粗化层次结构进行任何离线计算。在PointGCN[104]中，基于来自点云的k个最近邻构建图，并使用高斯核对每条边进行加权。卷积滤波器被定义为图谱域中的切比雪夫多项式。全局池化和多分辨率池化用于捕获点云的全局和局部特征。Pan 等人。[105] 通过对光谱域中的k个最近相邻图应用卷积，提出了3DTI-Net。几何变换的不变性是通过从相对欧几里得距离和方向距离中学习来实现的。</p>
<h3 id="基于层次数据结构的方法"><a href="#基于层次数据结构的方法" class="headerlink" title="基于层次数据结构的方法"></a>基于层次数据结构的方法</h3><p>这些网络是基于不同的分层数据结构（例如，octree 和 kd-tree）构建的。在这些方法中，点特征是沿树从叶节点到根节点分层学习的。Lei等[77]提出了一种使用球形卷积核的八叉树引导的CNN（如第3.3.2节所述）。网络的每一层对应于八叉树的一层，并在每一层应用一个球形卷积核。当前层中神经元的值被确定为上一层中所有相关子节点的平均值。与基于八叉树的OctNet [49]不同，Kd-Net [106]使用多个K-d树构建，每次迭代时具有不同的拆分方向。遵循自下而上的方法，使用 MLP 根据其子节点的表示来计算非叶节点的表示。根节点的特征（描述整个点云）最终被馈送到全连接层以预测分类分数。请注意，Kd-Net 根据节点的拆分类型在每个级别共享参数。3DContextNet [107] 使用标准的平衡 K-d 树来实现特征学习和聚合。在每个级别上，首先通过基于局部线索（对局部区域中点之间的相互依赖关系进行建模）和全局上下文线索（对一个位置相对于所有其他位置的关系进行建模）的 MLP 学习点特征。然后，使用 MLP 从其子节点计算非叶节点的特征，并通过最大池化进行聚合。对于分类，重复上述过程，直到到达根节点。通过进行点到节点k最近邻搜索[108]来构建SO-Net网络的层次结构。具体说来, 修正排列不变自组织映射（SOM）用于对点云的空间分布进行建模。单个点要素是通过一系列全连接图层从归一化的点到节点坐标中学习的。SOM 中每个节点的特征都是使用通道最大池化从与该节点关联的点要素中提取的。然后使用类似于 PointNet [5] 的方法从节点特征中学习最终特征。与PointNet++[54]相比，SOM的层次结构更加高效，并充分探索了点云的空间分布。</p>
<h3 id="其他方法"><a href="#其他方法" class="headerlink" title="其他方法"></a>其他方法</h3><p>此外，还提出了许多其他方案。RBFNet[113]通过聚合具有可学习核位置和大小的稀疏分布径向基函数（RBF）核的特征，显式地模拟了点的空间分布。3DPointCapsNet [112] 使用逐点 MLP 和卷积层学习与点无关的特征，并使用多个最大池化层提取全局潜在表示。基于无监督动态路由，学习强大的代表性潜胶囊。Qin等[116]提出了一种用于3D点云表示的端到端无监督域自适应网络PointDAN。为了捕捉点云的语义特性，提出了一种自监督的点云重构方法，点云的各个部分被随机重排[117]。Li 等.[118] 提出了一个自动增强框架 PointAugment，用于自动优化和增强点云样本以进行网络训练。具体来说，每个输入样本的形状变换和逐点位移是自动学习的，并且通过交替优化和更新其预兆和分类器的可学习参数来训练网络。受形状上下文[119]的启发，Xie等[109]提出了一种ShapeContextNet架构，将亲和点点选择和紧凑特征聚合结合到使用点积自注意力的软对齐操作中[120]。为了处理3D点云中的噪声和遮挡，Bobkov等[121]将手工制作的基于点对函数的4D旋转不变描述符输入到4D卷积神经网络中。Prokudin 等人。[122] 首先从单位球中随机采样一个分布均匀的基点集，然后将点云编码为到基点集的最小距离。因此，点云被转换为具有相对较小的固定长度的矢量。然后，可以使用现有的机器学习方法处理编码的表示。RCNet[115]利用标准RNN和2D CNN构建了用于3D点云处理的置换不变网络。首先将点云划分为平行光束，并沿特定维度进行排序，然后将每个光束馈入共享的RNN。学习到的特征被进一步输入到高效的 2D CNN 中，以实现分层特征聚合。为了增强其描述能力，提出RCNet-E沿不同的分区和排序方向对多个RCNet进行集成。Point2Sequences [114] 是另一个基于 RNN 的模型，用于捕获点云局部区域中不同区域之间的相关性。它将从多个尺度的局部区域学习的特征视为序列，并将这些序列从所有局部区域馈送到基于 RNN 的编码器-解码器结构中，以聚合局部区域特征。有几种方法还可以从 3D 点云和 2D 图像中学习。在PVNet[110]中，从多视图图像中提取的高级全局特征通过嵌入网络投射到点云的子空间中，并通过软注意力掩码与点云特征融合。最后，对融合特征和多视图特征采用残差连接进行形状识别。后来，PVRNet[111]进一步提出通过关系评分模块来利用3D点云与其多个视图之间的关系。基于关系得分，增强了原有的 2D 全局视图特征，实现了点-单视图融合和点-多视图融合。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>ModelNet10&#x2F;40 [6] 数据集是最常用的 3D 形状分类数据集。表 2 显示了不同基于点的网络所取得的结果。可以得出几个观察结果：</p>
<ul>
<li>逐点 MLP 网络通常用作其他类型网络学习逐点特征的基本构建块。</li>
<li>作为一种标准的深度学习架构，基于卷积的网络可以在不规则的 3D 点云上实现卓越的性能。 对于不规则数据，应更加关注离散卷积网络和连续卷积网络。</li>
<li>由于其固有的处理不规则数据的强大能力，基于图的网络近年来越来越受到关注。然而，将光谱域中基于图的网络扩展到各种图结构仍然具有挑战性。</li>
</ul>
<table>
<thead>
<tr>
<th>方法</th>
<th></th>
<th>输入</th>
<th>参数量(M)</th>
<th>ModelNet40 (OA)</th>
<th>ModelNet40 (mAcc)</th>
<th>ModelNet10 (OA)</th>
<th>ModelNet10 (mAcc)</th>
</tr>
</thead>
<tbody><tr>
<td>逐点MLP</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>PointNet [5]</td>
<td>顶点</td>
<td>3.48</td>
<td>89.2%</td>
<td>86.2%</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>PointNet++ [54]</td>
<td>顶点</td>
<td>1.48</td>
<td>90.7%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>MO-Net [55]</td>
<td>顶点</td>
<td>3.1</td>
<td>89.3%</td>
<td>86.1%</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Deep Sets [53]</td>
<td>顶点</td>
<td>-</td>
<td>87.1%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>PAT [56]</td>
<td>顶点</td>
<td>-</td>
<td>91.7%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>PointWeb [57]</td>
<td>顶点</td>
<td>-</td>
<td>92.3%</td>
<td>89.4%</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>SRN-PointNet++ [58]</td>
<td>顶点</td>
<td>-</td>
<td>91.5%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>JUSTLOOKUP [59]</td>
<td>顶点</td>
<td>-</td>
<td>89.5%</td>
<td>86.4%</td>
<td>92.9%</td>
<td>92.1%</td>
</tr>
<tr>
<td></td>
<td>PointASNL [61]</td>
<td>顶点</td>
<td>-</td>
<td>92.9%</td>
<td>-</td>
<td>95.7%</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>PointASNL [61]</td>
<td>顶点+法线</td>
<td>-</td>
<td>93.2%</td>
<td>-</td>
<td>95.9%</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>基于卷积</td>
<td>Pointwise-CNN [76]</td>
<td>顶点</td>
<td>-</td>
<td>86.1%</td>
<td>81.4%</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>PointConv [67]</td>
<td>顶点+法线</td>
<td>-</td>
<td>92.5%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>MC Convolution [68]</td>
<td>顶点</td>
<td>-</td>
<td>90.9%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>SpiderCNN [69]</td>
<td>顶点+法线</td>
<td>-</td>
<td>92.4%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>PointCNN [79]</td>
<td>顶点</td>
<td>0.45</td>
<td>92.2%</td>
<td>88.1%</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Flex-Convolution [75]</td>
<td>顶点</td>
<td>-</td>
<td>90.2%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>PCNN [70]</td>
<td>顶点</td>
<td>1.4</td>
<td>92.3%</td>
<td>-</td>
<td>94.9%</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Boulch [63]</td>
<td>顶点</td>
<td>-</td>
<td>91.6%</td>
<td>88.1%</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>RS-CNN [62]</td>
<td>顶点</td>
<td>-</td>
<td>93.6%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Spherical CNNs [71]</td>
<td>顶点</td>
<td>0.5</td>
<td>88.9%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>GeoCNN [78]</td>
<td>顶点</td>
<td>-</td>
<td>93.4%</td>
<td>91.1%</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Ψ-CNN [77]</td>
<td>顶点</td>
<td>-</td>
<td>92.0%</td>
<td>88.7%</td>
<td>94.6%</td>
<td>94.4%</td>
</tr>
<tr>
<td></td>
<td>A-CNN [82]</td>
<td>顶点</td>
<td>-</td>
<td>92.6%</td>
<td>90.3%</td>
<td>95.5%</td>
<td>95.3%</td>
</tr>
<tr>
<td></td>
<td>SFCNN [84]</td>
<td>顶点</td>
<td>-</td>
<td>91.4%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>SFCNN [84]</td>
<td>顶点+法线</td>
<td>-</td>
<td>92.3%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>DensePoint [64]</td>
<td>顶点</td>
<td>0.53</td>
<td>93.2%</td>
<td>-</td>
<td>96.6%</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>KPConv rigid [65]</td>
<td>顶点</td>
<td>-</td>
<td>92.9%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>KPConv deform [65]</td>
<td>顶点</td>
<td>-</td>
<td>92.7%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>InterpCNN [80]</td>
<td>顶点</td>
<td>12.8</td>
<td>93.0%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>ConvPoint [66]</td>
<td>顶点</td>
<td>-</td>
<td>91.8%</td>
<td>88.5%</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>基于图方法</td>
<td>ECC [85]</td>
<td>顶点</td>
<td>-</td>
<td>87.4%</td>
<td>83.2%</td>
<td>90.8%</td>
<td>90.0%</td>
</tr>
<tr>
<td></td>
<td>KCNet [93]</td>
<td>顶点</td>
<td>0.9</td>
<td>91.0%</td>
<td>-</td>
<td>94.4%</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>DGCNN [87]</td>
<td>顶点</td>
<td>1.84</td>
<td>92.2%</td>
<td>90.2%</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>LocalSpecGCN [103]</td>
<td>顶点+法线</td>
<td>-</td>
<td>92.1%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>RGCNN [100]</td>
<td>顶点+法线</td>
<td>2.24</td>
<td>90.5%</td>
<td>87.3%</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>LDGCNN [88]</td>
<td>顶点</td>
<td>-</td>
<td>92.9%</td>
<td>90.3%</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>3DTI-Net [105]</td>
<td>顶点</td>
<td>2.6</td>
<td>91.7%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>PointGCN [104]</td>
<td>顶点</td>
<td>-</td>
<td>89.5%</td>
<td>86.1%</td>
<td>91.9%</td>
<td>91.6%</td>
</tr>
<tr>
<td></td>
<td>ClusterNet [95]</td>
<td>顶点</td>
<td>-</td>
<td>87.1%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Hassani et al. [91]</td>
<td>顶点</td>
<td>-</td>
<td>89.1%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>DPAM [92]</td>
<td>顶点</td>
<td>-</td>
<td>91.9%</td>
<td>89.9%</td>
<td>94.6%</td>
<td>94.3%</td>
</tr>
<tr>
<td></td>
<td>Grid-GCN [97]</td>
<td>顶点</td>
<td>-</td>
<td>93.1%</td>
<td>91.3%</td>
<td>97.5%</td>
<td>97.4%</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>基于层次结构方法</td>
<td>KD-Net [106]</td>
<td>顶点</td>
<td>2.0</td>
<td>91.8%</td>
<td>88.5%</td>
<td>94.0%</td>
<td>93.5%</td>
</tr>
<tr>
<td></td>
<td>SO-Net [108]</td>
<td>顶点</td>
<td>-</td>
<td>90.9%</td>
<td>87.3%</td>
<td>94.1%</td>
<td>93.9%</td>
</tr>
<tr>
<td></td>
<td>SCN [109]</td>
<td>顶点</td>
<td>-</td>
<td>90.0%</td>
<td>87.6%</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>A-SCN [109]</td>
<td>顶点</td>
<td>-</td>
<td>89.8%</td>
<td>87.4%</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>3DContextNet [107]</td>
<td>顶点</td>
<td>-</td>
<td>90.2%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>3DContextNet [107]</td>
<td>顶点+法线</td>
<td>-</td>
<td>91.1%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>其他方法</td>
<td>3DmFV-Net [52]</td>
<td>顶点</td>
<td>4.6</td>
<td>91.6%</td>
<td>-</td>
<td>95.2%</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>PVNet [110]</td>
<td>顶点+多视图</td>
<td>-</td>
<td>93.2%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>PVRNet [111]</td>
<td>顶点+多视图</td>
<td>-</td>
<td>93.6%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>3DPointCapsNet [112]</td>
<td>顶点</td>
<td>-</td>
<td>89.3%</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>DeepRBFNet [113]</td>
<td>顶点</td>
<td>3.2</td>
<td>90.2%</td>
<td>87.8%</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>DeepRBFNet [113]</td>
<td>顶点+法线</td>
<td>3.2</td>
<td>92.1%</td>
<td>88.8%</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Point2Sequences [114]</td>
<td>顶点</td>
<td>-</td>
<td>92.6%</td>
<td>90.4%</td>
<td>95.3%</td>
<td>95.1%</td>
</tr>
<tr>
<td></td>
<td>RCNet [115]</td>
<td>顶点</td>
<td>-</td>
<td>91.6%</td>
<td>-</td>
<td>94.7%</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>RCNet-E [115]</td>
<td>顶点</td>
<td>-</td>
<td>92.3%</td>
<td>-</td>
<td>95.6%</td>
<td>-</td>
</tr>
</tbody></table>
<h1 id="3D对象检测及跟踪"><a href="#3D对象检测及跟踪" class="headerlink" title="3D对象检测及跟踪"></a>3D对象检测及跟踪</h1><p>在本节中，我们将回顾现有的 3D 目标检测、3D 目标跟踪和 3D 场景流估计方法。</p>
<h2 id="3D对象检测"><a href="#3D对象检测" class="headerlink" title="3D对象检测"></a>3D对象检测</h2><p>典型的 3D 对象检测器将场景的点云作为其输入，并在每个检测到的对象周围生成一个定向 3D 边界框，如图 6 所示。与图像中的目标检测方法[123]类似，3D目标检测方法可分为两类：基于区域建议的方法和单次方法。图 7 介绍了几种里程碑方法。<img src="/images/1701142595696-d6953899-9f3d-4a17-bd66-033b0c7b1c28.png"><br>图6：3D物体检测示意图。（a）和（b）最初分别出现在[124]和[125]中。</p>
<p><img src="/images/1701142654670-71e17a04-aaa1-4a73-99de-3da7c17d2a03.png"><br>图 7：最相关的基于深度学习的 3D 目标检测方法的时间顺序概述</p>
<table>
<thead>
<tr>
<th>方法</th>
<th></th>
<th></th>
<th>形态（雷达&#x2F;图片）</th>
<th>速度(fps)</th>
<th>汽车 (E&#x2F;简单)</th>
<th>汽车(M&#x2F;中等)</th>
<th>汽车 (H&#x2F;困难)</th>
<th>行人 (E&#x2F;简单)</th>
<th>行人 (M&#x2F;中等)</th>
<th>行人(H&#x2F;困难)</th>
<th>骑单车人(E&#x2F;简单)</th>
<th>骑单车人 (M&#x2F;中等)</th>
<th>骑单车人 (H&#x2F;困难)</th>
</tr>
</thead>
<tbody><tr>
<td>基于区域提议</td>
<td>多视图方法</td>
<td>MV3D [4]</td>
<td>L &amp; I</td>
<td>2.8</td>
<td>74.97</td>
<td>63.63</td>
<td>54.00</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>AVOD [126]</td>
<td>L &amp; I</td>
<td>12.5</td>
<td>76.39</td>
<td>66.47</td>
<td>60.23</td>
<td>36.10</td>
<td>27.86</td>
<td>25.76</td>
<td>57.19</td>
<td>42.08</td>
<td>38.29</td>
</tr>
<tr>
<td></td>
<td></td>
<td>ContFuse [127]</td>
<td>L &amp; I</td>
<td>16.7</td>
<td>83.68</td>
<td>68.78</td>
<td>61.67</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>MMF [128]</td>
<td>L &amp; I</td>
<td>12.5</td>
<td>88.40</td>
<td>77.43</td>
<td>70.22</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>SCANet [39]</td>
<td>L &amp; I</td>
<td>11.1</td>
<td>79.22</td>
<td>67.13</td>
<td>60.65</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>RT3D [131]</td>
<td>L &amp; I</td>
<td>11.1</td>
<td>23.74</td>
<td>19.14</td>
<td>18.86</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>基于分割的方法</td>
<td>IPOD [132]</td>
<td>L &amp; I</td>
<td>5.0</td>
<td>80.30</td>
<td>73.04</td>
<td>68.73</td>
<td>55.07</td>
<td>44.37</td>
<td>40.05</td>
<td>71.99</td>
<td>52.23</td>
<td>46.50</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PointRCNN [133]</td>
<td>L</td>
<td>10.0</td>
<td>86.96</td>
<td>75.64</td>
<td>70.70</td>
<td>47.98</td>
<td>39.37</td>
<td>36.01</td>
<td>74.96</td>
<td>58.82</td>
<td>52.53</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PointRGCN [134]</td>
<td>L</td>
<td>3.8</td>
<td>85.97</td>
<td>75.73</td>
<td>70.60</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PointPainting [135]</td>
<td>L &amp; I</td>
<td>2.5</td>
<td>82.11</td>
<td>71.70</td>
<td>67.08</td>
<td>50.32</td>
<td>40.97</td>
<td>37.87</td>
<td>77.63</td>
<td>63.78</td>
<td>55.89</td>
</tr>
<tr>
<td></td>
<td></td>
<td>STD [138]</td>
<td>L</td>
<td>12.5</td>
<td>87.95</td>
<td>79.71</td>
<td>75.09</td>
<td>53.29</td>
<td>42.47</td>
<td>38.35</td>
<td>78.69</td>
<td>61.59</td>
<td>55.30</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>基于视锥的方法</td>
<td>F-PointNets [139]</td>
<td>L &amp; I</td>
<td>5.9</td>
<td>82.19</td>
<td>69.79</td>
<td>60.59</td>
<td>50.53</td>
<td>42.15</td>
<td>38.08</td>
<td>72.27</td>
<td>56.12</td>
<td>49.01</td>
</tr>
<tr>
<td></td>
<td></td>
<td>SIFRNet [140]</td>
<td>L &amp; I</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PointFusion [142]</td>
<td>L &amp; I</td>
<td>-</td>
<td>77.92</td>
<td>63.00</td>
<td>53.27</td>
<td>33.36</td>
<td>28.04</td>
<td>23.38</td>
<td>49.34</td>
<td>29.42</td>
<td>26.98</td>
</tr>
<tr>
<td></td>
<td></td>
<td>RoarNet [143]</td>
<td>L &amp; I</td>
<td>10.0</td>
<td>83.71</td>
<td>73.04</td>
<td>59.16</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>F-ConvNet [144]</td>
<td>L &amp; I</td>
<td>2.1</td>
<td>87.36</td>
<td>76.39</td>
<td>66.69</td>
<td>52.16</td>
<td>43.38</td>
<td>38.80</td>
<td>81.98</td>
<td>65.07</td>
<td>56.54</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Patch Refinement [145]</td>
<td>L</td>
<td>6.7</td>
<td>88.67</td>
<td>77.20</td>
<td>71.82</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>其它方法</td>
<td>3D IoU loss [146]</td>
<td>L</td>
<td>12.5</td>
<td>86.16</td>
<td>76.50</td>
<td>71.39</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Fast Point R-CNN [147]</td>
<td>L</td>
<td>16.7</td>
<td>84.80</td>
<td>74.59</td>
<td>67.27</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PV-RCNN [148]</td>
<td>L</td>
<td>12.5</td>
<td>90.25</td>
<td>81.43</td>
<td>76.82</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>VoteNet [124]</td>
<td>L</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Feng et al. [149]</td>
<td>L</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>ImVoteNet [150]</td>
<td>L &amp; I</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Part-Aˆ2 [151]</td>
<td>L</td>
<td>12.5</td>
<td>87.81</td>
<td>78.49</td>
<td>73.51</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>单阶段方法</td>
<td>基于BEV的方法</td>
<td>PIXOR [129]</td>
<td>L</td>
<td>28.6</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>HDNET [152]</td>
<td>L</td>
<td>20.0</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>BirdNet [153]</td>
<td>L</td>
<td>9.1</td>
<td>13.53</td>
<td>9.47</td>
<td>8.49</td>
<td>12.25</td>
<td>8.99</td>
<td>8.06</td>
<td>16.63</td>
<td>10.46</td>
<td>9.53</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>基于离散化的方法</td>
<td>VeloFCN [154]</td>
<td>L</td>
<td>1.0</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>3D FCN [155]</td>
<td>L</td>
<td>&lt;0.2</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Vote3Deep [156]</td>
<td>L</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>3DBN [157]</td>
<td>L</td>
<td>7.7</td>
<td>83.77</td>
<td>73.53</td>
<td>66.23</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>VoxelNet [136]</td>
<td>L</td>
<td>2.0</td>
<td>77.47</td>
<td>65.11</td>
<td>57.73</td>
<td>39.48</td>
<td>33.69</td>
<td>31.51</td>
<td>61.22</td>
<td>48.36</td>
<td>44.37</td>
</tr>
<tr>
<td></td>
<td></td>
<td>SECOND [158]</td>
<td>L</td>
<td>26.3</td>
<td>83.34</td>
<td>72.55</td>
<td>65.82</td>
<td>48.96</td>
<td>38.78</td>
<td>34.91</td>
<td>71.33</td>
<td>52.08</td>
<td>45.83</td>
</tr>
<tr>
<td></td>
<td></td>
<td>MVX-Net [159]</td>
<td>L &amp; I</td>
<td>16.7</td>
<td>84.99</td>
<td>71.95</td>
<td>64.88</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>PointPillars [137]</td>
<td>L</td>
<td>62.0</td>
<td>82.58</td>
<td>74.31</td>
<td>68.99</td>
<td>51.45</td>
<td>41.92</td>
<td>38.89</td>
<td>77.10</td>
<td>58.65</td>
<td>51.92</td>
</tr>
<tr>
<td></td>
<td></td>
<td>SA-SSD [160]</td>
<td>L</td>
<td>25.0</td>
<td>88.75</td>
<td>79.79</td>
<td>74.16</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>基于点的方法</td>
<td>3DSSD [161]</td>
<td>L</td>
<td>25.0</td>
<td>88.36</td>
<td>79.57</td>
<td>74.55</td>
<td>54.64</td>
<td>44.27</td>
<td>40.23</td>
<td>82.48</td>
<td>64.10</td>
<td>56.90</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>其他方法</td>
<td>LaserNet [162]</td>
<td>L</td>
<td>83.3</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>LaserNet++ [163]</td>
<td>L &amp; I</td>
<td>26.3</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>OHS-Dense [164]</td>
<td>L</td>
<td>33.3</td>
<td>88.12</td>
<td>78.34</td>
<td>73.49</td>
<td>47.14</td>
<td>39.72</td>
<td>37.25</td>
<td>79.09</td>
<td>62.72</td>
<td>56.76</td>
</tr>
<tr>
<td></td>
<td></td>
<td>OHS-Direct [164]</td>
<td>L</td>
<td>33.3</td>
<td>86.40</td>
<td>77.74</td>
<td>72.97</td>
<td>51.29</td>
<td>44.81</td>
<td>41.13</td>
<td>77.70</td>
<td>63.16</td>
<td>57.16</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Point-GNN [125]</td>
<td>L</td>
<td>1.7</td>
<td>88.33</td>
<td>79.47</td>
<td>72.29</td>
<td>51.92</td>
<td>43.77</td>
<td>40.14</td>
<td>78.60</td>
<td>63.48</td>
<td>57.08</td>
</tr>
</tbody></table>
<p>表 3：KITTI 测试 3D 检测基准上的比较 3D 目标检测结果。汽车的 3D 边界框 IoU 阈值为 0.7，行人和骑自行车者的阈值为 0.5。模态是激光雷达（L）和图像（I）。“E”、“M”和“H”分别表示简单、中等和困难类别的对象。为简单起见，我们省略了值后面的“%”。符号“-”表示结果不可用。</p>
<p><img src="/images/1701310839390-70a95e97-7e7b-439b-9fdf-d6b2041c3624.png"></p>
<p>表 4：KITTI 测试 BEV 检测基准上的比较 3D 物体检测结果。汽车的 3D 边界框 IoU 阈值为 0.7，行人和骑自行车者的阈值为 0.5。模态是激光雷达（L）和图像（I）。“E”、“M”和“H”分别表示简单、中等和困难类别的对象。为简单起见，我们省略了值后面的“%”。符号“-”表示结果不可用。</p>
<h3 id="基于区域提议的方法"><a href="#基于区域提议的方法" class="headerlink" title="基于区域提议的方法"></a>基于区域提议的方法</h3><p>这些方法首先提出包含对象的几个可能区域（也称为提议），然后提取区域特征以确定每个建议的类别标签。根据其对象建议生成方法，这些方法可以进一步分为三类：基于多视图的方法、基于分割的方法和基于视锥体的方法。</p>
<p><img src="/images/1701154025889-ee9bd3d9-b476-4ccf-8efb-1e48b92626d3.png"><br>图8：三类基于区域提案的3D目标检测方法的典型网络。从上到下：（a）基于多视图的方法，（b）基于分割的方法和（c）基于视锥体的方法。</p>
<h4 id="基于多视图的方法-1"><a href="#基于多视图的方法-1" class="headerlink" title="基于多视图的方法"></a>基于多视图的方法</h4><p>这些方法融合从不同视图图（例如，LiDAR 前视图、鸟瞰图 （BEV） 和图像）中提出建议特征，以获得 3D 旋转框，如图 8（a） 所示。这些方法的计算成本通常很高。</p>
<p>Chen等[4] 从BEV地图生成了一组高精度的3D候选框，并将它们投影到多个视图（如LiDAR前视图图像、RGB图像）的特征图上。然后，他们将这些区域特征从不同角度组合起来，以预测定向的 3D 边界框，如图 8（a） 所示。尽管该方法在仅用 300 个提案即可在 0.25 的交集 （IoU） 下实现了 99.1% 的召回率，但其速度对于实际应用来说太慢了。随后，从两个方面发展了几种改进多视角三维目标检测方法的方法。首先，已经提出了几种方法可以有效地融合不同模态的信息。Ku等[126]提出了一个基于多模态融合的区域建议网络，生成对小物体具有较高召回率的3D提议。他们首先使用裁剪和调整大小操作从 BEV 和图像视图中提取大小相等的特征，然后使用元素均值池化融合这些特征。Liang 等.[127] 利用连续卷积实现不同分辨率的图像和 3D LiDAR 特征图的有效融合。具体来说，他们为BEV空间中的每个点提取了最接近的对应图像特征，然后使用双线性插值，通过将图像特征投影到BEV平面中来获得密集的BEV特征图。实验结果表明，密集BEV特征图比离散图像特征图和稀疏LiDAR特征图更适合3D目标检测。Liang等[128]提出了一种用于端到端训练的多任务多传感器3D目标检测网络。具体来说，利用多个任务（例如，2D 目标检测、地面估计和深度完成）来帮助网络学习更好的特征表示。进一步利用学习到的跨模态表示来产生高度准确的目标检测结果。实验结果表明，该方法在2D、3D和BEV检测任务上取得了显著的改进，在TOR4D基准测试上优于以往最先进的方法[129]，[130]。</p>
<p>其次，研究了不同的方法来提取输入数据的鲁棒表示。Lu 等.[39] 通过引入空间通道注意力 （SCA） 模块来探索多尺度上下文信息，该模块捕获场景的全局和多尺度上下文并突出显示有用的特征。他们还提出了一个扩展空间解采样（ESU）模块，通过组合多尺度的低级特征来获得具有丰富空间信息的高级特征，从而生成可靠的三维对象提议。虽然可以实现更好的检测性能，但上述多视图方法需要较长的运行时间，因为它们对每个提案执行特征池。随后，Zeng等[131]使用pre-RoI池化卷积来提高[4]的效率。具体来说，他们将大多数卷积操作移到了 RoI 池化模块之前。因此，对所有对象建议执行一次 RoI 卷积。实验结果表明，该方法可以以11.1fps的速度运行，比MV3D快5倍[4]。</p>
<h4 id="基于分割的方法"><a href="#基于分割的方法" class="headerlink" title="基于分割的方法"></a>基于分割的方法</h4><p>这些方法首先利用现有的语义分割技术去除大部分背景点，然后在前景点上生成大量高质量的提案以节省计算量，如图8（b）所示。与多视图方法[4]、[126]、[131]相比，这些方法实现了更高的物体召回率，更适合于物体高度遮挡和拥挤的复杂场景。Yang等[132]使用二维分割网络来预测前景像素，并将其投影到点云中以去除大部分背景点。然后，他们在预测的前景点上生成提案，并设计了一个名为 PointsIoU 的新标准，以减少提案的冗余和模糊性。继[132]之后，Shi等[133]提出了一个PointRCNN框架。具体来说，他们直接分割3D点云以获得前景点，然后融合语义特征和局部空间功能来生产高质量的 3D box。继[133]的区域提议网络（RPN）阶段之后，Jesus等[134]提出了一项利用图卷积网络（GCN）进行3D目标检测的开创性工作。具体来说，介绍了两个模块来使用图卷积来优化对象建议。第一个模块 R-GCN 利用提案中包含的所有点来实现每个提案的特征聚合。第二个模块 C-GCN 融合了来自所有提案的每帧信息，通过利用上下文来回归准确的对象框。Sourabh 等人。[135] 将点云投射到基于图像的分割网络的输出中，并将语义预测分数附加到点上。涂漆点被送入现有的检测器[133]、[136]、[137]，以实现显著的性能改进。Yang 等. [138] 将每个点与球形锚点相关联。然后，使用每个点的语义分数来删除多余的锚点。因此，与以前的方法[132]，[133]相比，该方法以更低的计算成本实现了更高的召回率。此外，该文还提出了PointsPool层来学习提案中内部点的紧凑特征，并引入了并行IoU分支，以提高定位精度和检测性能。</p>
<h4 id="基于视锥体的方法"><a href="#基于视锥体的方法" class="headerlink" title="基于视锥体的方法"></a>基于视锥体的方法</h4><p>这些方法首先利用现有的2D目标检测器生成目标的2D候选区域，然后为每个2D候选区域提取3D视锥体方案，如图8（c）所示。尽管这些方法可以有效地提出 3D 对象的可能位置，但分步管道使其性能受到 2D 图像检测器的限制。F-PointNets [139]是这方面的开创性工作。它为每个 2D 区域生成一个视锥体提议，并应用 PointNet [5]（或 PointNet++ [54]）来学习每个 3D 视锥体的点云特征，以进行模态 3D 框估计。在后续工作中，Zhao et al. [140] 提出了一个Point-SENet模块来预测一组比例因子，这些比例因子进一步用于自适应地突出有用的特征并抑制无信息的特征。他们还将PointSIFT[141]模块集成到网络中，以捕获点云的方向信息，从而实现了强大的形状缩放鲁棒性。与F-PointNets[139]相比，该方法在室内和室外数据集上都取得了显著的改进[14]，[25]。Xu等[142]利用2D图像区域及其对应的视锥点来精确地回归3D box。为了融合点云的图像特征和全局特征，他们提出了一个全局融合网络，用于直接回归箱形角位置。他们还提出了一个密集的融合网络，用于预测每个角的逐点偏移。Shin等[143]首先从二维图像中估计了物体的二维边界框和三维姿态，然后提取了多个几何上可行的候选物体。这些 3D 候选对象被输入到 box 回归网络中，以预测准确的 3D 对象box。Wang等[144]为每个2D区域生成了沿视锥轴的视锥体序列，并应用PointNet[5]提取了每个视锥体的特征。对视锥体级特征进行重构以生成 2D 特征图，然后将其输入到全卷积网络中进行 3D box估计。该方法在基于2D图像的方法中实现了最先进的性能，并在KITTI官方排行榜上名列前茅。Johannes 等人。[145] 首先在BEV地图上获得初步的检测结果，然后根据BEV预测提取小点子集（也称为部分区域）。应用局部细化网络来学习部分区域的局部特征，以预测高精度的三维边界框。</p>
<h4 id="其他方法。"><a href="#其他方法。" class="headerlink" title="其他方法。"></a>其他方法。</h4><pre><code> Zhou等[146]将两个3D旋转边界框的IoU集成到几个最先进的检测器[133]、[137]、[158]中，以达到一致的性能改进。Chen 等.[147]提出了一种两阶段网络架构，同时使用点云和体素表示。首先，将点云进行体素化并馈送到 3D 骨干网络以产生初始检测结果。其次，进一步利用初始预测的内点特征进行箱体细化;虽然这种设计在概念上很简单，但它实现 以16.7 fps 的速度性能。Shi等[148]提出了PointVoxel-RCNN（PV-RCNN）来利用3D卷积网络和基于PointNet的集合抽象来学习点云特征。具体来说，首先对输入点云进行体素化，然后输入到3D稀疏卷积网络中，以生成高质量的提议。然后，通过体素集抽象模块将学习到的体素特征编码为一小组关键点。此外，他们还提出了一个关键点到网格的ROI抽象模块，以捕获丰富的上下文信息以进行盒子细化。实验结果表明，该方法的性能明显优于以往方法，在KITTI 3D检测基准的Car类中排名第一。受Hough基于投票的2D目标检测器的启发，Qi等[124]提出了VoteNet，直接从点云中对物体的虚拟中心点进行投票，并通过聚合投票特征生成一组高质量的3D对象提议。VoteNet 明显优于以前仅使用几何信息的方法，并实现了在两个大型室内基准测试（即 ScanNet [11] 和 SUN RGB-D [25]）上的最新性能。然而，对于部分遮挡的物体，虚拟中心点的预测是不稳定的。此外，Feng 等人。[149] 增加了方向向量的辅助分支，以提高虚拟中心点和三维候选框的预测精度。此外，还构建了提议之间的 3D 对象-对象关系图，以强调对准确目标检测有用的特征。Qi等[150]提出了一种ImVoteNet检测器，将2D目标检测线索（例如，几何和语义/纹理线索）融合到3D投票流程中。Shi等[151]受到3D对象的真实值框提供物体内部部件的准确位置的观察的启发，提出了Part-A Net，它由部分感知阶段和部分聚集阶段组成。部件感知阶段应用具有稀疏卷积和稀疏反卷积的类似 UNet [165] 的网络来学习逐点特征，以预测和粗略生成对象内部部件位置。部件聚合阶段采用 RoI 感知池化来聚合预测的部件位置以进行box细化。
</code></pre>
<h3 id="单阶段方法"><a href="#单阶段方法" class="headerlink" title="单阶段方法"></a>单阶段方法</h3><p>这些方法使用单级网络直接预测类概率并回归对象的 3D 边界框。它们不需要区域提案生成和后处理。因此，它们可以高速运行。根据输入数据的类型，单次方法可分为三类：基于BEV的方法、基于离散化的方法和基于点的方法。</p>
<h4 id="基于-BEV-的方法："><a href="#基于-BEV-的方法：" class="headerlink" title="基于 BEV 的方法："></a>基于 BEV 的方法：</h4><p>这些方法主要以 BEV 表示作为输入。Yang等[129]将具有等间距单元的场景的点云离散化，并以类似的方式对反射率进行编码，从而产生正则表示。然后应用全卷积网络 （FCN） 网络来估计物体的位置和航向角。该方法在以 28.6 fps 的速度运行时优于大多数单次拍摄方法（包括 VeloFCN [154]、3D-FCN [155] 和 Vote3Deep [156]）。后来，Yang et al.[152] 利用高清 （HD） 地图提供的几何和语义先验信息来提高 [129] 的鲁棒性和检测性能。具体来说，他们从高精地图中获取地面点的坐标，然后使用相对于地面的距离进行BEV表示，以弥补道路坡度引起的平移方差。另外他们将二元道路掩模与沿通道维度的 BEV 表示连接起来，以专注于移动物体。由于高清地图并非随处可见，他们还提出了一个在线地图预测模块，用于从单个LiDAR点云中估计地图先验。这种地图感知方法在TOR4D [129]、[130]和KITTI [14]数据集上的表现明显优于其基线。然而，其对不同密度点云的泛化性能较差。为了解决这个问题，Beltr’ an et al.[153]提出了一种归一化图，以考虑不同LiDAR传感器之间的差异。归一化地图是一个 2D 网格，其分辨率与 BEV 地图相同，它对每个单元格中包含的最大点数进行编码。结果表明，该归一化图显著提高了基于BEV的探测器的泛化能力。</p>
<h4 id="基于离散化的方法"><a href="#基于离散化的方法" class="headerlink" title="基于离散化的方法"></a>基于离散化的方法</h4><p>这些方法将将点云转换为规则的离散表示，然后应用 CNN 来预测对象的类别和 3D 框。Li等[154]提出了第一种使用FCN进行3D目标检测的方法。 他们将点云转换为2D点图，并使用2D FCN来预测对象的边界框和置信度。后来，他们[155]将点云离散化为具有长、宽、高和通道维度的4D张量，并将基于2D FCN的检测技术扩展到3D域进行3D目标检测。与[154]相比，基于3D FCN的方法[155]在精度上获得了超过20%的增益，但由于3D卷积和数据的稀疏性，不可避免地会消耗更多的计算资源。为了解决体素的稀疏性问题，Engelcke等人。[156] 利用以特征为中心的投票方案为每个非空体素生成一组选票，并通过累积选票获得卷积结果。它的计算复杂度与占用的体素数量成正比。Li 等.[157] 通过堆叠多个稀疏的 3D CNN 构建了 3D 骨干网络。该方法旨在通过充分利用体素的稀疏性来节省内存并加速计算。该 3D 骨干网络可提取丰富的 3D 特征用于目标检测，而不会带来沉重的计算负担。Zhou等[136]提出了一个基于体素的端到端可训练框架VoxelNet。他们将点云划分为等间距的体素，并将每个体素中的特征编码为4D张量。然后连接区域建议网络以生成检测结果。虽然其性能很强，但由于体素和 3D 卷积的稀疏性，这种方法非常慢。后来，Yan等[158]利用稀疏卷积网络[166]提高了[136]的推理效率。他们还提出了正弦误差角损失来解决 0 和 π 方向之间的模糊性。Sindagi等[159]通过在早期阶段融合图像和点云特征来扩展VoxelNet。具体来说，他们将[136]生成的非空体素投影到图像中，并使用预先训练的网络来提取每个投影体素的图像特征。然后，将这些影像要素与体素要素连接起来，以生成精确的 3D 框。与[136]、[158]相比，该方法可以有效地利用多模态信息来减少误报和假负。Lang等[137]提出了一种名为PointPillars的3D目标检测器。该方法利用 PointNet [5] 来学习以垂直列（支柱）组织点云的特征并编码学习的特征作为 Pesudo 图像。然后应用 2D 对象检测管道来预测 3D 边界框。PointPillars在平均精度（AP）方面优于大多数融合方法（包括MV3D [4]、RoarNet [143]和AVOD [126]）。此外，PointPillars 可以在 3D 和 BEV KITTI [14] 基准测试中以 62 fps 的速度运行，使其非常适合实际应用。受到观察到点云的部分空间信息不可避免地在现有单次探测器逐渐缩小的特征图中丢失的启发，He 等人。[160] 提出了一种SA-SSD检测器，利用细粒度结构信息来提高定位精度。具体来说，他们首先将点云转换为张量，并将其输入骨干网络以提取多阶段特征。此外，采用点级监督辅助网络来引导特征学习点云结构。实验结果表明，SA-SSD在KITTI BEV检测基准测试中排名第一。</p>
<h4 id="基于点的方法-1"><a href="#基于点的方法-1" class="headerlink" title="基于点的方法"></a>基于点的方法</h4><p>这些方法直接将原始点云作为输入。3DSSD [161]是这方面的开创性工作。在[133]中，它引入了一种距离-FPS（D-FPS）和特征-FPS（F-FPS）的融合采样策略，以去除耗时的特征传播（FP）层和细化模块。然后，使用候选生成 （CG） 层来充分利用代表性点，这些代表性点被进一步馈送到带有 3D 中心度标签的无锚回归头中，以预测 3D 对象框。实验结果表明，3DSSD在保持25 fps速度的同时，性能优于两阶段基于点的方法PointRCNN [133]。</p>
<h4 id="其他方法-1"><a href="#其他方法-1" class="headerlink" title="其他方法"></a>其他方法</h4><pre><code> Meyer等[162]提出了一种名为LaserNet的高效3D目标检测器。此方法预测每个点的边界框上的概率分布，然后将这些每点分布组合在一起以生成最终的 3D 对象框。此外，该文还以点云的密集距离视图（RV）表示为输入，并提出了一种快速均值偏移算法来降低每点预测产生的噪声。LaserNet 在 0 至 50 米范围内实现了最先进的性能，其运行时间明显低于现有方法。Meyer等[163]随后扩展了LaserNet[162]，以利用RGB图像（例如50至70米）提供的密集纹理。具体来说，他们通过将 3D 点云投影到 2D 图像上来将 LiDAR 点与图像像素相关联，并利用这种关联将 RGB 信息融合到 3D 点中。他们还考虑将3D语义分割作为学习更好表示的辅助任务。该方法在保持LaserNet高效率的同时，在远距离（例如50至70米）目标检测和语义分割方面都取得了显着改善。Chen等[164]观察到孤立物体部位上的点可以提供有关物体位置和方向的丰富信息，受到这一观察的启发，提出了一种新的热点表示和第一个基于热点的无锚点检测器， 具体来说，首先将原始点云体素化，然后输入骨干网络以生成 3D 特征图。这些特征图用于对热点进行分类，同时预测 3D 边界框。注意，热点被分配在骨干网络的最后一个卷积层。实验结果表明，该方法具有相当的性能，并且对稀疏点云具有鲁棒性。Shi et el.[125] 提出了一种图神经网络Point-GNN，用于检测激光雷达点云中的3D物体。他们首先将输入点云编码为具有固定半径的近邻图，然后将该图输入Point-GNN以预测物体的类别和框。
</code></pre>
<h2 id="3D-对象跟踪"><a href="#3D-对象跟踪" class="headerlink" title="3D 对象跟踪"></a>3D 对象跟踪</h2><p>给定一个物体在第一帧中的位置，物体跟踪的任务是估计它在随后的帧中的状态[167]，[168]。由于3D目标跟踪可以使用点云中丰富的几何信息，因此有望克服基于图像的跟踪面临的几个缺点，包括遮挡、照明和比例变化。受到Siamese网络[169]在基于图像的目标跟踪方面的成功启发，Giancola等[170]提出了一种具有形状完成正则化的3D Siamese网络。具体来说，他们首先使用卡尔曼滤波器生成候选者，并使用形状正则化将模型和候选者编码为紧凑的表示。然后，余弦相似度用于搜索被跟踪对象在下一帧中的位置。该方法可以用作目标跟踪的替代方法，并且明显优于大多数2D目标跟踪方法，包括STAPLE[171]和SiamFC [169]。为了有效地搜索目标对象，Zarzar 等人。[172] 利用 2D Siamese 网络在 BEV 表示上生成大量粗略候选对象。然后，他们通过利用3D Siamese网络中的余弦相似性来完善候选者。该方法在精度（即18%）和成功率（即12%）方面都明显优于[170]。Simon等[173]提出了一种用于语义点云的3D目标检测和跟踪架构。他们首先通过融合2D视觉语义信息生成体素化语义点云，然后利用时间信息提高多目标跟踪的准确性和鲁棒性。此外，他们还引入了一个强大而简化的评估指标（即标度-旋转-平移分数 （SRF））来加速训练和推理。ComplexerYOLO实现了令人鼓舞的跟踪性能，并且仍然可以实时运行。此外，Qi 等人。[174]提出了点对盒（P2B）网络。他们将模板和搜索区域输入骨干以获得种子。搜索区域种子被增强为特定于目标的特征，然后通过 Hough 投票回归潜在的目标中心。实验结果表明，当以40 fps的速度运行时，P2B的性能比[170]高出10%以上。</p>
<h2 id="3D-场景流估计"><a href="#3D-场景流估计" class="headerlink" title="3D 场景流估计"></a>3D 场景流估计</h2><p>给定两个点云<img src="/images/94e79ad0c1aabeafef9e2fc4af6adf66.svg"><br>和 <img src="/images/6204886f5cc39a4b860ea98a7e95af1d.svg"><br>,3D 场景流 <img src="/images/6204886f5cc39a4b860ea98a7e95af1d.svg"><br>, 使得 <img src="/images/04272d36bac122a9e96b2ca42116dca4.svg"><br>,图9 显示了两个KITTI点云之间的3D场景流，类似2d视觉中的光流估计，已经有几种方法开始从一系列点云中学习有用的信息（例如.3D场景流，空间临时信息）.<img src="/images/1701243339739-524c9664-1203-430b-97b9-3ad2a08c877f.png"></p>
<pre><code>图9：两个KITTI点云之间的3D场景流，最初如图[175]所示。点云 X、Y 和 X 的平移点云分别以红色、绿色和蓝色突出显示。
</code></pre>
<p>Liu等[175]提出FlowNet3D直接从一对连续的点云中学习场景流。FlowNet3D 通过流嵌入层学习点级特征和运动特征。但是，FlowNet3D 存在两个问题。首先，一些预测的运动矢量在方向上与真实值有很大不同。其次，FlowNet难以应用于非静态场景，尤其是以可变形物体为主的场景。为了解决这个问题，Wang等人。[176] 引入了余弦距离损失，以最小化预测与真实值之间的角度。此外，他们还提出了点到平面距离损失，以提高刚性和动态场景的准确性。实验结果表明，这两个损失项将FlowNet3D的准确率从57.85%提高到63.43%，加快并稳定了训练过程。顾等.[177] 提出了一种分层变面体晶格流网（HPLFlowNet）来直接估计大规模点云的场景流。提出了几个双边卷积层，用于从原始点云中恢复结构信息，同时降低计算成本。为了有效地处理序列点云，Fan和Yang[178]提出了PointRNN、PointGRU和PointLSTM网络和序列到序列模型，用于跟踪移动点。PointRNN、PointGRU 和 PointLSTM 能够捕获空间临时信息并对动态点云进行建模。同样，Liu et al.[179] 提出MeteorNet直接从动态点云中学习表示。此方法学习聚合来自时空相邻点的信息。进一步引入直接分组和链流分组来确定时间邻域。然而，上述方法的性能受到数据集规模的限制。Mittal等[180]提出了两种自监督损失，以在大型未标记数据集上训练他们的网络。他们的主要思想是，鲁棒的场景流估计方法应该在前向和后向预测中都有效。由于场景流注释不可用，预测变换点的最近邻点被视为 pesudo 真值。但是，真正的地面实况可能与最近的点不同。为了避免这个问题，他们计算了相反方向的场景流，并提出了一个循环一致性损失，以将点平移到原始位置。实验结果表明，这种自监督方法超越了基于监督学习的方法的最新性能。</p>
<h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p>KITTI [14] 基准测试是自动驾驶领域最具影响力的数据集之一，在学术界和工业界都得到了广泛应用。表 3 和表 4 显示了不同探测器在 KITTI 测试 3D 基准测试中取得的结果。可以进行以下观察：</p>
<ol>
<li>基于区域提案的方法是这两个类别中最常研究的方法，并且在 KITTI 测试 3D 和 BEV 基准测试中都大大优于单次方法。</li>
<li>现有的 3D 对象检测器有两个限制。首先，现有方法的远距离检测能力相对较差。其次，如何充分利用图像中的纹理信息仍然是一个悬而未决的问题。</li>
<li>多任务学习是 3D 目标检测的未来发展方向。例如，MMF [128]学习跨模态表示，通过合并多个任务来实现最先进的检测性能</li>
<li>3D目标跟踪和场景流估计是新兴的研究课题，自2019年以来逐渐受到越来越多的关注。</li>
</ol>
<h1 id="3D点云分割"><a href="#3D点云分割" class="headerlink" title="3D点云分割"></a>3D点云分割</h1><p>3D 点云分割需要了解全局几何结构和每个点的细粒度细节。根据分割粒度，3D点云分割方法可分为语义分割（场景级）、实例分割（对象级）和部分分割（部分级）三大类。</p>
<h2 id="3D语义分割"><a href="#3D语义分割" class="headerlink" title="3D语义分割"></a>3D语义分割</h2><p>给定一个点云，语义分割的目标是根据点的语义含义将其分成几个子集。与 3D 形状分类的分类法（第 3 节）类似，语义分割有四种范式：基于投影、基于离散化、基于点和混合方法。基于投影和离散化方法的第一步是将点云转换为中间正则表示，例如多视图[181]、[182]、球面[183]、[184]、[185]、体积[166]、[186]、[187]、变面体晶格[188]、[189]和混合表示[190]、[191]，如图11所示。然后将中间分割结果投影回原始点云。相比之下，基于点的方法直接在不规则的点云上工作。几种具有代表性的方法如图10所示。<img src="/images/1701243836433-ee84fa7f-4b94-43d3-8a30-468250593e35.png"><br>图 10：最相关的基于深度学习的 3D 语义分割方法的时间顺序概述。</p>
<p><img src="/images/1701243857825-ab0293b6-e0fe-46ce-a0e1-0ebea7f4fc3b.png"><br>         图 11：中间表示的图示。（a）和（b）最初分别出现在[182]和[183]中。a是多视图表示，b是球面表示，c是密集离散化表示，d是稀疏离散化表示。</p>
<h3 id="基于投影的方法"><a href="#基于投影的方法" class="headerlink" title="基于投影的方法"></a>基于投影的方法</h3><p>这些方法通常将 3D 点云投影到 2D 图像中，包括多视图和球面图像。</p>
<h4 id="多视图表示"><a href="#多视图表示" class="headerlink" title="多视图表示"></a>多视图表示</h4><pre><code> Lawin等[181]首先将3D点云从多个虚拟相机视图投射到2D平面上。然后，使用多流 FCN 来预测合成图像的像素分数。通过将重新投影的分数融合到不同视图上，可以得到每个点的最终语义标签。类似地，Boulch等[182]首先使用多个相机位置生成了点云的几个RGB和深度快照。然后，他们使用2D分割网络对这些快照进行像素级标记。使用残差校正进一步融合从RGB和深度图像预测的分数[192]。Tatarchenko等[193]基于点云是从局部欧几里得曲面采样的假设，引入了用于密集点云分割的切线卷积。此方法首先将每个点周围的局部曲面几何投影到虚拟切平面。然后直接在曲面几何上操作切线卷积。该方法具有很强的可扩展性，能够处理具有数百万个点的大规模点云。总体而言，多视图分割方法的性能对视点选择和遮挡很敏感。此外，这些方法没有充分利用底层的几何和结构信息，因为投影步骤不可避免地会引入信息丢失。
</code></pre>
<h4 id="球形表示"><a href="#球形表示" class="headerlink" title="球形表示"></a>球形表示</h4><p>为了实现快速、准确的三维点云分割，Wu等[183]提出了一种基于SqueezeNet[194]和条件随机场（CRF）的端到端网络。为了进一步提高分割精度，引入SqueezeSegV2[184]，利用无监督的域适应流水线解决域偏移问题。Milioto等[185]提出了用于LiDAR点云实时语义分割的RangeNet++。首先将2D距离图像的语义标签传输到3D点云中，进一步使用基于GPU的基于KNN的高效后处理步骤，以缓解离散化误差和推理输出模糊的问题。与单视图投影相比，球面投影保留了更多的信息，适用于LiDAR点云的标注。 但是，这种中间表示不可避免地会带来一些问题，例如离散化错误和遮挡。</p>
<h3 id="基于离散化的方法-1"><a href="#基于离散化的方法-1" class="headerlink" title="基于离散化的方法"></a>基于离散化的方法</h3><p> 这些方法通常将点云转换为密集&#x2F;稀疏离散表示，例如体素和稀疏的排列晶格。</p>
<h4 id="密集离散化表示"><a href="#密集离散化表示" class="headerlink" title="密集离散化表示"></a>密集离散化表示</h4><p>这种方法通常将点云体素化为密集网格，然后利用标准的 3D 卷积。Huang等[195]首先将点云划分为一组占用体素，然后将这些中间数据馈送到全3D CNN进行体素分割。最后，体素内的所有点都分配了与体素相同的语义标签。该方法的性能受到体素粒度和点云分区导致的边界伪影的严重限制。此外，Tchapmi 等人。[196] 提出SEGCloud实现细粒度和全局一致的语义分割，该方法引入确定性三线性插值，将3D-FCNN[197]生成的粗体素预测映射回点云，然后使用全连接CRF（FCCRF）来加强这些推断的每点标签的空间一致性。Meng 等.[186] 引入了一种基于内核的插值变分自动编码器架构，用于对每个体素内的局部几何结构进行编码。对每个体素使用 RBF 来获取连续表示并捕获每个体素中点的分布，而不是二进制占用表示。VAE进一步用于将每个体素内的点分布映射到紧凑的潜在空间。然后，使用对称群和等价CNN来实现鲁棒的特征学习。由于 3D CNN 的良好可扩展性，基于体积的网络可以自由地在具有不同空间大小的点云上进行训练和测试。在全卷积点网络（FCPN）[187]中，首先从点云中分层抽象出不同层次的几何关系，然后使用三维卷积和加权平均池化来提取特征并纳入长程依赖关系。该方法可以处理大规模点云，在推理过程中具有良好的可扩展性。Dai等[198]提出了ScanComplete来实现3D扫描完成和每体素语义标记。该方法利用了全卷积神经网络的可扩展性，可以在训练和测试期间适应不同的输入数据大小。使用从粗到细的策略来分层提高预测结果的分辨率。总体而言，体积表示自然地保留了 3D 点云的邻域结构。其常规数据格式还允许直接应用标准 3D 卷积。这些因素导致了该领域性能的稳步提高。然而，体素化步骤本身就引入了离散化伪影和信息丢失。通常，高分辨率会导致高内存和计算成本，而低分辨率会导致细节丢失。在实践中，选择合适的网格分辨率并非易事。</p>
<h4 id="稀疏离散化表示"><a href="#稀疏离散化表示" class="headerlink" title="稀疏离散化表示"></a>稀疏离散化表示</h4><p>体素表示自然是稀疏的，因为非零值的数量只占很小的百分比。因此，在空间稀疏数据上应用密集卷积神经网络是低效的。为此，Graham 等人。[166] 提出了基于索引结构的子流形稀疏卷积网络。这种方法通过将卷积的输出限制为仅与占用的体素相关，从而显着降低了内存和计算成本。同时，其稀疏卷积也可以控制提取特征的稀疏性。这种子流形稀疏卷积适用于高维和空间稀疏数据的高效处理。此外，Choy等人。[199] 提出了一种名为MinkowskiNet的4D时空卷积神经网络，用于3D视频感知。该文提出一种广义稀疏卷积法，用于有效处理高维数据。进一步应用三边-稳态条件随机场来强制一致性。另一方面，Su等[188]提出了基于双边卷积层（BCL）的稀疏格子网络（SPLATNet）。该方法首先将原始点云插值到稀疏排列晶格上，然后应用BCL对稀疏晶格的占用部分进行卷积。然后将滤波后的输出插值回原始点云。此外，该方法允许对多视图图像和点云进行灵活的联合处理。此外，Rosu等[189]提出了LatticeNet来实现大型点云的高效处理。此外，还引入了一个名为 DeformsSlice 的数据相关插值模块，用于将晶格特征反向投影到点云。</p>
<h3 id="混合方法"><a href="#混合方法" class="headerlink" title="混合方法"></a>混合方法</h3><p>为了进一步利用所有可用信息，已经提出了几种从3D扫描中学习多模态特征的方法。Dai 和 Nießner [190] 提出了一个联合 3D 多视图网络，将 RGB 特征和几何特征结合起来。使用3D CNN流和多个2D流提取特征，并提出可微背投影层，将学习到的2D嵌入和3D几何特征共同融合。此外，Chiang et al.[200] 提出了一个统一的基于点的框架，用于从点云中学习 2D 纹理外观、3D 结构和全局上下文特征。该方法直接应用基于点的网络从稀疏采样的点集中提取局部几何特征和全局上下文，而无需任何体素化。Jaritz 等人。[191] 提出了多视图点网络（MVPNet）来聚合来自规范点云空间中二维多视图图像和空间几何特征的外观特征。</p>
<h3 id="基于点的方法-2"><a href="#基于点的方法-2" class="headerlink" title="基于点的方法"></a>基于点的方法</h3><p>基于点的网络直接在不规则的点云上工作。然而，点云是无序和非结构化的，因此无法直接应用标准 CNN。为此，提出了开创性的工作PointNet [5]，即使用共享MLP学习每点特征，并使用对称池化函数学习全局特征。基于PointNet，最近提出了一系列基于点的网络。总的来说，这些方法可以大致分为逐点MLP方法、点卷积方法、基于RNN的方法和基于图的方法。</p>
<h4 id="逐点-MLP-方法"><a href="#逐点-MLP-方法" class="headerlink" title="逐点 MLP 方法"></a>逐点 MLP 方法</h4><p>这些方法通常使用共享MLP作为其网络中的基本单元，以实现其高效率。然而，共享MLP提取的逐点特征无法捕获点云中的局部几何形状以及点之间的相互相互作用[5]。为了捕获每个点的更广泛上下文并学习更丰富的局部结构，引入了几个专用网络，包括基于相邻特征池化、基于注意力的聚合和局部-全局特征串联的方法。</p>
<h5 id="相邻要素池化"><a href="#相邻要素池化" class="headerlink" title="相邻要素池化"></a>相邻要素池化</h5><p>捕获局部几何模式，这些方法通过聚合来自本地相邻点的信息来学习每个点的特征。特别是，PointNet++ [54] 对点进行分层分组，并逐步从更大的局部区域学习，如图所示。 第12条（a）款。为了克服点云不均匀和密度变化带来的问题，还提出了多尺度分组和多分辨率分组的方法。后来，Jiang等[141]提出了PointSIFT模块来实现方向编码和尺度感知。该模块通过三阶段有序卷积对来自八个空间方向的信息进行堆叠和编码。将多尺度特征串联起来，以实现对不同尺度的适应。与PointNet++中使用的分组技术（即球查询）不同，Engelmann等[204]利用K-means聚类和KNN分别定义了世界空间和特征空间中的两个邻域。基于同一类的点在特征空间中预计更接近的假设，引入了成对距离损失和质心损失，以进一步规范化特征学习。为了模拟不同点之间的相互作用，Zhao等人.  [57]提出PointWeb，通过密集构建局部全链接网络来探索局部区域中所有点对之间的关系。该文提出一种自适应特征调整（AFA）模块，实现信息交换和特征细化。此聚合操作有助于网络学习判别性特征表示。Zhang等[205]基于同心球壳的统计量，提出了一种称为Shellconv的排列不变卷积。该方法首先查询一组多尺度同心球体，然后在不同壳体中使用max-pooling操作对统计量进行汇总，利用MLP和一维卷积得到最终的卷积输出。胡等.[206] 提出了一种高效且轻量级的网络，称为 RandLA-Net，用于大规模点云分割。该网络利用随机点采样在内存和计算方面实现了非常高的效率。进一步提出一种局部特征聚合模块，用于捕获和保留几何特征。</p>
<h5 id="基于注意力的聚合"><a href="#基于注意力的聚合" class="headerlink" title="基于注意力的聚合"></a>基于注意力的聚合</h5><p>在点云分割中引入一种注意力机制[120]进一步改善细分市场, Yang 等.[56] 提出了一种群体洗牌注意力来模拟点之间的关系，并提出了一种排列不变、与任务无关和可微分的 Gumbel 子集采样 （GSS） 来取代广泛使用的 FPS 方法。该模块对异常值不太敏感，可以选择具有代表性的点子集。为了更好地捕捉点云的空间分布，Chen等人。[207] 提出了一种基于空间布局和点云局部结构的局部空间感知（LSA）层来学习空间感知权重。与CRF类似，Zhao等人。[208] 提出了一个基于注意力的分数细化（ASR）模块，用于对网络产生的分割结果进行后处理。通过将相邻点的分数与学习到的注意力权重合并来优化初始分割结果。该模块可以很容易地集成到现有的深度网络中，以提高分割性能。</p>
<h5 id="局部-全局串联"><a href="#局部-全局串联" class="headerlink" title="局部-全局串联"></a>局部-全局串联</h5><p>Zhao等[112]提出了一种排列不变的PS-Net，以整合点云的局部结构和全局上下文。Edgeconv[87]和NetVLAD [209]反复堆叠，以捕获局部信息和场景级全局特征。</p>
<h3 id="点卷积方法"><a href="#点卷积方法" class="headerlink" title="点卷积方法"></a>点卷积方法</h3><pre><code> 这些方法倾向于为点云提出有效的卷积算子。Hua等[76]提出了一种逐点卷积算子，将相邻点分箱到核单元中，然后与核权重进行卷积。如图12（b）所示，Wang等[201]提出了一种基于参数化连续卷积层的PCCN网络。该层的核函数由MLP参数化，并跨越连续向量空间。托马斯等人。[65]提出了一种基于核点卷积（KPConv）的核点全卷积网络（KP-FCNN）。具体来说，KPConv的卷积权重由到核点的欧几里得距离决定，核点的数量不是固定的。核点的位置被表述为球体空间中最佳覆盖的优化问题。请注意，半径邻域用于保持一致的感受野，而网格子采样用于在每一层中实现不同密度点云下的高鲁棒性。在[211]中，Engelmann等人。提供了丰富的消融实验和可视化结果，以显示感受野对基于聚集的方法性能的影响。他们还提出了一种扩张点卷积 （DPC） 操作来聚合扩张的相邻特征，而不是 K 个最近邻。这种操作被证明在增加感受野方面非常有效，并且可以很容易地集成到现有的基于聚合的网络中。
</code></pre>
<h3 id="基于-RNN-的方法"><a href="#基于-RNN-的方法" class="headerlink" title="基于 RNN 的方法"></a>基于 RNN 的方法</h3><p>为了从点云中捕获固有的上下文特征，循环神经网络 （RNN） 也被用于点云的语义分割。基于PointNet [5]，Engelmann等人。[213] 首先将点块转换为多尺度块和网格块，以获得输入级上下文。然后，将 PointNet 提取的按块特征依次馈送到合并单元 （CU） 或循环合并单元 （RCU） 中，以获得输出级上下文, 实验结果表明，结合空间环境对分割性能的提高具有重要意义。Huang 等.[212] 提出了一个轻量级的局部依赖建模模块，并利用切片池化层将无序点特征集转换为有序的特征向量序列。如图12（c）所示，Ye等人。[202]首先提出了一个逐点金字塔池化（Pointwise Pyramid Pooling，3P）模块来捕获从粗到细的局部结构，然后利用双向分层RNN进一步获得长程空间依赖性。然后应用 RNN 来实现端到端学习。然而，这些方法在将局部邻域特征与全局结构特征聚合时，会丢失点云中丰富的几何特征和密度分布[220]。为了缓解刚性和静态池化操作带来的问题，Zhao et al.[220]提出了一种动态聚合网络（DARNet）来考虑全局场景复杂性和局部几何特征。使用自适应感受野和节点权重动态聚合中型特征。Liu等.[221] 提出了3DCNN-DQN-RNN，用于大规模点云的高效语义解析。该网络首先使用3D CNN网络学习空间分布和颜色特征，DQN进一步用于定位属于特定类别的对象。最终的级联特征向量被送入残差RNN以获得最终的分割结果。</p>
<h3 id="基于图的方法-1"><a href="#基于图的方法-1" class="headerlink" title="基于图的方法"></a>基于图的方法</h3><p>为了捕获 3D 点云的底层形状和几何结构，有几种方法求助于图形网络。如图12（d）所示，Landrieu等人。[203] 将点云表示为一组相互连接的简单形状和超点，并使用属性有向图（即超点图）来捕获结构和上下文信息。然后，将大规模点云分割问题分解为3个子问题，即几何齐次分割、超点嵌入和上下文分割。为了进一步改进分割步骤，Landrieu和Boussaha[214]提出了一个监督框架，将点云过度分割为纯超点。这个问题被表述为一个由邻接图构建的深度度量学习问题。此外，还提出了一种图结构的对比损失来帮助识别物体之间的边界。为了更好地捕捉高维空间中的局部几何关系，Kang等[222]提出了一种基于图嵌入模块（GEM）和金字塔注意力网络（PAN）的PyramNet。GEM模块将点云表述为有向无环图，并利用协方差矩阵代替欧氏距离来构建相邻相似矩阵。PAN模块中使用了四种不同大小的卷积核来提取具有不同语义强度的特征。在文献[215]中，提出了图注意力卷积（GAC）从局部相邻集选择性地学习相关特征。该操作是通过根据不同的相邻点和特征通道的空间位置和特征差异动态地为其分配注意力权重来实现的。GAC可以学习捕获判别特征进行分割，并且具有与常用的CRF模型相似的特征。Ma等[223]提出了一种点全局上下文推理（PointGCR）模块，使用无向图表示用于沿通道维度捕获全局上下文信息。PointGCR 是一个即插即用的端到端可训练模块。它可以很容易地集成到现有的分段网络中，以实现性能改进。此外，最近的几项工作试图在弱监督下实现点云的语义分割。Wei等[224]提出了一种两阶段方法来训练具有子云级标签的分割网络。徐， 等.[225] 研究了几种用于点云语义分割的不精确监督方案。他们还提出了一个网络，该网络只能使用部分标记的点（例如10%）进行训练。</p>
<h2 id="实例分割"><a href="#实例分割" class="headerlink" title="实例分割"></a>实例分割</h2><p>与语义分割相比，实例分割更具挑战性，因为它需要更准确、更细粒度的点推理。特别是，它不仅需要区分具有不同语义意义的点，还需要区分具有相同语义含义的单独实例。总体而言，现有方法可以分为两组：基于提案的方法和无提案的方法。图 13 说明了几种里程碑方法。<img src="/images/1701245931607-54e4b3be-f89a-4555-b238-a29f512807fb.png"><br>图 13：最相关的基于深度学习的 3D 实例分割方法的时间顺序概述。</p>
<h3 id="基于提案的方法"><a href="#基于提案的方法" class="headerlink" title="基于提案的方法"></a>基于提案的方法</h3><p>这些方法将实例分割问题转换为两个子任务：3D 对象检测和实例掩码预测。Hou等[226]提出了一种3D全卷积语义实例分割（3D-SIS）网络，用于实现RGB-D扫描的语义实例分割。该网络从颜色和几何特征中学习。与 3D 对象检测类似，3D 区域建议网络 （3DRPN） 和 3D 感兴趣区域 （3D-RoI） 图层用于预测边界框位置、对象类标签和实例掩码。Yi等[227]遵循边合成分析策略，提出了生成形状提案网络（GSPN）来生成高对象性的3D提案。这些建议由基于区域的 PointNet （R-PointNet） 进一步完善。通过预测每个类标签的每个点的二进制掩码来获得最终标签。与点云中三维边界框的直接回归不同，该方法通过强制几何理解去除了大量无意义的提议。成田等[228]通过将二维全景分割扩展到三维映射，提出了一种在线体积三维映射系统，共同实现大规模三维重建、语义标注和实例分割。他们首先利用 2D 语义和实例分割网络来获得像素级全景标签，然后将这些标签集成到体积图中。进一步使用全连接的CRF来实现准确的分割。这种语义映射系统可以实现高质量的语义映射和判别性目标识别。Yang等[229]提出了一种单阶段、无锚点和端到端的可训练网络，称为3D-BoNet，用于在点云上实现实例分割。该方法直接回归所有潜在实例的粗略 3D 边界框，然后利用点级二元分类器来获取实例标签。特别地，将边界框生成任务表述为最优赋值问题。此外，还提出了一种多准则损失函数来对生成的边界框进行正则化。这种方法不需要任何后处理，并且计算效率很高。Zhang等.[230] 提出了一种用于大规模室外LiDAR点云分割的网络。该方法使用自注意力块在点云的鸟瞰图上学习特征表示。最终实例标签是根据预测的水平中心和高度限制获得的。Shi等[231]提出了一种层次感知变分去噪递归自编码器（VDRAE）来预测室内三维空间的布局。对象建议通过递归上下文聚合和传播以迭代方式生成和优化。总体而言，基于提案的方法[226]、[227]、[229]、[232]直观明了，实例分割结果通常具有较好的客观性。但是，这些方法需要多阶段的训练和对冗余提案的修剪。因此，它们通常耗时且计算成本高昂。</p>
<h3 id="基于无提案的方法"><a href="#基于无提案的方法" class="headerlink" title="基于无提案的方法"></a>基于无提案的方法</h3><p>无提案方法 [233]、[234]、[235]、[236]、[237]、[238]、[239]、[240] 没有目标检测模块。相反，他们通常将实例分割视为语义分割之后的后续聚类步骤。特别是，大多数现有方法都基于属于同一实例的点应具有非常相似的特征的假设。因此，这些方法主要集中在判别特征学习和点分组上。Wang等[233]在一项开创性的工作中，首先引入了相似性组提案网络（SGPN）。该方法首先学习每个点的特征和语义图，然后引入相似度矩阵来表示每个配对特征之间的相似度。了解更多判别特征，他们使用双铰链损失来相互调整相似性矩阵和语义分割结果。最后，采用启发式和非极大值抑制方法将相似点合并为实例。由于相似矩阵的构建需要大量的内存消耗，因此该方法的可扩展性受到限制。同样，Liu et al.[237]首先利用子流形稀疏卷积[166]来预测每个体素的语义分数和相邻体素之间的亲和力。然后，他们引入了一种聚类算法，根据预测的亲和力和网格拓扑将点分组到实例中。Mo 等人。[241] 在PartNet中引入了一个按分割检测网络来实现实例分割。PointNet++ 作为主干，用于预测每个点的语义标签和不相交的实例掩码。此外，Liang 等人。[238] 提出了一种结构感知损失，用于学习判别嵌入。这种损失既考虑了要素的相似性，也考虑了点之间的几何关系。基于注意力的图CNN进一步用于通过聚合来自邻居的不同信息来自适应地细化学习的特征。</p>
<p>由于一个点的语义类别和实例标签通常是相互依赖的，因此提出了几种方法将这两个任务耦合到一个任务中。Wang等.[234] 通过引入端到端且可学习的关联分割实例和语义 （ASIS） 模块来集成这两个任务。实验表明，语义特征和实例特征可以相互支持，通过该ASIS模块实现性能提升。类似地，Zhao等[242]提出了JSNet来实现语义和实例分割。此外，Pham 等人。[235]首先引入了多任务逐点网络（MT-PNet）为每个点分配一个标签，并通过引入判别损失来规范特征空间中的嵌入[243]。然后，他们将预测的语义标签和嵌入融合到多值条件随机场（MV-CRF）模型中，以进行联合优化。最后，使用平均场变分推理来生成语义标签和实例标签。胡等. [244]首先提出了一种动态区域增长（DRG）方法，将点云动态地分离成一组不相交的补丁，然后使用无监督的Kmeans++算法对所有这些补丁进行分组。然后，在补丁之间的上下文信息的指导下执行多尺度补丁分割。最后，将这些标记的补丁合并到对象级别，以获得最终的语义和实例标签。</p>
<p>为了实现全3D场景的实例分割，Elich等[236]提出了一种混合2D-3D网络，从BEV表示和点云的局部几何特征中联合学习全局一致的实例特征。然后将学习到的特征组合起来，实现语义和实例分割。请注意，与启发式的GroupMerging算法[233]不同，而是使用更灵活的Meanshift [245]算法将这些点分组到实例中。或者，还引入了多任务学习来进行实例分割。Lahoud等[246]学习了每个实例的独特特征嵌入和方向信息来估计目标的中心。提出特征嵌入损失和方向损失来调整潜在特征空间中学习的特征嵌入。采用均值偏移聚类和非最大值抑制将体素分组到实例中. 该方法在 ScanNet [11] 基准测试中实现了最先进的性能。此外，预测的方向信息对于确定实例的边界特别有用。Zhang等. [247] 将概率嵌入引入点云的实例分割。该方法还结合了不确定性估计，并提出了一种新的聚类步骤损失函数。Jiang 等. [240] 提出了一种PointGroup网络，该网络由语义分割分支和偏移预测分支组成。进一步利用双集聚类算法和ScoreNet来获得更好的分组结果。总之，无提案方法不需要计算成本高昂的区域提案组件。但是，由于这些方法不显式检测对象边界，因此由这些方法分组的实例段的对象性通常较低。</p>
<h2 id="部件分割"><a href="#部件分割" class="headerlink" title="部件分割"></a>部件分割</h2><p>3D 形状零件分割的困难是双重的。首先，具有相同语义标签的形状部分具有较大的几何变化和模糊性。其次，具有相同语义的对象中的部件数量可能不同。</p>
<p>VoxSegNet[248]提出在有限的解决方案下对3D体素化数据进行细粒度的零件分割。该文提出一种空间密集提取（SDE）模块（由堆叠残差块组成），用于从稀疏体积数据中提取多尺度判别特征。通过逐步应用注意力特征聚合 （AFA） 模块，进一步重新加权和融合学习到的特征。Kalogerakis等[249]将FCN和基于表面的CRF相结合，实现了端到端的3D部件分割。他们首先从多个视图生成图像以实现最佳表面覆盖，并将这些图像输入 2D 网络以生成置信度图。然后，这些置信度图由基于表面的 CRF 聚合，该 CRF 负责对整个场景进行一致的标记。Yi等[250]引入了一种同步光谱CNN（SyncSpecCNN）对不规则和非同构形状图进行卷积处理。引入膨胀卷积核的谱参数化和谱变换器网络，解决了零件多尺度分析和形状间信息共享的问题。</p>
<p>Wang等[251]首先通过引入形状全卷积网络（SFCN）并采用3个低级几何特征作为输入，对3D网格进行了形状分割。然后，他们利用基于投票的多标签图切割来进一步完善分割结果。Zhu等[252]提出了一种用于3D形状共分割的弱监督CoSegNet。该网络将一组未分割的 3D 点云形状作为输入，并通过迭代最小化组一致性损失来生成形状部分标签。与CRF类似，提出了一种预训练的部件细化网络，以进一步细化和去噪部件方案。Chen等[253]提出了一种分支自编码器网络（BAE-NET），用于无监督、单次和弱监督的3D形状协同分割。该方法将形状共分割任务表述为表示学习问题，旨在通过最小化形状重构损失来找到最简单的部分表示。基于编码器-解码器架构，该网络的每个分支都可以学习特定零件形状的紧凑表示。然后，将从每个分支和点坐标中学习到的特征馈送到解码器以生成二进制值（指示该点是否属于该部分）。该方法具有良好的泛化能力，可以处理大型3D形状集合（最多5000+形状）。然而，该方法对初始参数敏感，且未将形状语义纳入网络，这阻碍了该方法在每次迭代中获得鲁棒稳定的估计。Yu 等.[254] 提出了一种自上而下的递归部分分解网络（PartNet），用于分层形状分割。与现有的将形状分割为固定标签集的方法不同，该网络将零件分割表述为级联二值标记问题，并根据几何结构将输入点云分解为任意数量的部件。Luo 等.[255] 引入了一个基于学习的分组框架，用于零镜头 3D 部件分割任务。为了提高跨类别泛化能力，该方法倾向于学习分组策略，该策略限制网络在部分局部上下文中学习部分级特征。</p>
<h2 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h2><p>表 5 显示了现有方法在公共基准测试上取得的结果，包括 S3DIS [10]、Semantic3D [12]、ScanNet [39] 和 SemanticKITTI [15]。以下问题需要进一步调查：</p>
<ol>
<li>由于具有常规的数据表示，基于投影的方法和基于离散化的方法都可以利用其二维图像对应物的成熟网络架构。然而，基于投影的方法的主要局限性在于3D-2D投影导致的信息丢失，而基于离散化的方法的主要瓶颈是分辨率的提高导致的计算和内存成本的立方增加。为此，基于索引结构构建稀疏卷积将是一个可行的解决方案，值得进一步探索。</li>
<li>基于点的网络是最常研究的方法。然而，点表示自然没有明确的邻域信息，大多数现有的基于点的方法都诉诸于昂贵的邻域搜索机制（例如，KNN [79]或球查询[54]）。这固有地限制了这些方法的效率，最近提出的点-体素联合表示[256]将是进一步研究的一个有趣方向。</li>
<li>在不平衡的数据中学习仍然是点云分割中一个具有挑战性的问题。尽管有几种方法[65]、[203]、[205]取得了显著的整体表现，但它们在少数群体中的表现仍然有限。例如，RandLA-Net [206] 在 Semantic3D 的 reduced-8 子集上实现了 76.0% 的总体 IoU，但在硬景观类上实现了 41.1% 的非常低的 IOU</li>
<li>大多数现有方法[5]、[54]、[79]、[205]、[207]适用于小点云（例如，1m×1m，4096个点）。在实践中，深度传感器获取的点云通常是巨大且大规模的。因此，需要进一步研究大规模点云的高效分割问题。</li>
<li>一些工作[178]、[179]、[199]已经开始从动态点云中学习时空信息。预计时空信息有助于提高后续任务的性能，例如3D目标识别、分割和完成。</li>
</ol>
<p><img src="/images/1701310801343-6bb38215-4a6c-481b-a922-9c1480150799.png"></p>
<p>表 5：S3DIS（包括 Area5 和 6 倍交叉验证）[10]、Semantic3D（包括语义 8 和简化 8 子集）[12]、ScanNet [11] 和 SemanticKITTI [15] 数据集上的语义分割结果比较结果。总体准确度 （OA）、平均交集与并集 （mIoU） 是主要的评估指标。为简单起见，我们省略了值后面的“%”。符号“-”表示结果不可用。</p>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>本文介绍了最先进的 3D 理解方法的当代综述，包括 3D 形状分类、3D 目标检测和跟踪以及 3D 场景和对象分割。已经提出了这些方法的综合分类和性能比较。还涵盖了各种方法的优缺点，并列出了潜在的研究方向。</p>
<blockquote>
</blockquote>
<h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br></pre></td><td class="code"><pre><span class="line"> [1] Z. Liang, Y. Guo, Y. Feng, W. Chen, L. Qiao, L. Zhou, J. Zhang,and H. Liu, “Stereo matching using multi-level cost volume andmulti-scale feature constancy,” IEEE TPAMI, 2019.</span><br><span class="line"> [2] Y. Guo, F. Sohel, M. Bennamoun, M. Lu, and J. Wan, “Rotationalprojection statistics for 3D local surface description and objectrecognition,” IJCV, 2013.</span><br><span class="line"> [3] Y. Guo, M. Bennamoun, F. Sohel, M. Lu, and J. Wan, “3D objectrecognition in cluttered scenes with local surface features: asurvey,” IEEE TPAMI, 2014.</span><br><span class="line"> [4] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3D objectdetection network for autonomous driving,” in CVPR, 2017.</span><br><span class="line"> [5] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “PointNet: Deep learningon point sets for 3D classification and segmentation,” in CVPR,2017.</span><br><span class="line"> [6] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao,“3D shapeNets: A deep representation for volumetric shapes,” inCVPR, 2015.</span><br><span class="line"> [7] M. A. Uy, Q.-H. Pham, B.-S. Hua, T. Nguyen, and S.-K. Yeung,“Revisiting point cloud classification: A new benchmark datasetand classification model on real-world data,” in ICCV, 2019.</span><br><span class="line"> [8] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang,Z. Li, S. Savarese, M. Savva, S. Song, and H. Su, “ShapeNet:An information-rich 3D model repository,” arXiv preprintarXiv:1512.03012, 2015.</span><br><span class="line"> [9] K. Mo, S. Zhu, A. X. Chang, L. Yi, S. Tripathi, L. J. Guibas, andH. Su, “PartNet: A large-scale benchmark for fine-grained andhierarchical part-level 3D object understanding,” in CVPR, 2019.</span><br><span class="line"> [10] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer,and S. Savarese, “3D semantic parsing of large-scale indoorspaces,” in CVPR, 2016.</span><br><span class="line"> [11] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, andM. Nießner, “ScanNet: Richly-annotated 3D reconstructions ofindoor scenes,” in CVPR, 2017.</span><br><span class="line"> [12] T. Hackel, N. Savinov, L. Ladicky, J. Wegner, K. Schindler, andM. Pollefeys, “Semantic3D.net: A new large-scale point cloudclassification benchmark,” ISPRS, 2017.</span><br><span class="line"> [13] X. Song, P. Wang, D. Zhou, R. Zhu, C. Guan, Y. Dai, H. Su,H. Li, and R. Yang, “Apollocar3D: A large 3D car instanceunderstanding benchmark for autonomous driving,” in CVPR,2019.</span><br><span class="line"> [14] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving,” in CVPR, 2012.</span><br><span class="line"> [15] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss, and J. Gall, “SemanticKITTI: A dataset for semantic sceneunderstanding of lidar sequences,” in ICCV, 2019.</span><br><span class="line"> [16] G. Elbaz, T. Avraham, and A. Fischer, “3D point cloud registration for localization using a deep neural network auto-encoder,”in CVPR, 2017, pp. 4631–4640.</span><br><span class="line"> [17] A. Zeng, K.-T. Yu, S. Song, D. Suo, E. Walker, A. Rodriguez, andJ. Xiao, “Multi-view self-supervised deep learning for 6D poseestimation in the amazon picking challenge,” in ICRA, 2017, pp.1386–1383.</span><br><span class="line"> [18] X. Han, H. Laga, and M. Bennamoun, “Image-based 3D objectreconstruction: State-of-the-art and trends in the deep learningera,” IEEE TPAMI, 2019.</span><br><span class="line"> [19] A. Ioannidou, E. Chatzilari, S. Nikolopoulos, and I. Kompatsiaris,“Deep learning advances in computer vision with 3D data: Asurvey,” ACM Computing Surveys, 2017.</span><br><span class="line"> [20] E. Ahmed, A. Saint, A. E. R. Shabayek, K. Cherenkova, R. Das,G. Gusev, D. Aouada, and B. Ottersten, “Deep learning advanceson different 3D data representations: A survey,” arXiv preprintarXiv:1808.01462, 2018.</span><br><span class="line"> [21] Y. Xie, J. Tian, and X. Zhu, “A review of point cloud semanticsegmentation,” IEEE GRSM, 2020.</span><br><span class="line"> [22] M. M. Rahman, Y. Tan, J. Xue, and K. Lu, “Recent advances in 3Dobject detection in the era of deep neural networks: A survey,”IEEE TIP, 2019.</span><br><span class="line"> [23] K. Siddiqi, J. Zhang, D. Macrini, A. Shokoufandeh, S. Bouix, andS. Dickinson, “Retrieving articulated 3-D models using medialsurfaces,” Machine Vision and Applications, vol. 19, no. 4, pp.261–275, 2008.</span><br><span class="line"> [24] M. De Deuge, B. Douillard, C. Hung, and A. Quadros, “Unsupervised feature learning for classification of outdoor 3D scans,” inACRA, 2013.</span><br><span class="line"> [25] S. Song, S. P. Lichtenberg, and J. Xiao, “Sun RGB-D: A RGB-Dscene understanding benchmark suite,” in CVPR, 2015.</span><br><span class="line"> [26] A. Patil, S. Malla, H. Gang, and Y.-T. Chen, “The H3D dataset forfull-surround 3D multi-object detection and tracking in crowdedurban scenes,” in ICRA, 2019.</span><br><span class="line"> [27] M.-F. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett,D. Wang, P. Carr, S. Lucey, D. Ramanan et al., “Argoverse: 3Dtracking and forecasting with rich maps,” in CVPR, 2019.</span><br><span class="line"> [28] R. Kesten, M. Usman, J. Houston, T. Pandya, K. Nadhamuni,A. Ferreira, M. Yuan, B. Low, A. Jain, P. Ondruska et al., “Lyftlevel 5 av dataset 2019,” 2019.</span><br><span class="line"> [29] Q.-H. Pham, P. Sevestre, R. S. Pahwa, H. Zhan, C. H. Pang,Y. Chen, A. Mustafa, V. Chandrasekhar, and J. Lin, “A*3D dataset:Towards autonomous driving in challenging environments,”ICRA, 2020.</span><br><span class="line"> [30] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik,P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han,J. Ngiam, H. Zhao, A. Timofeev, S. Ettinger, M. Krivokon, A. Gao,A. Joshi, Y. Zhang, J. Shlens, Z. Chen, and D. Anguelov, “Scalability in perception for autonomous driving: Waymo open dataset,”in CVPR, 2020.</span><br><span class="line"> [31] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu,A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: Amultimodal dataset for autonomous driving,” in CVPR, 2020.</span><br><span class="line"> [32] D. Munoz, J. A. Bagnell, N. Vandapel, and M. Hebert, “Contextual classification with functional max-margin markov networks,” in CVPR, 2009, pp. 975–982.</span><br><span class="line"> [33] F. Rottensteiner, G. Sohn, J. Jung, M. Gerke, C. Baillard, S. Benitez,and U. Breitkopf, “The isprs benchmark on urban object classification and 3D building reconstruction,” ISPRS, 2012.</span><br><span class="line"> [34] A. Serna, B. Marcotegui, F. Goulette, and J.-E. Deschaud, “Parisrue-madame database: a 3D mobile laser scanner dataset for benchmarking urban detection, segmentation and classification methods,” in ICRA, 2014.</span><br><span class="line"> [35] B. Vallet, M. Bredif, A. Serna, B. Marcotegui, and N. Paparoditis, “Terramobilita/iqmulus urban point cloud analysis benchmark,”Computers <span class="built_in">&amp;</span> Graphics, vol. 49, pp. 126–133, 2015.</span><br><span class="line"> [36] X. Roynard, J.-E. Deschaud, and F. Goulette, “Paris-lille-3d: Alarge and high-quality ground-truth urban point cloud datasetfor automatic segmentation and classification,” IJRR, 2018.</span><br><span class="line"> [37] W. Tan, N. Qin, L. Ma, Y. Li, J. Du, G. Cai, K. Yang, and J. Li,“Toronto-3D: A large-scale mobile lidar dataset for semantic segmentation of urban roadways,” arXiv preprint arXiv:2003.08284,2020.</span><br><span class="line"> [38] N. Varney, V. K. Asari, and Q. Graehling, “Dales: A large-scaleaerial lidar data set for semantic segmentation,” arXiv preprintarXiv:2004.11985, 2020.</span><br><span class="line"> [39] H. Lu, X. Chen, G. Zhang, Q. Zhou, Y. Ma, and Y. Zhao, “SCANet:Spatial-channel attention network for 3D object detection,” inICASSP, 2019.</span><br><span class="line"> [40] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller, “Multiview convolutional neural networks for 3D shape recognition,”in ICCV, 2015.</span><br><span class="line"> [41] T. Yu, J. Meng, and J. Yuan, “Multi-view harmonized bilinearnetwork for 3D object recognition,” in CVPR, 2018.</span><br><span class="line"> [42] Z. Yang and L. Wang, “Learning relationships for multi-view 3Dobject recognition,” in ICCV, 2019.</span><br><span class="line"> [43] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. J. Guibas,“Volumetric and multi-view CNNs for object classification on 3Ddata,” in CVPR, 2016.</span><br><span class="line"> [44] Y. Feng, Z. Zhang, X. Zhao, R. Ji, and Y. Gao, “GVCNN: Groupview convolutional neural networks for 3D shape recognition,”in CVPR, 2018.</span><br><span class="line"> [45] C. Wang, M. Pelillo, and K. Siddiqi, “Dominant set clustering andpooling for multi-view 3D object recognition,” BMVC, 2017.</span><br><span class="line"> [46] C. Ma, Y. Guo, J. Yang, and W. An, “Learning multi-view representation with LSTM for 3D shape recognition and retrieval,”IEEE TMM, 2018.</span><br><span class="line"> [47] X. Wei, R. Yu, and J. Sun, “View-gcn: View-based graph convolutional network for 3D shape analysis,” in CVPR, 2020.</span><br><span class="line"> [48] D. Maturana and S. Scherer, “VoxNet: A 3D convolutional neuralnetwork for real-time object recognition,” in IROS, 2015.</span><br><span class="line"> [49] G. Riegler, A. Osman Ulusoy, and A. Geiger, “OctNet: Learningdeep 3D representations at high resolutions,” in CVPR, 2017.</span><br><span class="line"> [50] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong, “OCNN: Octree-based convolutional neural networks for 3D shapeanalysis,” ACM TOG, 2017.</span><br><span class="line"> [51] T. Le and Y. Duan, “PointGrid: A deep network for 3D shapeunderstanding,” in CVPR, 2018.</span><br><span class="line"> [52] Y. Ben-Shabat, M. Lindenbaum, and A. Fischer, “3D point cloudclassification and segmentation using 3D modified fisher vector representation for convolutional neural networks,” arXivpreprint arXiv:1711.08241, 2017.</span><br><span class="line"> [53] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J. Smola, “Deep sets,” in NeurIPS, 2017.</span><br><span class="line"> [54] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “PointNet++: Deephierarchical feature learning on point sets in a metric space,” inNeurIPS, 2017.</span><br><span class="line"> [55] M. Joseph-Rivlin, A. Zvirin, and R. Kimmel, “Mo-Net: Flavor themoments in learning to classify shapes,” in ICCVW, 2018.</span><br><span class="line"> [56] J. Yang, Q. Zhang, B. Ni, L. Li, J. Liu, M. Zhou, and Q. Tian,“Modeling point clouds with self-attention and gumbel subsetsampling,” in CVPR, 2019.</span><br><span class="line"> [57] H. Zhao, L. Jiang, C.-W. Fu, and J. Jia, “PointWeb: Enhancing local neighborhood features for point cloud processing,” in CVPR,2019.</span><br><span class="line"> [58] Y. Duan, Y. Zheng, J. Lu, J. Zhou, and Q. Tian, “Structuralrelational reasoning of point clouds,” in CVPR, 2019.</span><br><span class="line"> [59] H. Lin, Z. Xiao, Y. Tan, H. Chao, and S. Ding, “Justlookup: Onemillisecond deep feature extraction for point clouds by lookuptables,” in ICME, 2019.</span><br><span class="line"> [60] X. Sun, Z. Lian, and J. Xiao, “SRINet: Learning strictly rotationinvariant representations for point cloud classification and segmentation,” in ACM MM, 2019.</span><br><span class="line"> [61] X. Yan, C. Zheng, Z. Li, S. Wang, and S. Cui, “Pointasnl: Robustpoint clouds processing using nonlocal neural networks withadaptive sampling,” in CVPR, 2020.</span><br><span class="line"> [62] Y. Liu, B. Fan, S. Xiang, and C. Pan, “Relation-shape convolutional neural network for point cloud analysis,” in CVPR, 2019.</span><br><span class="line"> [63] A. Boulch, “Generalizing discrete convolutions for unstructuredpoint clouds,” arXiv preprint arXiv:1904.02375, 2019.</span><br><span class="line"> [64] Y. Liu, B. Fan, G. Meng, J. Lu, S. Xiang, and C. Pan, “DensePoint:Learning densely contextual representation for efficient pointcloud processing,” in ICCV, 2019.</span><br><span class="line"> [65] H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette,and L. J. Guibas, “KPConv: Flexible and deformable convolutionfor point clouds,” in ICCV, 2019.</span><br><span class="line"> [66] A. Boulch, “ConvPoint: continuous convolutions for point cloudprocessing,” Computers <span class="built_in">&amp;</span> Graphics, 2020.</span><br><span class="line"> [67] W. Wu, Z. Qi, and L. Fuxin, “PointConv: Deep convolutionalnetworks on 3D point clouds,” in CVPR, 2019.</span><br><span class="line"> [68] P. Hermosilla, T. Ritschel, P.-P. Vazquez,  A. Vinacua, and T. Ropinski, “Monte carlo convolution for learning on   nonuniformly sampled point clouds,” ACM TOG, 2018.</span><br><span class="line"> [69] Y. Xu, T. Fan, M. Xu, L. Zeng, and Y. Qiao, “SpiderCNN: Deeplearning on point sets with parameterized convolutional filters,”in ECCV, 2018.</span><br><span class="line"> [70] A. Matan, M. Haggai, and L. Yaron, “Point convolutional neuralnetworks by extension operators,” ACM TOG, 2018.</span><br><span class="line"> [71] C. Esteves, C. Allen-Blanchette, A. Makadia, and K. Daniilidis, “Learning so(3) equivariant representations with sphericalCNNs,” in ECCV, 2017.</span><br><span class="line"> [72] N. Thomas, T. Smidt, S. Kearnes, L. Yang, L. Li, K. Kohlhoff,and P. Riley, “Tensor field networks: Rotation-and translationequivariant neural networks for 3D point clouds,” arXiv preprintarXiv:1802.08219, 2018.</span><br><span class="line"> [73] T. S. Cohen, M. Geiger, J. Koehler, and M. Welling, “SphericalCNNs,” ICLR, 2018.</span><br><span class="line"> [74] A. Poulenard, M.-J. Rakotosaona, Y. Ponty, and M. Ovsjanikov,“Effective rotation-invariant point CNN with spherical harmonics kernels,” in 3DV, 2019.</span><br><span class="line"> [75] F. Groh, P. Wieschollek, and H. P. Lensch, “Flex-Convolution,” inACCV, 2018.</span><br><span class="line"> [76] B.-S. Hua, M.-K. Tran, and S.-K. Yeung, “Pointwise convolutionalneural networks,” in CVPR, 2018.</span><br><span class="line"> [77] H. Lei, N. Akhtar, and A. Mian, “Octree guided cnn with spherical kernels for 3D point clouds,” in CVPR, 2019.</span><br><span class="line"> [78] S. Lan, R. Yu, G. Yu, and L. S. Davis, “Modeling local geometricstructure of 3D point clouds using geo-cnn,” in CVPR, 2019.</span><br><span class="line"> [79] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen, “PointCNN:Convolution on x-transformed points,” in NeurIPS, 2018.</span><br><span class="line"> [80] J. Mao, X. Wang, and H. Li, “Interpolated convolutional networksfor 3D point cloud understanding,” in ICCV, 2019.</span><br><span class="line"> [81] Z. Zhang, B.-S. Hua, D. W. Rosen, and S.-K. Yeung, “Rotationinvariant convolutions for 3D point clouds deep learning,” in3DV, 2019.</span><br><span class="line"> [82] A. Komarichev, Z. Zhong, and J. Hua, “A-CNN: Annularlyconvolutional neural networks on point clouds,” in CVPR, 2019.</span><br><span class="line"> [83] S. Kumawat and S. Raman, “LP-3DCNN: Unveiling local phasein 3D convolutional neural networks,” in CVPR, 2019.</span><br><span class="line"> [84] Y. Rao, J. Lu, and J. Zhou, “Spherical fractal convolutional neuralnetworks for point cloud recognition,” in CVPR, 2019.</span><br><span class="line"> [85] M. Simonovsky and N. Komodakis, “Dynamic edge-conditionedfilters in convolutional neural networks on graphs,” in CVPR,2017.</span><br><span class="line"> [86] R. B. Rusu and S. Cousins, “3D is here: Point cloud library (PCL),”in ICRA, 2011.</span><br><span class="line"> [87] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M.Solomon, “Dynamic graph CNN for learning on point clouds,”ACM TOG, 2019.</span><br><span class="line"> [88] K. Zhang, M. Hao, J. Wang, C. W. de Silva, and C. Fu, “Linkeddynamic graph CNN: Learning on point cloud via linking hierarchical features,” arXiv preprint arXiv:1904.10014, 2019.</span><br><span class="line"> [89] Y. Yang, C. Feng, Y. Shen, and D. Tian, “FoldingNet: Point cloudauto-encoder via deep grid deformation,” in CVPR, 2018.</span><br><span class="line"> [90] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper withconvolutions,” in CVPR, 2015.</span><br><span class="line"> [91] K. Hassani and M. Haley, “Unsupervised multi-task featurelearning on point clouds,” in ICCV, 2019.</span><br><span class="line"> [92] J. Liu, B. Ni, C. Li, J. Yang, and Q. Tian, “Dynamic pointsagglomeration for hierarchical point sets learning,” in ICCV,2019.</span><br><span class="line"> [93] Y. Shen, C. Feng, Y. Yang, and D. Tian, “Mining point cloud localstructures by kernel correlation and graph pooling,” in CVPR,2018.</span><br><span class="line"> [94] M. Dominguez, R. Dhamdhere, A. Petkar, S. Jain, S. Sah, andR. Ptucha, “General-purpose deep point cloud feature extractor,”in WACV, 2018.</span><br><span class="line"> [95] C. Chen, G. Li, R. Xu, T. Chen, M. Wang, and L. Lin, “ClusterNet: Deep hierarchical cluster network with rigorously rotationinvariant representation for point cloud analysis,” in CVPR, 2019.</span><br><span class="line"> [96] D. Mullner, “Modern hierarchical, agglomerative clustering algo- ¨rithms,” arXiv preprint arXiv:1109.2378, 2011.</span><br><span class="line"> [97] Q. Xu, X. Sun, C.-Y. Wu, P. Wang, and U. Neumann, “Grid-gcnfor fast and scalable point cloud learning,” in CVPR, 2020.</span><br><span class="line"> [98] J. Bruna, W. Zaremba, A. Szlam, and Y. Lecun, “Spectral networksand locally connected networks on graphs,” ICLR, 2014.</span><br><span class="line"> [99] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutionalneural networks on graphs with fast localized spectral filtering,”in NeurIPS, 2016.</span><br><span class="line"> [100] G. Te, W. Hu, A. Zheng, and Z. Guo, “RGCNN: Regularizedgraph CNN for point cloud segmentation,” in ACM MM, 2018.</span><br><span class="line"> [101] R. Li, S. Wang, F. Zhu, and J. Huang, “Adaptive graph convolutional neural networks,” in AAAI, 2018.</span><br><span class="line"> [102] Y. Feng, H. You, Z. Zhang, R. Ji, and Y. Gao, “Hypergraph neuralnetworks,” in AAAI, 2019.</span><br><span class="line"> [103] C. Wang, B. Samari, and K. Siddiqi, “Local spectral graph convolution for point set feature learning,” in ECCV, 2018.</span><br><span class="line"> [104] Y. Zhang and M. Rabbat, “A Graph-CNN for 3D point cloudclassification,” in ICASSP, 2018.</span><br><span class="line"> [105] G. Pan, J. Wang, R. Ying, and P. Liu, “3DTI-Net: Learn innertransform invariant 3D geometry features using dynamic GCN,”arXiv preprint arXiv:1812.06254, 2018.</span><br><span class="line"> [106] R. Klokov and V. Lempitsky, “Escape from cells: Deep kdnetworks for the recognition of 3D point cloud models,” in ICCV,2017.</span><br><span class="line"> [107] W. Zeng and T. Gevers, “3DContextNet: K-d tree guided hierarchical learning of point clouds using local and global contextualcues,” in ECCV, 2018.</span><br><span class="line"> [108] J. Li, B. M. Chen, and G. Hee Lee, “SO-Net: Self-organizingnetwork for point cloud analysis,” in CVPR, 2018.</span><br><span class="line"> [109] S. Xie, S. Liu, Z. Chen, and Z. Tu, “Attentional ShapeContextNetfor point cloud recognition,” in CVPR, 2018.</span><br><span class="line"> [110] H. You, Y. Feng, R. Ji, and Y. Gao, “PVNet: A joint convolutionalnetwork of point cloud and multi-view for 3D shape recognition,” in ACM MM, 2018.</span><br><span class="line"> [111] H. You, Y. Feng, X. Zhao, C. Zou, R. Ji, and Y. Gao, “PVRNet:Point-view relation neural network for 3D shape recognition,” inAAAI, 2019.</span><br><span class="line"> [112] Y. Zhao, T. Birdal, H. Deng, and F. Tombari, “3D point capsulenetworks,” in CVPR, 2019.</span><br><span class="line"> [113] W. Chen, X. Han, G. Li, C. Chen, J. Xing, Y. Zhao, and H. Li,“Deep RBFNet: Point cloud feature learning using radial basisfunctions,” arXiv preprint arXiv:1812.04302, 2018.</span><br><span class="line"> [114] X. Liu, Z. Han, Y.-S. Liu, and M. Zwicker, “Point2Sequence:Learning the shape representation of 3D point clouds with anattention-based sequence to sequence network,” in AAAI, 2019.</span><br><span class="line"> [115] P. Wu, C. Chen, J. Yi, and D. Metaxas, “Point cloud processingvia recurrent set encoding,” in AAAI, 2019.</span><br><span class="line"> [116] C. Qin, H. You, L. Wang, C.-C. J. Kuo, and Y. Fu, “PointDAN:A multi-scale 3D domain adaption network for point cloudrepresentation,” in NIPS, 2019.</span><br><span class="line"> [117] B. Sievers and J. Sauder, “Self-supervised deep learning on pointclouds by reconstructing space,” in NIPS, 2019.</span><br><span class="line"> [118] R. Li, X. Li, P.-A. Heng, and C.-W. Fu, “PointAugment: Anauto-augmentation framework for point cloud classification,” inCVPR, 2020.</span><br><span class="line"> [119] S. Belongie, J. Malik, and J. Puzicha, “Shape matching and objectrecognition using shape contexts,” IEEE TPAMI, 2002.</span><br><span class="line"> [120] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”in NeurIPS, 2017.</span><br><span class="line"> [121] D. Bobkov, S. Chen, R. Jian, Z. Iqbal, and E. Steinbach, “Noiseresistant deep learning for object classification in 3D point cloudsusing a point pair descriptor,” IEEE RAL, 2018.</span><br><span class="line"> [122] S. Prokudin, C. Lassner, and J. Romero, “Efficient learning onpoint clouds with basis point sets,” in ICCV, 2019.</span><br><span class="line"> [123] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, andM. Pietikainen, “Deep learning for generic object detection: A ¨survey,” IJCV, 2020.</span><br><span class="line"> [124] C. R. Qi, O. Litany, K. He, and L. J. Guibas, “Deep hough votingfor 3D object detection in point clouds,” ICCV, 2019.</span><br><span class="line"> [125] W. Shi and R. Rajkumar, “Point-GNN: Graph neural network for3D object detection in a point cloud,” in CVPR, 2020.</span><br><span class="line"> [126] J. Ku, M. Mozifian, J. Lee, A. Harakeh, and S. L. Waslander,“Joint 3D proposal generation and object detection from viewaggregation,” in IROS, 2018.</span><br><span class="line"> [127] M. Liang, B. Yang, S. Wang, and R. Urtasun, “Deep continuousfusion for multi-sensor 3D object detection,” in ECCV, 2018.</span><br><span class="line"> [128] M. Liang, B. Yang, Y. Chen, R. Hu, and R. Urtasun, “Multi-taskmulti-sensor fusion for 3D object detection,” in CVPR, 2019.</span><br><span class="line"> [129] B. Yang, W. Luo, and R. Urtasun, “PIXOR: Real-time 3D objectdetection from point clouds,” in CVPR, 2018.</span><br><span class="line"> [130] W. Luo, B. Yang, and R. Urtasun, “Fast and furious: Real timeend-to-end 3D detection, tracking and motion forecasting with asingle convolutional net,” in CVPR, 2018.</span><br><span class="line"> [131] Y. Zeng, Y. Hu, S. Liu, J. Ye, Y. Han, X. Li, and N. Sun,“RT3D: Real-time 3D vehicle detection in lidar point cloud forautonomous driving,” IEEE RAL, 2018.</span><br><span class="line"> [132] Z. Yang, Y. Sun, S. Liu, X. Shen, and J. Jia, “IPOD: Intensivepoint-based object detector for point cloud,” arXiv preprintarXiv:1812.05276, 2018.</span><br><span class="line"> [133] S. Shi, X. Wang, and H. Li, “PointRCNN: 3D object proposalgeneration and detection from point cloud,” in CVPR, 2019.</span><br><span class="line"> [134] Z. Jesus, G. Silvio, and G. Bernard, “PointRGCN: Graph convolution networks for 3D vehicles detection refinement,” arXivpreprint arXiv:1911.12236, 2019.</span><br><span class="line"> [135] V. Sourabh, L. Alex H., H. Bassam, and B. Oscar, “PointPainting:Sequential fusion for 3D object detection,” in CVPR, 2020.</span><br><span class="line"> [136] Y. Zhou and O. Tuzel, “VoxelNet: End-to-end learning for pointcloud based 3D object detection,” in CVPR, 2018.</span><br><span class="line"> [137] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom, “PointPillars: Fast encoders for object detection from pointclouds,” in CVPR, 2019.</span><br><span class="line"> [138] Z. Yang, Y. Sun, S. Liu, X. Shen, and J. Jia, “STD: Sparse-to-dense3D object detector for point cloud,” in ICCV, 2019.</span><br><span class="line"> [139] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “FrustumPointNets for 3D object detection from RGB-D data,” in CVPR,2018.</span><br><span class="line"> [140] X. Zhao, Z. Liu, R. Hu, and K. Huang, “3D object detection usingscale invariant and feature reweighting networks,” in AAAI,2019.</span><br><span class="line"> [141] M. Jiang, Y. Wu, and C. Lu, “PointSIFT: A sift-like network module for 3D point cloud semantic segmentation,” arXiv preprintarXiv:1807.00652, 2018.</span><br><span class="line"> [142] D. Xu, D. Anguelov, and A. Jain, “PointFusion: Deep sensorfusion for 3D bounding box estimation,” in CVPR, 2018.</span><br><span class="line"> [143] K. Shin, Y. P. Kwon, and M. Tomizuka, “RoarNet: A robust 3Dobject detection based on region approximation refinement,” inIEEE IV, 2019.</span><br><span class="line"> [144] Z. Wang and K. Jia, “Frustum convNet: Sliding frustums to aggregate local point-wise features for amodal 3D object detection,”in IROS, 2019.</span><br><span class="line"> [145] L. Johannes, M. Andreas, A. Thomas, H. Markus, N. Bernhard,and H. Sepp, “Patch refinement - localized 3D object detection,”arXiv preprint arXiv:1910.04093, 2019.</span><br><span class="line"> [146] D. Zhou, J. Fang, X. Song, C. Guan, J. Yin, Y. Dai, and R. Yang,“Iou loss for 2D/3D object detection,” in 3DV, 2019.</span><br><span class="line"> [147] Y. Chen, S. Liu, X. Shen, and J. Jia, “Fast point r-cnn,” in ICCV,2019.</span><br><span class="line"> [148] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li,“PV-RCNN: Point-voxel feature set abstraction for 3D objectdetection,” in CVPR, 2020.</span><br><span class="line"> [149] M. Feng, S. Z. Gilani, Y. Wang, L. Zhang, and A. Mian, “Relationgraph network for 3D object detection in point clouds,” arXivpreprint arXiv:1912.00202, 2019.</span><br><span class="line"> [150] C. R. Qi, X. Chen, O. Litany, and L. J. Guibas, “ImVoteNet:Boosting 3D object detection in point clouds with image votes,”in CVPR, 2020.</span><br><span class="line"> [151] S. Shi, Z. Wang, X. Wang, and H. Li, “From points to parts:3D object detection from point cloud with part-aware and partaggregation network,” TPAMI, 2020.</span><br><span class="line"> [152] B. Yang, M. Liang, and R. Urtasun, “HDNET: Exploiting hd mapsfor 3D object detection,” in CoRL, 2018.</span><br><span class="line"> [153] J. Beltran, C. Guindel, F. M. Moreno, D. Cruzado, F. Garc  ıa, andA. De La Escalera, “BirdNet: a 3D object detection frameworkfrom lidar information,” in ITSC, 2018.</span><br><span class="line"> [154] B. Li, T. Zhang, and T. Xia, “Vehicle detection from 3D lidar usingfully convolutional network,” arXiv preprint arXiv:1608.07916,2016.</span><br><span class="line"> [155] B. Li, “3D fully convolutional network for vehicle detection inpoint cloud,” in IROS, 2017.</span><br><span class="line"> [156] M. Engelcke, D. Rao, D. Z. Wang, C. H. Tong, and I. Posner,“Vote3Deep: Fast object detection in 3D point clouds using efficient convolutional neural networks,” in ICRA, 2017.</span><br><span class="line"> [157] X. Li, J. E. Guivant, N. Kwok, and Y. Xu, “3D backbone networkfor 3D object detection,” in CoRR, 2019.</span><br><span class="line"> [158] Y. Yan, Y. Mao, and B. Li, “SECOND: Sparsely embedded convolutional detection,” Sensors, 2018.</span><br><span class="line"> [159] V. A. Sindagi, Y. Zhou, and O. Tuzel, “MVX-Net: Multimodalvoxelnet for 3D object detection,” in ICRA, 2019.</span><br><span class="line"> [160] C. He, H. Zeng, J. Huang, X.-S. Hua, and L. Zhang, “Structureaware single-stage 3D object detection from point cloud,” inCVPR, 2020.</span><br><span class="line"> [161] Z. Yang, Y. Sun, S. Liu, and J. Jia, “3DSSD: Point-based 3D singlestage object detector,” in CVPR, 2020.</span><br><span class="line"> [162] G. P. Meyer, A. Laddha, E. Kee, C. Vallespi-Gonzalez, and C. K.Wellington, “LaserNet: An efficient probabilistic 3D object detector for autonomous driving,” CVPR, 2019.</span><br><span class="line"> [163] G. P. Meyer, J. Charland, D. Hegde, A. Laddha, and C. VallespiGonzalez, “Sensor fusion for joint 3D object detection and semantic segmentation,” CVPRW, 2019.</span><br><span class="line"> [164] Q. Chen, L. Sun, Z. Wang, K. Jia, and A. Yuille, “Object ashotspots: An anchor-free 3D object detection approach via firingof hotspots,” arXiv preprint arXiv:1912.12791, 2019.</span><br><span class="line"> [165] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutionalnetworks for biomedical image segmentation,” in MICCAI, 2015,pp. 234–241.</span><br><span class="line"> [166] B. Graham, M. Engelcke, and L. van der Maaten, “3D semanticsegmentation with submanifold sparse convolutional networks,”in CVPR, 2018.</span><br><span class="line"> [167] Q. Hu, Y. Guo, Y. Chen, J. Xiao, and W. An, “Correlation filtertracking: Beyond an open-loop system,” in BMVC, 2017.</span><br><span class="line"> [168] H. Liu, Q. Hu, B. Li, and Y. Guo, “Robust long-term tracking viainstance specific proposals,” IEEE TIM, 2019.</span><br><span class="line"> [169] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H.Torr, “Fully-convolutional siamese networks for object tracking,”in ECCV, 2016.</span><br><span class="line"> [170] S. Giancola, J. Zarzar, and B. Ghanem, “Leveraging shape completion for 3D siamese tracking,” CVPR, 2019.</span><br><span class="line"> [171] M. Mueller, N. Smith, and B. Ghanem, “Context-aware correlation filter tracking,” in CVPR, 2017.</span><br><span class="line"> [172] J. Zarzar, S. Giancola, and B. Ghanem, “Efficient tracking proposals using 2D-3D siamese networks on lidar,” arXiv preprintarXiv:1903.10168, 2019.</span><br><span class="line"> [173] M. Simon, K. Amende, A. Kraus, J. Honer, T. Samann, H. Kaulber- ¨sch, S. Milz, and H. M. Gross, “Complexer-YOLO: Real-time 3Dobject detection and tracking on semantic point clouds,” CVPRW,2019.</span><br><span class="line"> [174] H. Qi, C. Feng, Z. Cao, F. Zhao, and Y. Xiao, “P2B: Point-to-boxnetwork for 3D object tracking in point clouds,” in CVPR, 2020.</span><br><span class="line"> [175] X. Liu, C. R. Qi, and L. J. Guibas, “FlowNet3D: Learning sceneflow in 3D point clouds,” in CVPR, 2019.</span><br><span class="line"> [176] Z. Wang, S. Li, H. Howard-Jenkins, V. Prisacariu, and M. Chen,“FlowNet3D++: Geometric losses for deep scene flow estimation,” in WACV, 2020.</span><br><span class="line"> [177] X. Gu, Y. Wang, C. Wu, Y. J. Lee, and P. Wang, “HPLFlowNet:Hierarchical permutohedral lattice flowNet for scene flow estimation on large-scale point clouds,” in CVPR, 2019.</span><br><span class="line"> [178] H. Fan and Y. Yang, “PointRNN: Point recurrent neural network for moving point cloud processing,” arXiv preprintarXiv:1910.08287, 2019.</span><br><span class="line"> [179] X. Liu, M. Yan, and J. Bohg, “MeteorNet: Deep learning ondynamic 3D point cloud sequences,” in ICCV, 2019.</span><br><span class="line"> [180] H. Mittal, B. Okorn, and D. Held, “Just go with the flow: Selfsupervised scene flow estimation,” in CVPR, 2020.</span><br><span class="line"> [181] F. J. Lawin, M. Danelljan, P. Tosteberg, G. Bhat, F. S. Khan, andM. Felsberg, “Deep projective 3D semantic segmentation,” inCAIP, 2017.</span><br><span class="line"> [182] A. Boulch, B. Le Saux, and N. Audebert, “Unstructured pointcloud semantic labeling using deep segmentation networks.” in3DOR, 2017.</span><br><span class="line"> [183] B. Wu, A. Wan, X. Yue, and K. Keutzer, “SqueezeSeg: Convolutional neural nets with recurrent crf for real-time road-objectsegmentation from 3D lidar point cloud,” in ICRA, 2018.</span><br><span class="line"> [184] B. Wu, X. Zhou, S. Zhao, X. Yue, and K. Keutzer, “SqueezeSegV2:Improved model structure and unsupervised domain adaptationfor road-object segmentation from a lidar point cloud,” in ICRA,2019.</span><br><span class="line"> [185] A. Milioto, I. Vizzo, J. Behley, and C. Stachniss, “RangeNet++:Fast and accurate lidar semantic segmentation,” in IROS, 2019.</span><br><span class="line"> [186] H.-Y. Meng, L. Gao, Y.-K. Lai, and D. Manocha, “VV-Net: Voxelvae net with group convolutions for point cloud segmentation,”in ICCV, 2019.</span><br><span class="line"> [187] D. Rethage, J. Wald, J. Sturm, N. Navab, and F. Tombari, “Fullyconvolutional point networks for large-scale point clouds,” inECCV, 2018.</span><br><span class="line"> [188] H. Su, V. Jampani, D. Sun, S. Maji, E. Kalogerakis, M.-H. Yang,and J. Kautz, “SplatNet: Sparse lattice networks for point cloudprocessing,” in CVPR, 2018.</span><br><span class="line"> [189] R. A. Rosu, P. Schutt, J. Quenzel, and S. Behnke, “LatticeNet: Fast ¨point cloud segmentation using permutohedral lattices,” arXivpreprint arXiv:1912.05905, 2019.</span><br><span class="line"> [190] A. Dai and M. Nießner, “3DMV: Joint 3D-multi-view predictionfor 3D semantic scene segmentation,” in ECCV, 2018.</span><br><span class="line"> [191] M. Jaritz, J. Gu, and H. Su, “Multi-view pointNet for 3D sceneunderstanding,” in ICCVW, 2019.</span><br><span class="line"> [192] N. Audebert, B. Le Saux, and S. Lefevre, “Semantic segmentationof earth observation data using multimodal and multi-scale deepnetworks,” in ACCV, 2016.</span><br><span class="line"> [193] M. Tatarchenko, J. Park, V. Koltun, and Q.-Y. Zhou, “Tangentconvolutions for dense prediction in 3D,” in CVPR, 2018.</span><br><span class="line"> [194] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally,and K. Keutzer, “SqueezeNet: Alexnet-level accuracy with 50xfewer parameters and &lt; 0.5 MB model size,” in ICLR, 2016.</span><br><span class="line"> [195] J. Huang and S. You, “Point cloud labeling using 3D convolutional neural network,” in ICPR, 2016.</span><br><span class="line"> [196] L. Tchapmi, C. Choy, I. Armeni, J. Gwak, and S. Savarese, “SEGCloud: Semantic segmentation of 3D point clouds,” in 3DV, 2017.</span><br><span class="line"> [197] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmentation,” in CVPR, 2015.</span><br><span class="line"> [198] A. Dai, D. Ritchie, M. Bokeloh, S. Reed, J. Sturm, and M. Nießner,“ScanComplete: Large-scale scene completion and semantic segmentation for 3D scans,” in CVPR, 2018.</span><br><span class="line"> [199] C. Choy, J. Gwak, and S. Savarese, “4D spatio-temporal convnets:Minkowski convolutional neural networks,” in CVPR, 2019.</span><br><span class="line"> [200] H.-Y. Chiang, Y.-L. Lin, Y.-C. Liu, and W. H. Hsu, “A unifiedpoint-based framework for 3D segmentation,” in 3DV, 2019.</span><br><span class="line"> [201] S. Wang, S. Suo, W.-C. Ma, A. Pokrovsky, and R. Urtasun, “Deepparametric continuous convolutional neural networks,” in CVPR,2018.</span><br><span class="line"> [202] X. Ye, J. Li, H. Huang, L. Du, and X. Zhang, “3D recurrentneural networks with context fusion for point cloud semanticsegmentation,” in ECCV, 2018.</span><br><span class="line"> [203] L. Landrieu and M. Simonovsky, “Large-scale point cloud semantic segmentation with superpoint graphs,” in CVPR, 2018.</span><br><span class="line"> [204] F. Engelmann, T. Kontogianni, J. Schult, and B. Leibe, “Knowwhat your neighbors do: 3D semantic segmentation of pointclouds,” in ECCVW, 2018.</span><br><span class="line"> [205] Z. Zhang, B.-S. Hua, and S.-K. Yeung, “ShellNet: Efficient pointcloud convolutional neural networks using concentric shellsstatistics,” in ICCV, 2019.</span><br><span class="line"> [206] Q. Hu, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, andA. Markham, “RandLA-Net: Efficient semantic segmentation oflarge-scale point clouds,” CVPR, 2020.</span><br><span class="line"> [207] L.-Z. Chen, X.-Y. Li, D.-P. Fan, M.-M. Cheng, K. Wang, and S.-P. Lu, “LSANet: Feature learning on point sets by local spatialattention,” arXiv preprint arXiv:1905.05442, 2019.</span><br><span class="line"> [208] C. Zhao, W. Zhou, L. Lu, and Q. Zhao, “Pooling scores ofneighboring points for improved 3D point cloud segmentation,”in ICIP, 2019.</span><br><span class="line"> [209] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, and J. Sivic,“NetVLAD: CNN architecture for weakly supervised placerecognition,” in CVPR, 2016.</span><br><span class="line"> [210] F. Engelmann, T. Kontogianni, J. Schult, and B. Leibe, “Knowwhat your neighbors do: 3D semantic segmentation of pointclouds,” in ECCV, 2018.</span><br><span class="line"> [211] F. Engelmann, T. Kontogianni, and B. Leibe, “Dilated point convolutions: On the receptive field of point convolutions,” in ICRA,2020.</span><br><span class="line"> [212] Q. Huang, W. Wang, and U. Neumann, “Recurrent slice networksfor 3D segmentation of point clouds,” in CVPR, 2018.</span><br><span class="line"> [213] F. Engelmann, T. Kontogianni, A. Hermans, and B. Leibe, “Exploring spatial context for 3D semantic segmentation of pointclouds,” in ICCV, 2017.</span><br><span class="line"> [214] L. Landrieu and M. Boussaha, “Point cloud oversegmentationwith graph-structured deep metric learning,” in CVPR, 2019.</span><br><span class="line"> [215] L. Wang, Y. Huang, Y. Hou, S. Zhang, and J. Shan, “Graphattention convolution for point cloud semantic segmentation,”in CVPR, 2019.</span><br><span class="line"> [216] L. Pan, C.-M. Chew, and G. H. Lee, “Pointatrousgraph: Deephierarchical encoder-decoder with atrous convolution for pointclouds,” arXiv preprint arXiv:1907.09798, 2019.</span><br><span class="line"> [217] Z. Liang, M. Yang, L. Deng, C. Wang, and B. Wang, “Hierarchicaldepthwise graph convolutional neural network for 3D semanticsegmentation of point clouds,” in ICRA, 2019.</span><br><span class="line"> [218] L. Jiang, H. Zhao, S. Liu, X. Shen, C.-W. Fu, and J. Jia, “Hierarchical point-edge interaction network for point cloud semanticsegmentation,” in ICCV, 2019.</span><br><span class="line"> [219] H. Lei, N. Akhtar, and A. Mian, “Spherical convolutional neuralnetwork for 3D point clouds,” arXiv preprint arXiv:1805.07872,2018.</span><br><span class="line"> [220] Z. Zhao, M. Liu, and K. Ramani, “DAR-Net: Dynamic aggregation network for semantic scene segmentation,” arXiv preprintarXiv:1907.12022, 2019.</span><br><span class="line"> [221] F. Liu, S. Li, L. Zhang, C. Zhou, R. Ye, Y. Wang, and J. Lu,“3DCNN-DQN-RNN: A deep reinforcement learning frameworkfor semantic parsing of large-scale 3D point clouds,” in ICCV,2017.</span><br><span class="line"> [222] Z. Kang and N. Li, “PyramNet: Point cloud pyramid attentionnetwork and graph embedding module for classification andsegmentation,” in ICONIP, 2019.</span><br><span class="line"> [223] Y. Ma, Y. Guo, H. Liu, Y. Lei, and G. Wen, “Global contextreasoning for semantic segmentation of 3D point clouds,” inWACV, 2020.</span><br><span class="line"> [224] J. Wei, G. Lin, K.-H. Yap, T.-Y. Hung, and L. Xie, “Multi-pathregion mining for weakly supervised 3D semantic segmentationon point clouds,” in CVPR, 2020.</span><br><span class="line"> [225] X. Xu and G. H. Lee, “Weakly supervised semantic point cloudsegmentation: Towards 10x fewer labels,” in CVPR, 2020, pp.13 706–13 715.</span><br><span class="line"> [226] J. Hou, A. Dai, and M. Nießner, “3D-SIS: 3D semantic instancesegmentation of RGB-D scans,” in CVPR, 2019.</span><br><span class="line"> [227] L. Yi, W. Zhao, H. Wang, M. Sung, and L. J. Guibas, “GSPN:Generative shape proposal network for 3D instance segmentationin point cloud,” in CVPR, 2019.</span><br><span class="line"> [228] G. Narita, T. Seno, T. Ishikawa, and Y. Kaji, “PanopticFusion:Online volumetric semantic mapping at the level of stuff andthings,” in IROS, 2019.</span><br><span class="line"> [229] B. Yang, J. Wang, R. Clark, Q. Hu, S. Wang, A. Markham, andN. Trigoni, “Learning object bounding boxes for 3D instancesegmentation on point clouds,” in NeurIPS, 2019.</span><br><span class="line"> [230] F. Zhang, C. Guan, J. Fang, S. Bai, R. Yang, P. Torr, andV. Prisacariu, “Instance segmentation of lidar point clouds,” inICRA, 2020.</span><br><span class="line"> [231] Y. Shi, A. X. Chang, Z. Wu, M. Savva, and K. Xu, “Hierarchy denoising recursive autoencoders for 3D scene layout prediction,”in CVPR, 2019.</span><br><span class="line"> [232] F. Engelmann, M. Bokeloh, A. Fathi, B. Leibe, and M. Nießner,“3d-mpa: Multi-proposal aggregation for 3d semantic instancesegmentation,” in CVPR, 2020.</span><br><span class="line"> [233] W. Wang, R. Yu, Q. Huang, and U. Neumann, “SGPN: Similaritygroup proposal network for 3D point cloud instance segmentation,” in CVPR, 2018.</span><br><span class="line"> [234] X. Wang, S. Liu, X. Shen, C. Shen, and J. Jia, “Associativelysegmenting instances and semantics in point clouds,” in CVPR,2019.</span><br><span class="line"> [235] Q.-H. Pham, T. Nguyen, B.-S. Hua, G. Roig, and S.-K. Yeung,“JSIS3D: Joint semantic-instance segmentation of 3D point cloudswith multi-task pointwise networks and multi-value conditionalrandom fields,” in CVPR, 2019.</span><br><span class="line"> [236] C. Elich, F. Engelmann, J. Schult, T. Kontogianni, and B. Leibe,“3D-BEVIS: Birds-eye-view instance segmentation,” in GCPR,2019.</span><br><span class="line"> [237] C. Liu and Y. Furukawa, “MASC: Multi-scale affinity withsparse convolution for 3D instance segmentation,” arXiv preprintarXiv:1902.04478, 2019.</span><br><span class="line"> [238] Z. Liang, M. Yang, and C. Wang, “3D graph embedding learningwith a structure-aware loss function for point cloud semanticinstance segmentation,” arXiv preprint arXiv:1902.05247, 2019.</span><br><span class="line"> [239] L. Han, T. Zheng, L. Xu, and L. Fang, “Occuseg: Occupancyaware 3d instance segmentation,” in CVPR, 2020.</span><br><span class="line"> [240] L. Jiang, H. Zhao, S. Shi, S. Liu, C.-W. Fu, and J. Jia, “PointGroup:Dual-set point grouping for 3D instance segmentation,” in CVPR,2020.</span><br><span class="line"> [241] K. Mo, S. Zhu, A. X. Chang, L. Yi, S. Tripathi, L. J. Guibas, andH. Su, “PartNet: A large-scale benchmark for fine-grained andhierarchical part-level 3D object understanding,” in CVPR, 2019.</span><br><span class="line"> [242] L. Zhao and W. Tao, “JSNet: Joint instance and semantic segmentation of 3D point clouds,” in AAAI, 2020.</span><br><span class="line"> [243] B. De Brabandere, D. Neven, and L. Van Gool, “Semantic instancesegmentation with a discriminative loss function,” in CVPRW,2017.</span><br><span class="line"> [244] S.-M. Hu, J.-X. Cai, and Y.-K. Lai, “Semantic labeling and instancesegmentation of 3D point clouds using patch context analysis andmultiscale processing,” IEEE TVCG, 2018.</span><br><span class="line"> [245] D. Comaniciu and P. Meer, “Mean shift: A robust approachtoward feature space analysis,” IEEE TPAMI, 2002.</span><br><span class="line"> [246] J. Lahoud, B. Ghanem, M. Pollefeys, and M. R. Oswald, “3Dinstance segmentation via multi-task metric learning,” in ICCV,2019.</span><br><span class="line"> [247] B. Zhang and P. Wonka, “Point cloud instance segmentation using probabilistic embeddings,” arXiv preprint arXiv:1912.00145,2019.</span><br><span class="line"> [248] Z. Wang and F. Lu, “VoxSegNet: Volumetric CNNs for semanticpart segmentation of 3D shapes,” IEEE TVCG, 2019.</span><br><span class="line"> [249] E. Kalogerakis, M. Averkiou, S. Maji, and S. Chaudhuri, “3Dshape segmentation with projective convolutional networks,” inCVPR, 2017.</span><br><span class="line"> [250] L. Yi, H. Su, X. Guo, and L. J. Guibas, “SyncSpecCNN: Synchronized spectral CNN for 3D shape segmentation,” in CVPR, 2017.</span><br><span class="line"> [251] P. Wang, Y. Gan, P. Shui, F. Yu, Y. Zhang, S. Chen, and Z. Sun, “3Dshape segmentation via shape fully convolutional networks,”Computers <span class="built_in">&amp;</span> Graphics, 2018.</span><br><span class="line"> [252] C. Zhu, K. Xu, S. Chaudhuri, L. Yi, L. Guibas, and H. Zhang,“CoSegNet: Deep co-segmentation of 3D shapes with groupconsistency loss,” arXiv preprint arXiv:1903.10297, 2019.</span><br><span class="line"> [253] Z. Chen, K. Yin, M. Fisher, S. Chaudhuri, and H. Zhang, “BAENET: Branched autoencoder for shape co-segmentation,” inICCV, 2019.</span><br><span class="line"> [254] F. Yu, K. Liu, Y. Zhang, C. Zhu, and K. Xu, “PartNet: A recursivepart decomposition network for fine-grained and hierarchicalshape segmentation,” in CVPR, 2019.</span><br><span class="line"> [255] T. Luo, K. Mo, Z. Huang, J. Xu, S. Hu, L. Wang, and H. Su, “Learning to group: A bottom-up framework for 3D part discovery inunseen categories,” in ICLR, 2020.</span><br><span class="line"> [256] Z. Liu, H. Tang, Y. Lin, and S. Han, “Point-Voxel CNN forefficient 3D deep learning,” in NeurIPS, 2019.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="params">###</span> 翻译</span><br><span class="line"></span><br><span class="line">[1] Z. Liang、Y. Guo、Y. Feng、W. Chen、L. Qiao、L. Zhou、J. Zhang 和 H. Liu，“使用多级成本量和多尺度特征恒定性的立体匹配”，IEEE TPAMI，2019 年。</span><br><span class="line">[2] Y. Guo、F. Sohel、M. Bennamoun、M. Lu 和 J. Wan，“用于 3D 局部表面描述和对象识别的旋转投影统计”，IJCV，2013 年。</span><br><span class="line">[3] Y. Guo、M. Bennamoun、F. Sohel、M. Lu 和 J. Wan，“具有局部表面特征的杂乱场景中的 3D 对象识别：asurvey”，IEEE TPAMI，2014 年。</span><br><span class="line">[4] X. Chen、H. Ma、J. Wan、B. Li 和 T. Xia，“用于自动驾驶的多视图 3D 目标检测网络”，CVPR，2017 年。</span><br><span class="line">[5] C. R. Qi、H. Su、K. Mo 和 L. J. Guibas，“PointNet：用于 3D 分类和分割的深度学习点集”，CVPR，2017 年。</span><br><span class="line">[6] Z. Wu、S. Song、A. Khosla、F. Yu、L. Zhang、X. Tang 和 J. Xiao，“3D shapeNets：体积形状的深度表示”，inCVPR，2015 年。</span><br><span class="line">[7] M.A.乌伊，Q.-H.范，B.-S.Hua、T. Nguyen 和 S.-K.Yeung，“重新审视点云分类：基于真实世界数据的新基准数据集和分类模型”，ICCV，2019 年。</span><br><span class="line">[8] A. X. Chang、T. Funkhouser、L. Guibas、P. Hanrahan、Q. Huang、Z. Li、S. Savarese、M. Savva、S. Song 和 H. Su，“ShapeNet：信息丰富的 3D 模型存储库”，arXiv preprintarXiv：1512.03012,2015 年。</span><br><span class="line">[9] K. Mo， S. Zhu， A. X. Chang， L. Yi， S. Tripathi， L. J. Guibas， 和 H.Su，“PartNet：细粒度和分层部件级 3D 对象理解的大规模基准测试”，CVPR，2019 年。</span><br><span class="line">[10] I. Armeni、O. Sener、AR Zamir、H. Jiang、I. Brilakis、M. Fischer 和 S. Savarese，“大型室内空间的 3D 语义解析”，CVPR，2016 年。</span><br><span class="line">[11] A. Dai、A. X. Chang、M. Savva、M. Halber、T. Funkhouser 和 M.Nießner，“ScanNet：室内场景的丰富注释 3D 重建”，CVPR，2017 年。</span><br><span class="line">[12] T.哈克尔，N.萨维诺夫，L.拉迪基，J.韦格纳，K.辛德勒和M。Pollefeys，“Semantic3D.net：一种新的大规模点云分类基准”，ISPRS，2017 年。</span><br><span class="line">[13] X. Song， P. Wang， D. Zhou， R. Zhu， C. Guan， Y. Dai， H. Su，H. Li， and R. Yang， “Apollocar3D： A large 3D car instanceunderstanding benchmark for autonomous driving”， in CVPR，2019.</span><br><span class="line">[14] A. Geiger、P. Lenz 和 R. Urtasun，“我们准备好迎接自动驾驶了吗”，CVPR，2012 年。</span><br><span class="line">[15] J. Behley、M. Garbade、A. Milioto、J. Quenzel、S. Behnke、C. Stachniss 和 J. Gall，“SemanticKITTI：激光雷达序列语义场景理解数据集”，载于 ICCV，2019 年。</span><br><span class="line">[16] G. Elbaz、T. Avraham 和 A. Fischer，“使用深度神经网络自动编码器进行定位的 3D 点云配准”，CVPR，2017 年，第 4631–4640 页。</span><br><span class="line">[17] 曾国棠Yu， S. Song， D. Suo， E. Walker， A. Rodriguez， 和 J.Xiao，“在亚马逊拣选挑战中用于 6D 姿态估计的多视图自监督深度学习”，ICRA，2017 年，第 1386-1383 页。</span><br><span class="line">[18] X. Han、H. Laga 和 M. Bennamoun，“基于图像的 3D 对象重建：深度学习时代的现状和趋势”，IEEE TPAMI，2019 年。</span><br><span class="line">[19] A. Ioannidou、E. Chatzilari、S. Nikolopoulos 和 I. Kompatsiaris，“使用 3D 数据在计算机视觉中的深度学习进展：Asurvey”，ACM Computing Surveys，2017 年。</span><br><span class="line">[20] E. Ahmed、A. Saint、A. E. R. Shabayek、K. Cherenkova、R. Das、G. Gusev、D. Aouada 和 B. Ottersten，“深度学习改进不同的 3D 数据表示：一项调查，“arXiv preprintarXiv：1808.01462,2018 年。</span><br><span class="line">[21] Y. Xie、J. Tian 和 X. Zhu，“点云语义分割综述”，IEEE GRSM，2020 年。</span><br><span class="line">[22] M. M. Rahman、Y. Tan、J. Xue 和 K. Lu，“深度神经网络时代 3D 对象检测的最新进展：一项调查”，IEEE TIP，2019 年。</span><br><span class="line">[23] K. Siddiqi、J. Zhang、D. Macrini、A. Shokoufandeh、S. Bouix 和 S.Dickinson，“使用中间表面检索铰接式 3-D 模型”，机器视觉与应用，第 19 卷，第 4 期，第 261–275 页，2008 年。</span><br><span class="line">[24] M. De Deuge、B. Douillard、C. Hung 和 A. Quadros，“用于户外 3D 扫描分类的无监督特征学习”，inACRA，2013 年。</span><br><span class="line">[25] S. Song、S. P. Lichtenberg 和 J. Xiao，“Sun RGB-D：RGB-Dscene 理解基准测试套件”，CVPR，2015 年。</span><br><span class="line">[26] A.帕蒂尔，S.马拉，H.刚和Y.-T。Chen，“拥挤城市场景中用于全环绕 3D 多目标检测和跟踪的 H3D 数据集”，ICRA，2019 年。</span><br><span class="line">[27] M.-F.Chang， J. Lambert， P. Sangkloy， J. Singh， S. Bak， A. Hartnett，D. Wang， P. Carr， S. Lucey， D. Ramanan et al.， “Argoverse： 3Dtracking and forecasting with rich maps”， in CVPR， 2019.</span><br><span class="line">[28] R. Kesten、M. Usman、J. Houston、T. Pandya、K. Nadhamuni、A. Ferreira、M. Yuan、B. Low、A. Jain、P. Ondruska 等人，“Lyftlevel 5 av 数据集 2019”，2019 年。</span><br><span class="line">[29] 问-H.Pham、P. Sevestre、R. S. Pahwa、H. Zhan、C. H. Pang、Y. Chen、A. Mustafa、V. Chandrasekhar 和 J. Lin，“A*3D 数据集：在具有挑战性的环境中实现自动驾驶”，ICRA，2020 年。</span><br><span class="line">[30] P. Sun、H. Kretzschmar、X. Dotiwalla、A. Chouard、V. Patnaik、P. Tsui、J. Guo、Y. Zhou、Y. Chai、B. Caine、V. Vasudevan、W. Han、J. Ngiam、H. Zhao、A. Timofeev、S. Ettinger、M. Krivokon、A. Gao、A. Joshi、Y. Zhang、J. Shlens、Z. Chen 和 D. Anguelov，“自动驾驶感知的可扩展性：Waymo 开放数据集，“在 CVPR 中，2020 年。</span><br><span class="line">[31] H. Caesar、V. Bankiti、AH Lang、S. Vora、V. E. Liong、Q. Xu、A. Krishnan、Y. Pan、G. Baldan 和 O. Beijbom，“nuscenes：自动驾驶的多模态数据集”，CVPR，2020 年。</span><br><span class="line">[32] D. Munoz、J. A. Bagnell、N. Vandapel 和 M. Hebert，“具有函数最大边距马尔可夫网络的上下文分类”，载于 CVPR，2009 年，第 975-982 页。</span><br><span class="line">[33] F. Rottensteiner、G. Sohn、J. Jung、M. Gerke、C. Baillard、S. Benitez 和 U. Breitkopf，“城市对象分类和 3D 建筑重建的 isprs 基准”，ISPRS，2012 年。</span><br><span class="line">[34] A.塞尔纳，B.马科特吉，F.古莱特和J.-E.Deschaud，“Parisrue-madame 数据库：用于城市检测、分割和分类方法基准测试的 3D 移动激光扫描仪数据集”，ICRA，2014 年。</span><br><span class="line">[35] B. Vallet、M. Bredif、A. Serna、B. Marcotegui 和 N. Paparoditis，“Terramobilita/iqmulus 城市点云分析基准”，《计算机与图形学》，第 49 卷，第 126-133 页，2015 年。</span><br><span class="line">[36] X.罗伊纳德，J.-E.Deschaud 和 F. Goulette，“Paris-lille-3d：用于自动分割和分类的大型高质量地面实况城市点云数据集”，IJRR，2018 年。</span><br><span class="line">[37] W. Tan、N. Qin、L. Ma、Y. Li、J. Du、G. Cai、K. Yang 和 J. Li，“Toronto-3D：用于城市道路语义分割的大规模移动激光雷达数据集”，arXiv 预印本 arXiv：2003.08284,2020。</span><br><span class="line">[38] N. Varney、V. K. Asari 和 Q. Graehling，“Dales：用于语义序列的大规模航空激光雷达数据集ntation，“ arXiv 预印本arXiv：2004.11985， 2020.</span><br><span class="line">[39] H. Lu、X. Chen、G. Zhang、Q. Zhou、Y. Ma 和 Y. Zhao，“SCANet：用于 3D 对象检测的空间通道注意力网络”，inICASSP，2019 年。</span><br><span class="line">[40] H. Su、S. Maji、E. Kalogerakis 和 E. Learned-Miller，“用于 3D 形状识别的多视图卷积神经网络”，载于 ICCV，2015 年。</span><br><span class="line">[41] T. Yu、J. Meng 和 J. Yuan，“用于 3D 对象识别的多视图协调双线性网络”，CVPR，2018 年。</span><br><span class="line">[42] Z. Yang 和 L. Wang，“多视图 3D 对象识别的学习关系”，载于 ICCV，2019 年。</span><br><span class="line">[43] C. R. Qi、H. Su、M. Nießner、A. Dai、M. Yan 和 L. J. Guibas，“用于 3Ddata 对象分类的体积和多视图 CNN”，载于 CVPR，2016 年。</span><br><span class="line">[44] Y. Feng、Z. Zhang、X. Zhao、R. Ji 和 Y. Gao，“GVCNN：用于 3D 形状识别的 Groupview 卷积神经网络”，CVPR，2018 年。</span><br><span class="line">[45] C. Wang、M. Pelillo 和 K. Siddiqi，“用于多视图 3D 对象识别的显性集聚类和池化”，BMVC，2017 年。</span><br><span class="line">[46] 马志强， 郭彦， 杨振， 和 W.An，“使用 LSTM 学习多视图表示以进行 3D 形状识别和检索”，IEEE TMM，2018 年。</span><br><span class="line">[47] X. Wei、R. Yu 和 J. Sun，“View-gcn：用于 3D 形状分析的基于视图的图卷积网络”，CVPR，2020 年。</span><br><span class="line">[48] D. Maturana 和 S. Scherer，“VoxNet：用于实时对象识别的 3D 卷积神经网络”，IROS，2015 年。</span><br><span class="line">[49] G. Riegler、A. Osman Ulusoy 和 A. Geiger，“OctNet：在高分辨率下学习深度 3D 表示”，CVPR，2017 年。</span><br><span class="line">[50] 附言王彦彦， Y.-X.郭振英Sun 和 X. Tong，“OCNN：用于 3D 形状分析的基于 Octee 的卷积神经网络”，ACM TOG，2017 年。</span><br><span class="line">[51] T. Le 和 Y. Duan，“PointGrid：用于 3D 形状理解的深度网络”，CVPR，2018 年。</span><br><span class="line">[52] Y. Ben-Shabat、M. Lindenbaum 和 A. Fischer，“使用 3D 改进的 fisher 向量表示进行卷积神经网络的 3D 点云分类和分割”，arXiv预印本 arXiv：1711.08241,2017 年。</span><br><span class="line">[53] M. Zaheer、S. Kottur、S. Ravanbakhsh、B. Poczos、R. R. Salakhutdinov 和 AJ Smola，“深度集”，载于 NeurIPS，2017 年。</span><br><span class="line">[54] C. R. Qi、L. Yi、H. Su 和 L. J. Guibas，“PointNet++：度量空间中点集的深度分层特征学习”，inNeurIPS，2017 年。</span><br><span class="line">[55] M. Joseph-Rivlin、A. Zvirin 和 R. Kimmel，“Mo-Net：学习对形状进行分类的风味时刻”，载于 ICCVW，2018 年。</span><br><span class="line">[56] J. Yang、Q. Zhang、B. Ni、L. Li、J. Liu、M. Zhou 和 Q. Tian，“使用自注意力和 gumbel 子集采样建模点云”，CVPR，2019 年。</span><br><span class="line">[57] 赵海， 蒋林， C.-W.Fu 和 J. Jia，“PointWeb：增强点云处理的局部邻域特征”，CVPR，2019。</span><br><span class="line">[58] Y. Duan、Y. Zheng、J. Lu、J. Zhou 和 Q. Tian，“点云的结构关系推理”，载于 CVPR，2019 年。</span><br><span class="line">[59] H. Lin、Z. Xiao、Y. Tan、H. Chao 和 S. Ding，“Justlookup：通过可查找表对点云进行毫秒深度特征提取”，ICME，2019 年。</span><br><span class="line">[60] X. Sun、Z. Lian 和 J. Xiao，“SRINet：学习点云分类和分割的严格旋转不变表示”，ACM MM，2019 年。</span><br><span class="line">[61] X. Yan、C. Zheng、Z. Li、S. Wang 和 S. Cui，“Pointasnl：使用非局部神经网络进行鲁棒点云处理ptive 采样“，CVPR，2020 年。</span><br><span class="line">[62] Y. Liu、B. Fan、S. Xiang 和 C. Pan，“用于点云分析的关系形状卷积神经网络”，CVPR，2019 年。</span><br><span class="line">[63] A. Boulch，“推广非结构化点云的离散卷积”，arXiv 预印本 arXiv：1904.02375,2019 年。</span><br><span class="line">[64] Y. Liu、B. Fan、G. Meng、J. Lu、S. Xiang 和 C. Pan，“DensePoint：Learning densely contextual representation for efficient pointcloud processing”，载于 ICCV，2019 年。</span><br><span class="line">[65] H.托马斯，CR齐，J.-E.Deschaud、B. Marcotegui、F. Goulette 和 L. J. Guibas，“KPConv：点云的灵活和可变形卷积”，ICCV，2019 年。</span><br><span class="line">[66] A. Boulch，“ConvPoint：用于点云处理的连续卷积”，计算机与图形学，2020 年。</span><br><span class="line">[67] W. Wu、Z. Qi 和 L. Fuxin，“PointConv：3D 点云上的深度卷积网络”，CVPR，2019 年。</span><br><span class="line">[68] P.赫莫西拉，T.里切尔，P.-P.Vazquez、A. Vinacua 和 T. Ropinski，“用于在非均匀采样点云上学习的蒙特卡洛卷积”，ACM TOG，2018 年。</span><br><span class="line">[69] Y. Xu、T. Fan、M. Xu、L. Zeng 和 Y. Qiao，“SpiderCNN：使用参数化卷积滤波器对点集进行深度学习”，载于 ECCV，2018 年。</span><br><span class="line">[70] A. Matan、M. Haggai 和 L. Yaron，“扩展算子的点卷积神经网络”，ACM TOG，2018 年。</span><br><span class="line">[71] C. Esteves、C. Allen-Blanchette、A. Makadia 和 K. Daniilidis，“使用球形 CNN 学习 so（3） 等变表示”，ECCV，2017 年。</span><br><span class="line">[72] N. Thomas、T. Smidt、S. Kearnes、L. Yang、L. Li、K. Kohlhoff 和 P. Riley，“张量场网络：3D 点云的旋转和平移等变神经网络”，arXiv 预印本arXiv：1802.08219,2018 年。</span><br><span class="line">[73] TS Cohen、M. Geiger、J. Koehler 和 M. Welling，“球形CNN”，ICLR，2018 年。</span><br><span class="line">[74] A.普莱纳德，M.-J.Rakotosaona、Y. Ponty 和 M. Ovsjanikov，“具有球谐核的有效旋转不变点 CNN”，3DV，2019 年。</span><br><span class="line">[75] F. Groh、P. Wieschollek 和 H. P. Lensch，“Flex-Convolution”，inACCV，2018 年。</span><br><span class="line">[76] 学士华， M.-K.Tran 和 S.-K.Yeung，“逐点卷积神经网络”，CVPR，2018 年。</span><br><span class="line">[77] H. Lei、N. Akhtar 和 A. Mian，“Octree 引导的 CNN 与用于 3D 点云的球形内核”，CVPR，2019 年。</span><br><span class="line">[78] S. Lan、R. Yu、G. Yu 和 L. S. Davis，“使用 geo-cnn 对 3D 点云的局部几何结构进行建模”，CVPR，2019 年。</span><br><span class="line">[79] Y. Li、R. Bu、M. Sun、W. Wu、X. Di 和 B. Chen，“PointCNN：x 变换点上的卷积”，载于 NeurIPS，2018 年。</span><br><span class="line">[80] J. Mao、X. Wang 和 H. Li，“用于 3D 点云理解的插值卷积网络”，载于 ICCV，2019 年。</span><br><span class="line">[81] 张志明， B.-S.华、DW Rosen 和 SKYeung，“Rotationinvariant convolutions for 3D point clouds deep learning”，in3DV，2019 年。</span><br><span class="line">[82] A. Komarichev、Z. Zhong 和 J. Hua，“A-CNN：点云上的环状卷积神经网络”，CVPR，2019 年。</span><br><span class="line">[83] S. Kumawat 和 S. Raman，“LP-3DCNN：揭开局部相位 3D 卷积神经网络”，CVPR，2019 年。</span><br><span class="line">[84] Y. Rao、J. Lu 和 J. Zhou，“用于点云识别的球形分形卷积神经网络”，CVPR，2019 年。</span><br><span class="line">[85] M. Simonovsky 和 N. Komodakis，“图卷积神经网络中的动态边缘条件滤波器”，CVPR，2017。</span><br><span class="line">[86] R.B. Rusu 和 S. Cousins，“3D 在这里：点云库 （PCL）”，ICRA，2011 年。</span><br><span class="line">[87] Y. Wang、Y. Sun、Z. Liu、SE Sarma、MM Bronstein 和 JM Solomon，“用于点云学习的动态图 CNN”，ACM TOG，2019 年。</span><br><span class="line">[88] K. Zhang、M. Hao、J. Wang、C. W. de Silva 和 C. Fu，“Linkeddynamic graph CNN：通过链接分层特征在点云上学习”，arXiv 预印本 arXiv：1904.10014,2019。</span><br><span class="line">[89] Y. Yang、C. Feng、Y. Shen 和 D. Tian，“FoldingNet：通过深度网格变形的点云自动编码器”，载于 CVPR，2018 年。</span><br><span class="line">[90] C. Szegedy、W. Liu、Y. Jia、P. Sermanet、S. Reed、D. Anguelov、D. Erhan、V. Vanhoucke 和 A. Rabinovich，“更深入地使用卷积”，CVPR，2015 年。</span><br><span class="line">[91] K. Hassani 和 M. Haley，“点云上的无监督多任务特征学习”，载于 ICCV，2019 年。</span><br><span class="line">[92] J. Liu、B. Ni、C. Li、J. Yang 和 Q. Tian，“分层点集学习的动态点集聚”，载于 ICCV，2019。</span><br><span class="line">[93] Y. Shen， C. Feng， Y. Yang， and D. Tian， “Mining point cloud localstructures by kernel correlation and graph pooling”， in CVPR，2018.</span><br><span class="line">[94] M.多明格斯，R.Dhamdhere，A.佩特卡，S.耆那教，S.萨和R。Ptucha，“通用深度点云特征提取器”，WACV，2018 年。</span><br><span class="line">[95] C. Chen、G. Li、R. Xu、T. Chen、M. Wang 和 L. Lin，“ClusterNet：用于点云分析的具有严格旋转不变表示的深度分层集群网络”，CVPR，2019 年。</span><br><span class="line">[96] D. Mullner，“现代分层、聚合聚类算法”，arXiv 预印本 arXiv：1109.2378,2011 年。</span><br><span class="line">[97] 徐庆， 孙旭， C.-Y.Wu、P. Wang 和 U. Neumann，“Grid-gcnfor fast and scalable point cloud learning”，CVPR，2020 年。</span><br><span class="line">[98] J. Bruna、W. Zaremba、A. Szlam 和 Y. Lecun，“图上的光谱网络和本地连接网络”，ICLR，2014 年。</span><br><span class="line">[99] M. Defferrard、X. Bresson 和 P. Vandergheynst，“具有快速局部光谱过滤的图上的卷积神经网络”，载于 NeurIPS，2016 年。</span><br><span class="line">[100] G. Te、W. Hu、A. Zheng 和 Z. Guo，“RGCNN：用于点云分割的正则化图 CNN”，ACM MM，2018 年。</span><br><span class="line">[101] R. Li、S. Wang、F. Zhu 和 J. Huang，“自适应图卷积神经网络”，AAAI，2018 年。</span><br><span class="line">[102] Y. Feng、H. You、Z. Zhang、R. Ji 和 Y. Gao，“Hypergraph 神经网络”，AAAI，2019 年。</span><br><span class="line">[103] C. Wang、B. Samari 和 K. Siddiqi，“用于点集特征学习的局部谱图卷积”，载于 ECCV，2018 年。</span><br><span class="line">[104] Y. Zhang 和 M. Rabbat，“用于 3D 点云分类的 Graph-CNN”，ICASSP，2018 年。</span><br><span class="line">[105] G. Pan、J. Wang、R. Ying 和 P. Liu，“3DTI-Net：使用动态 GCN 学习内变换不变 3D 几何特征”，arXiv 预印本 arXiv：1812.06254,2018 年。</span><br><span class="line">[106] R. Klokov 和 V. Lempitsky，“逃离细胞：用于识别 3D 点云模型的深度 kdnetworks”，载于 ICCV，2017。</span><br><span class="line">[107] W. Zeng 和 T. Gevers，“3DContextNet：使用局部和全局上下文线索对点云进行 K-d 树引导的分层学习”，载于 ECCV，2018 年。</span><br><span class="line">[108] J. Li、B. M. Chen 和 G. Hee Lee，“SO-Net：用于点云分析的自组织网络”，CVPR，2018 年。</span><br><span class="line">[109] S. Xie、S. Liu、Z. Chen 和 Z. Tu，“用于点云识别的 Attentional ShapeContextNet”，CVPR，2018 年。</span><br><span class="line">[110] H. You， Y. Feng， R. Ji，和 Y. Gao，“PVNet：用于 3D 形状识别的点云和多视图的联合卷积网络”，ACM MM，2018 年。</span><br><span class="line">[111] H. You、Y. Feng、X. Zhao、C. Zou、R. Ji 和 Y. Gao，“PVRNet：用于 3D 形状识别的点视关系神经网络”，AAAI，2019 年。</span><br><span class="line">[112] Y. Zhao、T. Birdal、H. Deng 和 F. Tombari，“3D 点胶囊网络”，CVPR，2019 年。</span><br><span class="line">[113] W. Chen， X. Han， G. Li， C. Chen， J. Xing， Y. Zhao， and H. Li，“Deep RBFNet： Point cloud feature learning using radial basisfunctions，” arXiv preprint arXiv：1812.04302， 2018.</span><br><span class="line">[114] 刘旭， 韩彦彦.Liu 和 M. Zwicker，“Point2Sequence：Learning the shape representation of 3D point clouds with anattention-based sequence to sequence network”，AAAI，2019 年。</span><br><span class="line">[115] P. Wu、C. Chen、J. Yi 和 D. Metaxas，“通过循环集编码进行点云处理”，AAAI，2019 年。</span><br><span class="line">[116] 秦志强， 游海， 王玲玲J. Kuo和Y. Fu，“PointDAN：用于点云表示的多尺度3D域自适应网络”，NIPS，2019年。</span><br><span class="line">[117] B. Sievers 和 J. Sauder，“通过重建空间对点云进行自监督深度学习”，载于 NIPS，2019 年。</span><br><span class="line">[118] R.李，X.Li，P.-A.Heng 和 C.-W.Fu，“PointAugment：用于点云分类的自动增强框架”，inCVPR，2020 年。</span><br><span class="line">[119] S. Belongie、J. Malik 和 J. Puzicha，“使用形状上下文进行形状匹配和对象识别”，IEEE TPAMI，2002 年。</span><br><span class="line">[120] A.瓦斯瓦尼，N.沙泽尔，N.帕尔马，J.乌斯科雷特，L.琼斯，A.N.戈麦斯，罗。Kaiser 和 I. Polosukhin，“注意力就是你所需要的一切”，NeurIPS，2017 年。</span><br><span class="line">[121] D. Bobkov、S. Chen、R. Jian、Z. Iqbal 和 E. Steinbach，“使用点对描述符在 3D 点云中进行对象分类的抗噪深度学习”，IEEE RAL，2018 年。</span><br><span class="line">[122] S. Prokudin、C. Lassner 和 J. Romero，“使用基点集高效学习点云”，ICCV，2019 年。</span><br><span class="line">[123] L. Liu， W. Ouyang， X. Wang， P. Fieguth， J. Chen， X. Liu， 和 M.Pietikainen，“用于通用对象检测的深度学习：一项调查”，IJCV，2020 年。</span><br><span class="line">[124] C.R.齐，O.利塔尼，K.He 和 L. J. Guibas，“Deep hough votingfor 3D object detection in point clouds”，ICCV，2019 年。</span><br><span class="line">[125] W. Shi 和 R. Rajkumar，“Point-GNN：用于点云中 3D 对象检测的图形神经网络”，CVPR，2020 年。</span><br><span class="line">[126] J. Ku、M. Mozifian、J. Lee、A. Harakeh 和 SL Waslander，“联合 3D 提案生成和来自视图聚合的对象检测”，IROS，2018 年。</span><br><span class="line">[127] M. Liang、B. Yang、S. Wang 和 R. Urtasun，“用于多传感器 3D 目标检测的深度连续融合”，载于 ECCV，2018 年。</span><br><span class="line">[128] M. Liang、B. Yang、Y. Chen、R. Hu 和 R. Urtasun，“用于 3D 目标检测的多任务多传感器融合”，CVPR，2019 年。</span><br><span class="line">[129] B. Yang、W. Luo 和 R. Urtasun，“PIXOR：来自点云的实时 3D 对象检测”，CVPR，2018 年。</span><br><span class="line">[130] W. Luo、B. Yang 和 R. Urtasun，“速度与激情：使用单个卷积网络进行实时端到端 3D 检测、跟踪和运动预测”，CVPR，2018 年。</span><br><span class="line">[131] Y. Zeng、Y. Hu、S. Liu、J. Ye、Y. Han、X. Li 和 N. Sun，“RT3D：用于自动驾驶的激光雷达点云中的实时 3D 车辆检测”，IEEE RAL，2018 年。</span><br><span class="line">[132] Z. Yang、Y. Sun、S. Liu、X. Shen 和 J. Jia，“IPOD：基于密集点的对象点云检测器，“arXiv预印本arXiv：1812.05276,2018。</span><br><span class="line">[133] S. Shi、X. Wang 和 H. Li，“PointRCNN：来自点云的 3D 对象提案生成和检测”，CVPR，2019 年。</span><br><span class="line">[134] Z. Jesus、G. Silvio 和 G. Bernard，“PointRGCN：用于 3D 车辆检测细化的图卷积网络”，arXiv预印本 arXiv：1911.12236,2019 年。</span><br><span class="line">[135] V. Sourabh、L. Alex H.、H. Bassam 和 B. Oscar，“PointPainting：用于 3D 对象检测的顺序融合”，CVPR，2020 年。</span><br><span class="line">[136] Y. Zhou 和 O. Tuzel，“VoxelNet：基于点云的 3D 对象检测的端到端学习”，CVPR，2018 年。</span><br><span class="line">[137] A. H. Lang、S. Vora、H. Caesar、L. Zhou、J. Yang 和 O. Beijbom，“PointPillars：用于点云对象检测的快速编码器”，CVPR，2019 年。</span><br><span class="line">[138] Z. Yang、Y. Sun、S. Liu、X. Shen 和 J. Jia，“STD：用于点云的稀疏到密集 3D 对象检测器”，载于 ICCV，2019 年。</span><br><span class="line">[139] C. R. Qi、W. Liu、C. Wu、H. Su 和 L. J. Guibas，“用于从 RGB-D 数据检测 3D 对象的 FrustumPointNets”，载于 CVPR，2018。</span><br><span class="line">[140] X. Zhao、Z. Liu、R. Hu 和 K. Huang，“使用尺度不变和特征重加权网络进行 3D 目标检测”，AAAI，2019 年。</span><br><span class="line">[141] M. Jiang、Y. Wu 和 C. Lu，“PointSIFT：用于 3D 点云语义分割的类似筛子的网络模块”，arXiv 预印本arXiv：1807.00652,2018 年。</span><br><span class="line">[142] D. Xu、D. Anguelov 和 A. Jain，“PointFusion：用于 3D 边界框估计的深度传感器融合”，CVPR，2018 年。</span><br><span class="line">[143] K. Shin、Y. P. Kwon 和 M. Tomizuka，“RoarNet：基于区域近似细化的鲁棒 3D 对象检测”，IEEE IV，2019 年。</span><br><span class="line">[144] Z. Wang 和 K. Jia，“视锥体 convNet：滑动视锥体聚合局部逐点特征以进行模态 3D 对象检测”，IROS，2019 年。</span><br><span class="line">[145] L. Johannes、M. Andreas、A. Thomas、H. Markus、N. Bernhard 和 H. Sepp，“补丁细化 - 本地化 3D 对象检测”，arXiv 预印本 arXiv：1910.04093,2019 年。</span><br><span class="line">[146] D. Zhou， J. Fang， X. Song， C. Guan， J. Yin， Y. Dai， and R. Yang，“用于 2D/3D 对象检测的 Iou 损失”，载于 3DV，2019 年。</span><br><span class="line">[147] Y. Chen、S. Liu、X. Shen 和 J. Jia，“Fast point r-cnn”，载于 ICCV，2019。</span><br><span class="line">[148] S. Shi， C. Guo， L. Jiang， Z. Wang， J. Shi， X. Wang， and H. Li，“PV-RCNN：用于 3D 对象检测的点体素特征集抽象”，CVPR，2020 年。</span><br><span class="line">[149] M. Feng、S. Z. Gilani、Y. Wang、L. Zhang 和 A. Mian，“用于点云中 3D 对象检测的关系图网络”，arXivpreprint arXiv：1912.00202,2019 年。</span><br><span class="line">[150] C. R. Qi、X. Chen、O. Litany 和 L. J. Guibas，“ImVoteNet：Boosting 3D object detection in point clouds with image votes”，CVPR，2020 年。</span><br><span class="line">[151] S. Shi、Z. Wang、X. Wang 和 H. Li，“从点到零件：具有零件感知和零件聚合网络的点云 3D 对象检测”，TPAMI，2020 年。</span><br><span class="line">[152] B. Yang、M. Liang 和 R. Urtasun，“HDNET：利用高清地图进行 3D 对象检测”，CoRL，2018 年。</span><br><span class="line">[153] J.贝尔特兰，C.金德尔，FM莫雷诺，D.克鲁萨多，F.加克ıa和A。De La Escalera，“BirdNet：来自激光雷达信息的 3D 对象检测框架”，ITSC，2018 年。</span><br><span class="line">[154] B. Li、T. Zhang 和 T. Xia，“使用卷积网络从 3D 激光雷达进行车辆检测”，arXiv 预印本 arXiv：1608.07916,2016。</span><br><span class="line">[155] B. Li，“3D 全卷积网络k 表示车辆检测点云“，IROS，2017 年。</span><br><span class="line">[156] M. Engelcke、D. Rao、D. Z. Wang、C. H. Tong 和 I. Posner，“Vote3Deep：使用高效卷积神经网络在 3D 点云中快速检测对象”，ICRA，2017 年。</span><br><span class="line">[157] X. Li、J. E. Guivant、N. Kwok 和 Y. Xu，“用于 3D 目标检测的 3D 骨干网络”，CoRR，2019 年。</span><br><span class="line">[158] Y. Yan、Y. Mao 和 B. Li，“SECOND：稀疏嵌入卷积检测”，传感器，2018 年。</span><br><span class="line">[159] V. A. Sindagi、Y. Zhou 和 O. Tuzel，“MVX-Net：用于 3D 对象检测的多模态体素网络”，ICRA，2019 年。</span><br><span class="line">[160] 何志强， 曾海， 黄旭东， X.-S.Hua 和 L. Zhang，“基于点云的结构感知单阶段 3D 目标检测”，CVPR，2020 年。</span><br><span class="line">[161] Z. Yang、Y. Sun、S. Liu 和 J. Jia，“3DSSD：基于点的 3D 单级目标检测器”，CVPR，2020 年。</span><br><span class="line">[162] GP Meyer、A. Laddha、E. Kee、C. Vallespi-Gonzalez 和 C. K.Wellington，“LaserNet：用于自动驾驶的高效概率 3D 目标检测器”，CVPR，2019 年。</span><br><span class="line">[163] GP Meyer、J. Charland、D. Hegde、A. Laddha 和 C. VallespiGonzalez，“用于联合 3D 对象检测和语义分割的传感器融合”，CVPRW，2019 年。</span><br><span class="line">[164] Q. Chen、L. Sun、Z. Wang、K. Jia 和 A. Yuille，“物体射点：一种通过发射热点的无锚 3D 物体检测方法”，arXiv 预印本 arXiv：1912.12791,2019 年。</span><br><span class="line">[165] O. Ronneberger、P. Fischer 和 T. Brox，“U-Net：用于生物医学图像分割的卷积网络”，载于 MICCAI，2015 年，第 234-241 页。</span><br><span class="line">[166] B. Graham、M. Engelcke 和 L. van der Maaten，“具有子流形稀疏卷积网络的 3D 语义分割”，CVPR，2018 年。</span><br><span class="line">[167] 胡庆， 郭勇， 陈彦， 肖军， 和 W.An，“Correlation filtertracking： Beyond an open-loop system”，BMVC，2017 年。</span><br><span class="line">[168] H. Liu、Q. Hu、B. Li 和 Y. Guo，“通过实例特定提案进行稳健的长期跟踪”，IEEE TIM，2019 年。</span><br><span class="line">[169] L. Bertinetto、J. Valmadre、J. F. Henriques、A. Vedaldi 和 P. H.Torr，“用于对象跟踪的全卷积连体网络”，ECCV，2016 年。</span><br><span class="line">[170] S. Giancola、J. Zarzar 和 B. Ghanem，“利用形状补全进行 3D 连体跟踪”，CVPR，2019 年。</span><br><span class="line">[171] M. Mueller、N. Smith 和 B. Ghanem，“上下文感知相关过滤器跟踪”，CVPR，2017 年。</span><br><span class="line">[172] J. Zarzar、S. Giancola 和 B. Ghanem，“在激光雷达上使用 2D-3D 连体网络的高效跟踪建议”，arXiv 预印本arXiv：1903.10168,2019 年。</span><br><span class="line">[173] M. Simon、K. Amende、A. Kraus、J. Honer、T. Samann、H. Kaulber-̈sch、S. Milz 和 HM Gross，“Complexer-YOLO：语义点云上的实时 3D 对象检测和跟踪”，CVPRW，2019 年。</span><br><span class="line">[174] H. Qi、C. Feng、Z. Cao、F. Zhao 和 Y. Xiao，“P2B：用于点云中 3D 对象跟踪的点对盒网络”，CVPR，2020 年。</span><br><span class="line">[175] X. Liu、C. R. Qi 和 L. J. Guibas，“FlowNet3D：学习 3D 点云中的场景流”，CVPR，2019 年。</span><br><span class="line">[176] Z. Wang、S. Li、H. Howard-Jenkins、V. Prisacariu 和 M. Chen，“FlowNet3D++：深度场景流量估计的几何损失”，WACV，2020 年。</span><br><span class="line">[177] X. Gu、Y. Wang、C. Wu、Y. J. Lee 和 P. Wang，“HPLFlowNet：Hierarchical permutohedral lattice flowNet for scene flow estimation on large-scale point clouds”，载于 CVPR，2019 年。</span><br><span class="line">[178] 樊鸿燊和 Y. Yang，“PointRNN：用于移动点云处理的点递归神经网络”，arXiv 预印本arXiv：1910.08287,2019 年。</span><br><span class="line">[179] X. Liu、M. Yan 和 J. Bohg，“MeteorNet：动态 3D 点云序列的深度学习”，载于 ICCV，2019 年。</span><br><span class="line">[180] H. Mittal、B. Okonn 和 D. Held，“顺其自然：自监督场景流估计”，CVPR，2020 年。</span><br><span class="line">[181] F.J.劳文，M.丹内尔扬，P.托斯特伯格，G.巴特，FS汗和M。Felsberg，“深度投射 3D 语义分割”，inCAIP，2017 年。</span><br><span class="line">[182] A. Boulch、B. Le Saux 和 N. Audebert，“使用深度分割网络的非结构化点云语义标记”，in3DOR，2017 年。</span><br><span class="line">[183] B. Wu、A. Wan、X. Yue 和 K. Keutzer，“SqueezeSeg：具有递归 crf 的卷积神经网络，用于从 3D 激光雷达点云进行实时道路物体分割”，ICRA，2018 年。</span><br><span class="line">[184] B. Wu、X. Zhou、S. Zhao、X. Yue 和 K. Keutzer，“SqueezeSegV2：改进的模型结构和无监督域适应，用于激光雷达点云的道路对象分割”，ICRA，2019 年。</span><br><span class="line">[185] A. Milioto、I. Vizzo、J. Behley 和 C. Stachniss，“RangeNet++：快速准确的激光雷达语义分割”，IROS，2019 年。</span><br><span class="line">[186] H.-Y.孟， L. Gao， Y.-K.Lai 和 D. Manocha，“VV-Net：用于点云分割的具有组卷积的体素网络”，ICCV，2019 年。</span><br><span class="line">[187] D. Rethage、J. Wald、J. Sturm、N. Navab 和 F. Tombari，“大规模点云的全卷积点网络”，inECCV，2018 年。</span><br><span class="line">[188] H. Su， V. Jampani， D. Sun， S. Maji， E. Kalogerakis， M.-H.Yang 和 J. Kautz，“SplatNet：用于点云处理的稀疏晶格网络”，CVPR，2018 年。</span><br><span class="line">[189] RA Rosu、P. Schutt、J. Quenzel 和 S. Behnke，“LatticeNet：使用变面体晶格的快速点云分割”，arXiv预印本 arXiv：1912.05905,2019 年。</span><br><span class="line">[190] A. Dai 和 M. Nießner，“3DMV：用于 3D 语义场景分割的联合 3D 多视图预测”，载于 ECCV，2018 年。</span><br><span class="line">[191] M. Jaritz、J. Gu 和 H. Su，“用于 3D 场景理解的多视图 pointNet”，载于 ICCVW，2019 年。</span><br><span class="line">[192] N. Audebert、B. Le Saux 和 S. Lefevre，“使用多模态和多尺度深度网络对地球观测数据进行语义分割”，载于 ACCV，2016 年。</span><br><span class="line">[193] M. Tatarchenko、J. Park、V. Koltun 和 Q.-Y.Zhou，“切线卷积用于 3D 密集预测”，CVPR，2018 年。</span><br><span class="line">[194] F. N. Iandola、S. Han、MW Moskewicz、K. Ashraf、W. J. Dally 和 K. Keutzer，“SqueezeNet：具有 50 倍更少参数和 0.5 MB 模型大小&lt;的 Alexnet 级精度”，ICLR，2016 年。</span><br><span class="line">[195] J. Huang 和 S.您，“使用 3D 卷积神经网络进行点云标记”，ICPR，2016 年。</span><br><span class="line">[196] L. Tchapmi、C. Choy、I. Armeni、J. Gwak 和 S. Savarese，“SEGCloud：3D 点云的语义分割”，载于 3DV，2017 年。</span><br><span class="line">[197] J. Long、E. Shelhamer 和 T. Darrell，“用于语义分割的全卷积网络”，CVPR，2015 年。</span><br><span class="line">[198] A. Dai、D. Ritchie、M. Bokeloh、S. Reed、J. Sturm 和 M. Nießner，“ScanComplete：3D 扫描的大规模场景完成和语义分割”，CVPR，2018 年。</span><br><span class="line">[199] C. Choy、J. Gwak 和 S. Savarese，“4D 时空卷积网络：Minkowski 卷积神经网络”，CVPR，2019 年。</span><br><span class="line">[200] H.-Y.蒋，Y.-L.林永昊Liu， 和 W. H. Hsu，“用于 3D 分割的基于统一点的框架”，3DV，2019 年。</span><br><span class="line">[201] 王淑娟， 索淑娴， W.-C.Ma、A. Pokrovsky 和 R. Urtasun，“Deepparametric 连续卷积神经网络”，CVPR，2018 年。</span><br><span class="line">[202] X. Ye、J. Li、H. Huang、L. Du 和 X. Zhang，“用于点云语义分割的上下文融合的 3D 循环神经网络”，载于 ECCV，2018 年。</span><br><span class="line">[203] L. Landrieu 和 M. Simonovsky，“使用超点图进行大规模点云语义分割”，CVPR，2018 年。</span><br><span class="line">[204] F. Engelmann、T. Kontogianni、J. Schult 和 B. Leibe，“知道你的邻居做什么：点云的 3D 语义分割”，ECCVW，2018 年。</span><br><span class="line">[205] Z. Zhang， B.-S.华和 S.-K.Yeung，“ShellNet：使用同心壳统计的高效点云卷积神经网络”，ICCV，2019 年。</span><br><span class="line">[206] 胡庆， 杨斌， 谢玲， S. Rosa， 郭彦， 王志， N. Trigoni， 和 A.Markham，“RandLA-Net：大规模点云的高效语义分割”，CVPR，2020 年。</span><br><span class="line">[207] L.-Z.陈晓燕李 D.-P.范，M.-M.Cheng， K. Wang， 和 S.-P.Lu，“LSANet：通过局部空间注意对点集进行特征学习”，arXiv 预印本 arXiv：1905.05442,2019 年。</span><br><span class="line">[208] C. Zhao、W. Zhou、L. Lu 和 Q. Zhao，“汇总相邻点的分数以改进 3D 点云分割”，ICIP，2019 年。</span><br><span class="line">[209] R. Arandjelovic、P. Gronat、A. Torii、T. Pajdla 和 J. Sivic，“NetVLAD：用于弱监督位置识别的 CNN 架构”，CVPR，2016 年。</span><br><span class="line">[210] F. Engelmann、T. Kontogianni、J. Schult 和 B. Leibe，“知道你的邻居做什么：点云的 3D 语义分割”，ECCV，2018 年。</span><br><span class="line">[211] F. Engelmann、T. Kontogianni 和 B. Leibe，“扩张点卷积：关于点卷积的感受场”，ICRA，2020 年。</span><br><span class="line">[212] Q. Huang、W. Wang 和 U. Neumann，“用于点云 3D 分割的循环切片网络”，CVPR，2018 年。</span><br><span class="line">[213] F. Engelmann、T. Kontogianni、A. Hermans 和 B. Leibe，“探索点云 3D 语义分割的空间上下文”，载于 ICCV，2017 年。</span><br><span class="line">[214] L. Landrieu 和 M. Boussaha，“点云过度分割与图结构化深度度量学习”，CVPR，2019 年。</span><br><span class="line">[215] L. Wang、Y. Huang、Y. Hou、S. Zhang 和 J. Shan，“用于点云语义分割的图形注意力卷积”，CVPR，2019 年。</span><br><span class="line">[216] L.潘，C.-M.Chew 和 G. H. Lee，“Pointatrousgraph：Deephierarchical encoder-decoder with atrous convolution for pointclouds”，arXiv 预印本 arXiv：1907.09798,2019 年。</span><br><span class="line">[217] Z. Liang、M. Yang、L. Deng、C. Wang 和 B. Wang，“用于点云 3D 语义分割的分层深度图卷积神经网络”，ICRA，2019 年。</span><br><span class="line">[218] 蒋玲， 赵海， 刘淑娟， 沈旭， C.-W.Fu 和 J. Jia，“用于点云语义分割的分层点边交互网络”，载于 ICCV，2019 年。</span><br><span class="line">[219] H. Lei、N. Akhtar 和 A. Mian，“用于 3D 点云的球形卷积神经网络”，arXiv 预印本 arXiv：1805.07872,2018。</span><br><span class="line">[220] Z. Zhao、M. Liu 和 K. Ramani，“DAR-Net：用于语义场景分割的动态聚合网络”，arXiv 预印本arXiv：1907.12022,2019。</span><br><span class="line">[221] F. Liu， S. Li， L. Zhang， C. Zhou， R. Ye， Y. Wang， and J. Lu，“3DCNN-DQN-RNN：用于语义解析的深度强化学习框架。f 大规模 3D 点云“，ICCV，2017。</span><br><span class="line">[222] Z. Kang 和 N. Li，“PyramNet：用于分类和分割的点云金字塔注意力网络和图嵌入模块”，ICONIP，2019 年。</span><br><span class="line">[223] Y. Ma、Y. Guo、H. Liu、Y. Lei 和 G. Wen，“3D 点云语义分割的全局上下文推理”，inWACV，2020 年。</span><br><span class="line">[224] 魏建， 林国强， K.-H.Yap， T.-Y.Hung 和 L. Xie，“点云上弱监督 3D 语义分割的多路径区域挖掘”，CVPR，2020 年。</span><br><span class="line">[225] X. Xu 和 G. H. Lee，“弱监督语义点云分割：迈向 10 倍更少的标签”，CVPR，2020 年，第 13 706–13 715 页。</span><br><span class="line">[226] J. Hou、A. Dai 和 M. Nießner，“3D-SIS：RGB-D 扫描的 3D 语义实例分割”，CVPR，2019 年。</span><br><span class="line">[227] L. Yi、W. Zhao、H. Wang、M. Sung 和 L. J. Guibas，“GSPN：用于点云中 3D 实例分割的生成形状建议网络”，CVPR，2019 年。</span><br><span class="line">[228] G. Narita、T. Seno、T. Ishikawa 和 Y. Kaji，“PanopticFusion：事物和事物级别的在线体积语义映射”，IROS，2019 年。</span><br><span class="line">[229] B. Yang， J. Wang， R. Clark， Q. Hu， S. Wang， A. Markham， 和 N.Trigoni，“在点云上学习用于 3D 实例分割的对象边界框”，NeurIPS，2019 年。</span><br><span class="line">[230] F. Zhang， C. Guan， J. Fang， S. Bai， R. Yang， P. Torr， 和 V.Prisacariu，“激光雷达点云的实例分割”，ICRA，2020 年。</span><br><span class="line">[231] Y. Shi、A. X. Chang、Z. Wu、M. Savva 和 K. Xu，“用于 3D 场景布局预测的层次结构去噪递归自动编码器”，CVPR，2019 年。</span><br><span class="line">[232] F. Engelmann、M. Bokeloh、A. Fathi、B. Leibe 和 M. Nießner，“3d-mpa：3d 语义实例分割的多建议聚合”，CVPR，2020 年。</span><br><span class="line">[233] W. Wang、R. Yu、Q. Huang 和 U. Neumann，“SGPN：用于 3D 点云实例分割的 Similaritygroup 提案网络”，CVPR，2018 年。</span><br><span class="line">[234] X. Wang、S. Liu、X. Shen、C. Shen 和 J. Jia，“关联分割点云中的实例和语义”，CVPR，2019。</span><br><span class="line">[235] 问-H.范，T. Nguyen，B.-S.Hua、G. Roig 和 S.-K.Yeung，“JSIS3D：具有多任务逐点网络和多值条件随机字段的三维点云的联合语义实例分割”，CVPR，2019。</span><br><span class="line">[236] C. Elich、F. Engelmann、J. Schult、T. Kontogianni 和 B. Leibe，“3D-BEVIS：鸟瞰视图实例分割”，GCPR，2019 年。</span><br><span class="line">[237] C. Liu 和 Y. Furukawa，“MASC：用于 3D 实例分割的稀疏卷积的多尺度亲和力”，arXiv 预印本arXiv：1902.04478,2019 年。</span><br><span class="line">[238] Z. Liang、M. Yang 和 C. Wang，“3D 图嵌入学习与点云语义实例分割的结构感知损失函数”，arXiv 预印本 arXiv：1902.05247,2019 年。</span><br><span class="line">[239] L. Han、T. Zheng、L. Xu 和 L. Fang，“Occuseg：Occupancyaware 3d 实例分割”，CVPR，2020 年。</span><br><span class="line">[240] 蒋璋玲， 赵晖， 石淑娟， 刘淑娴.Fu 和 J. Jia，“PointGroup：用于 3D 实例分割的双设定点分组”，CVPR，2020 年。</span><br><span class="line">[241] K. Mo， S. Zhu， A. X. Chang， L. Yi， S. Tripathi， L. J. Guibas， 和 H.Su，“PartNet：细粒度和分层部件级 3D 对象理解的大规模基准测试”，CVPR，2019 年。</span><br><span class="line">[242] L. Zhao 和 W. Tao，“JSNet：3D poi 的联合实例和语义分割nt clouds“，AAAI，2020 年。</span><br><span class="line">[243] B. De Brabandere、D. Neven 和 L. Van Gool，“具有判别性损失函数的语义实例分割”，CVPRW，2017 年。</span><br><span class="line">[244] S.-M.胡锦涛蔡和Y.-K.Lai，“使用补丁上下文分析和多尺度处理对 3D 点云进行语义标记和实例分割”，IEEE TVCG，2018 年。</span><br><span class="line">[245] D. Comaniciu 和 P. Meer，“均值偏移：一种稳健的特征空间分析方法”，IEEE TPAMI，2002 年。</span><br><span class="line">[246] J. Lahoud、B. Ghanem、M. Pollefeys 和 M. R. Oswald，“通过多任务指标学习进行 3D 实例分割”，载于 ICCV，2019。</span><br><span class="line">[247] B. Zhang 和 P. Wonka，“使用概率嵌入的点云实例分割”，arXiv 预印本 arXiv：1912.00145,2019。</span><br><span class="line">[248] Z. Wang 和 F. Lu，“VoxSegNet：用于 3D 形状语义部分分割的体积 CNN”，IEEE TVCG，2019 年。</span><br><span class="line">[249] E. Kalogerakis、M. Averkiou、S. Maji 和 S. Chaudhuri，“使用投影卷积网络进行 3D 形状分割”，inCVPR，2017 年。</span><br><span class="line">[250] L. Yi、H. Su、X. Guo 和 L. J. Guibas，“SyncSpecCNN：用于 3D 形状分割的同步光谱 CNN”，CVPR，2017 年。</span><br><span class="line">[251] P. Wang、Y. Gan、P. Shui、F. Yu、Y. Zhang、S. Chen 和 Z. Sun，“通过形状全卷积网络进行 3D 形状分割”，计算机与图形学，2018 年。</span><br><span class="line">[252] C. Zhu、K. Xu、S. Chaudhuri、L. Yi、L. Guibas 和 H. Zhang，“CoSegNet：具有组一致性损失的 3D 形状的深度共分割”，arXiv 预印本 arXiv：1903.10297,2019 年。</span><br><span class="line">[253] Z. Chen、K. Yin、M. Fisher、S. Chaudhuri 和 H. Zhang，“BAENET：用于形状共分割的分支自动编码器”，inICCV，2019 年。</span><br><span class="line">[254] F. Yu、K. Liu、Y. Zhang、C. Zhu 和 K. Xu，“PartNet：用于细粒度和分层形状分割的递归部分分解网络”，CVPR，2019 年。</span><br><span class="line">[255] T. Luo、K. Mo、Z. Huang、J. Xu、S. Hu、L. Wang 和 H. Su，“学习分组：3D 零件发现看不见类别的自下而上的框架”，ICLR，2020 年。</span><br><span class="line">[256] Z. Liu、H. Tang、Y. Lin 和 S. Han，“用于高效 3D 深度学习的点体素 CNN”，载于 NeurIPS，2019 年。</span><br></pre></td></tr></table></figure>









    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="SindreYang 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="SindreYang 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>SindreYang
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://blog.mviai.com/2025/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91__3D%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0-2020%E5%B9%B4/" title="论文翻译__3D点云深度学习综述-2020年">http://blog.mviai.com/2025/论文翻译__3D点云深度学习综述-2020年/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/images/wechat.png">
            <span class="icon">
              <i class="fa fa-wechat"></i>
            </span>

            <span class="label">WeChat</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91_%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B_%E6%96%B9%E6%B3%95%E5%92%8C%E5%BA%94%E7%94%A8%E7%9A%84%E7%BB%BC%E5%90%88%E7%BB%BC%E8%BF%B0/" rel="prev" title="论文翻译_扩散模型_方法和应用的综合综述">
      <i class="fa fa-chevron-left"></i> 论文翻译_扩散模型_方法和应用的综合综述
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91_3D%E7%89%99%E9%BD%BF%E6%89%AB%E6%8F%8F%E5%88%86%E5%89%B2%E5%92%8C%E8%AF%86%E5%88%AB%E6%8C%91%E6%88%98/" rel="next" title="论文翻译_3D牙齿扫描分割和识别挑战">
      论文翻译_3D牙齿扫描分割和识别挑战 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="SOHUCS"></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%93%BE%E6%8E%A5%EF%BC%9A"><span class="nav-number">1.</span> <span class="nav-text">链接：</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D"><span class="nav-number">2.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-number">3.</span> <span class="nav-text">背景</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE"><span class="nav-number">3.1.</span> <span class="nav-text">数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%A8%E4%BA%8E3D%E5%BD%A2%E7%8A%B6%E5%88%86%E7%B1%BB%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">3.1.1.</span> <span class="nav-text">用于3D形状分类的数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%A8%E4%BA%8E3D%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E5%92%8C%E8%B7%9F%E8%B8%AA%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">3.1.2.</span> <span class="nav-text">用于3D物体检测和跟踪的数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%A8%E4%BA%8E3D%E7%82%B9%E4%BA%91%E5%88%86%E5%89%B2%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">3.1.3.</span> <span class="nav-text">用于3D点云分割的数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="nav-number">3.2.</span> <span class="nav-text">评估指标</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3D%E5%BD%A2%E7%8A%B6%E5%88%86%E7%B1%BB"><span class="nav-number">4.</span> <span class="nav-text">3D形状分类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%A4%9A%E8%A7%86%E5%9B%BE%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">4.1.</span> <span class="nav-text">基于多视图的方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E4%BD%93%E7%B4%A0%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">4.2.</span> <span class="nav-text">基于体素的方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%82%B9%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">4.3.</span> <span class="nav-text">基于点的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%90%E7%82%B9MLP%E6%96%B9%E6%B3%95"><span class="nav-number">4.3.1.</span> <span class="nav-text">逐点MLP方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%8D%B7%E7%A7%AF%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">4.3.2.</span> <span class="nav-text">基于卷积的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3D-%E8%BF%9E%E7%BB%AD%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="nav-number">4.3.2.1.</span> <span class="nav-text">3D 连续卷积方法：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A6%BB%E6%95%A3%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="nav-number">4.3.2.2.</span> <span class="nav-text">离散卷积方法：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">4.3.3.</span> <span class="nav-text">基于图的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%A9%BA%E9%97%B4%E5%9F%9F%E7%9A%84%E5%9B%BE%E6%96%B9%E6%B3%95"><span class="nav-number">4.3.3.1.</span> <span class="nav-text">基于空间域的图方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%85%89%E8%B0%B1%E5%9F%9F%E7%9A%84%E5%9B%BE%E6%96%B9%E6%B3%95"><span class="nav-number">4.3.3.2.</span> <span class="nav-text">基于光谱域的图方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%B1%82%E6%AC%A1%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">4.3.4.</span> <span class="nav-text">基于层次数据结构的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E6%96%B9%E6%B3%95"><span class="nav-number">4.3.5.</span> <span class="nav-text">其他方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">4.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3D%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B%E5%8F%8A%E8%B7%9F%E8%B8%AA"><span class="nav-number">5.</span> <span class="nav-text">3D对象检测及跟踪</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3D%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B"><span class="nav-number">5.1.</span> <span class="nav-text">3D对象检测</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%8C%BA%E5%9F%9F%E6%8F%90%E8%AE%AE%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">5.1.1.</span> <span class="nav-text">基于区域提议的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%A4%9A%E8%A7%86%E5%9B%BE%E7%9A%84%E6%96%B9%E6%B3%95-1"><span class="nav-number">5.1.1.1.</span> <span class="nav-text">基于多视图的方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%88%86%E5%89%B2%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">5.1.1.2.</span> <span class="nav-text">基于分割的方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E8%A7%86%E9%94%A5%E4%BD%93%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">5.1.1.3.</span> <span class="nav-text">基于视锥体的方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E6%96%B9%E6%B3%95%E3%80%82"><span class="nav-number">5.1.1.4.</span> <span class="nav-text">其他方法。</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E9%98%B6%E6%AE%B5%E6%96%B9%E6%B3%95"><span class="nav-number">5.1.2.</span> <span class="nav-text">单阶段方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E-BEV-%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="nav-number">5.1.2.1.</span> <span class="nav-text">基于 BEV 的方法：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%A6%BB%E6%95%A3%E5%8C%96%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">5.1.2.2.</span> <span class="nav-text">基于离散化的方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%82%B9%E7%9A%84%E6%96%B9%E6%B3%95-1"><span class="nav-number">5.1.2.3.</span> <span class="nav-text">基于点的方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E6%96%B9%E6%B3%95-1"><span class="nav-number">5.1.2.4.</span> <span class="nav-text">其他方法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3D-%E5%AF%B9%E8%B1%A1%E8%B7%9F%E8%B8%AA"><span class="nav-number">5.2.</span> <span class="nav-text">3D 对象跟踪</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3D-%E5%9C%BA%E6%99%AF%E6%B5%81%E4%BC%B0%E8%AE%A1"><span class="nav-number">5.3.</span> <span class="nav-text">3D 场景流估计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-1"><span class="nav-number">5.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3D%E7%82%B9%E4%BA%91%E5%88%86%E5%89%B2"><span class="nav-number">6.</span> <span class="nav-text">3D点云分割</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3D%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2"><span class="nav-number">6.1.</span> <span class="nav-text">3D语义分割</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%8A%95%E5%BD%B1%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">6.1.1.</span> <span class="nav-text">基于投影的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E8%A7%86%E5%9B%BE%E8%A1%A8%E7%A4%BA"><span class="nav-number">6.1.1.1.</span> <span class="nav-text">多视图表示</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%90%83%E5%BD%A2%E8%A1%A8%E7%A4%BA"><span class="nav-number">6.1.1.2.</span> <span class="nav-text">球形表示</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%A6%BB%E6%95%A3%E5%8C%96%E7%9A%84%E6%96%B9%E6%B3%95-1"><span class="nav-number">6.1.2.</span> <span class="nav-text">基于离散化的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%86%E9%9B%86%E7%A6%BB%E6%95%A3%E5%8C%96%E8%A1%A8%E7%A4%BA"><span class="nav-number">6.1.2.1.</span> <span class="nav-text">密集离散化表示</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A8%80%E7%96%8F%E7%A6%BB%E6%95%A3%E5%8C%96%E8%A1%A8%E7%A4%BA"><span class="nav-number">6.1.2.2.</span> <span class="nav-text">稀疏离散化表示</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B7%E5%90%88%E6%96%B9%E6%B3%95"><span class="nav-number">6.1.3.</span> <span class="nav-text">混合方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%82%B9%E7%9A%84%E6%96%B9%E6%B3%95-2"><span class="nav-number">6.1.4.</span> <span class="nav-text">基于点的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%90%E7%82%B9-MLP-%E6%96%B9%E6%B3%95"><span class="nav-number">6.1.4.1.</span> <span class="nav-text">逐点 MLP 方法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9B%B8%E9%82%BB%E8%A6%81%E7%B4%A0%E6%B1%A0%E5%8C%96"><span class="nav-number">6.1.4.1.1.</span> <span class="nav-text">相邻要素池化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E8%81%9A%E5%90%88"><span class="nav-number">6.1.4.1.2.</span> <span class="nav-text">基于注意力的聚合</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B1%80%E9%83%A8-%E5%85%A8%E5%B1%80%E4%B8%B2%E8%81%94"><span class="nav-number">6.1.4.1.3.</span> <span class="nav-text">局部-全局串联</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%82%B9%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95"><span class="nav-number">6.1.5.</span> <span class="nav-text">点卷积方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E-RNN-%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">6.1.6.</span> <span class="nav-text">基于 RNN 的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%9A%84%E6%96%B9%E6%B3%95-1"><span class="nav-number">6.1.7.</span> <span class="nav-text">基于图的方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2"><span class="nav-number">6.2.</span> <span class="nav-text">实例分割</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%8F%90%E6%A1%88%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">6.2.1.</span> <span class="nav-text">基于提案的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%97%A0%E6%8F%90%E6%A1%88%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">6.2.2.</span> <span class="nav-text">基于无提案的方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%83%A8%E4%BB%B6%E5%88%86%E5%89%B2"><span class="nav-number">6.3.</span> <span class="nav-text">部件分割</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-2"><span class="nav-number">6.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA"><span class="nav-number">7.</span> <span class="nav-text">结论</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%95%E7%94%A8"><span class="nav-number">8.</span> <span class="nav-text">引用</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SindreYang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">SindreYang</p>
  <div class="site-description" itemprop="description">沉淀后我愿意做一个温暖的人。有自己的喜好，有自己的原则，有自己的信仰，不急功近利，不浮夸轻薄，宠辱不惊，淡定安逸，心静如水。------不忘初心，方得始终</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">321</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1NpbmRyZVlhbmc=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;SindreYang"><i class="fa fa-fw fa-github"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnl4QG12aWFpLmNvbQ==" title="E-Mail → mailto:yx@mviai.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        
  <div class="languages">
    <label class="lang-select-label">
      <i class="fa fa-language"></i>
      <span>简体中文</span>
      <i class="fa fa-angle-up" aria-hidden="true"></i>
    </label>
    <select class="lang-select" data-canonical="">
      
        <option value="zh-CN" data-href="/2025/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91__3D%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0-2020%E5%B9%B4/" selected="">
          简体中文
        </option>
      
        <option value="en" data-href="/en/2025/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91__3D%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0-2020%E5%B9%B4/" selected="">
          English
        </option>
      
    </select>
  </div>

        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SindreYang</span>
</div><!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/love.js"></script>
<!-- 背景波浪 -->
<script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>


<!-- 腾讯企业邮箱 -->
<style>
.bizmail_loginpanel {
    font-size: 12px;
    width: 300px;
    height: auto;
    background: transparent;
    margin-left: auto;
    margin-right: auto;
}

.bizmail_LoginBox {
    padding: 10px 15px;
}


.bizmail_loginpanel form {
    margin: 0;
    padding: 0;
}

.bizmail_loginpanel input.text {
    font-size: 12px;
    width: 100px;
    height: 20px;
    margin: 0 2px;
    background-color: transparent;
    border:1px solid transparent;
    box-shadow: none;
    color: black;
}

.bizmail_loginpanel .bizmail_column {
    height: 28px;
}

.bizmail_loginpanel .bizmail_column label {
    display: block;
    float: left;
    width: 30px;
    height: 24px;
    line-height: 24px;
    font-size: 12px;
}

.bizmail_loginpanel .bizmail_column .bizmail_inputArea {
    float: left;
    width: 240px;
}

.bizmail_loginpanel .bizmail_column span {
    font-size: 12px;
    word-wrap: break-word;
    margin-left: 2px;
    line-height: 200%;
}

.bizmail_loginpanel .bizmail_SubmitArea {
    margin-left: 30px;
    clear: both;
}

.bizmail_loginpanel .bizmail_SubmitArea a {
    font-size: 12px;
    margin-left: 5px;
}

.bizmail_loginpanel select {
    width: 110px;
    height: 20px;
    margin: 0 2px;
}
.bizmail_loginpanel input {

    background-color: rgba(83, 126, 236, 0.562);
}


</style>

<script type="text/javascript">
function checkInput() {
    var e = document.form1.uin,
        i = document.form1.pwd;
    return 0 == e.value.length ? e.focus() : 0 == i.value.length ? i.focus() : (document.form1.submit(), setTimeout(" document.form1.pwd.value = '' ", 500)), !1
}

function writeLoginPanel(e) {
    if (e && e.domainlist && -1 != e.domainlist.indexOf(".")) {
        var a = "return checkInput()",
            t = '<div id="divLoginpanelHor" class="bizmail_loginpanel" style="width:550px;"><div class="bizmail_LoginBox"><form name="form1" action="https://exmail.qq.com/cgi-bin/login" target="_blank" method="post" onsubmit="' + a + '"><input type="hidden" name="firstlogin" value="false" /><input type="hidden" name="errtemplate" value="dm_loginpage" /><input type="hidden" name="aliastype" value="other" /><input type="hidden" name="dmtype" value="bizmail" /><input type="hidden" name="p" value="" /><label>\u8d26\u53f7:</label><input type="text" name="uin" class="text" value="" />@#domainlist#<label>&nbsp&nbsp&nbsp;\u5bc6\u7801:</label><input type="password" name="pwd" class="text" value="" /><input type="submit" class="" name="" value="\u767b\u5f55" />&nbsp;<a href="https://exmail.qq.com/cgi-bin/readtemplate?check=false&t=biz_rf_portal#recovery" target="_blank">\u5fd8\u8bb0\u5bc6\u7801\uff1f</a></form></div></div>',
            n = '<div id="divLoginpanelVer" class="bizmail_loginpanel"><div class="bizmail_LoginBox"><form name="form1" action="https://exmail.qq.com/cgi-bin/login" target="_blank" method="post" onsubmit="' + a + '"><input type="hidden" name="firstlogin" value="false" /><input type="hidden" name="errtemplate" value="dm_loginpage" /><input type="hidden" name="aliastype" value="other" /><input type="hidden" name="dmtype" value="bizmail" /><input type="hidden" name="p" value="" /><div class="bizmail_column"><label>\u8d26\u53f7:</label><div class="bizmail_inputArea"><input type="text" name="uin" class="text" value="" />@#domainlist#</div></div><div class="bizmail_column"><label>\u5bc6\u7801:</label><div class="bizmail_inputArea"><input type="password" name="pwd" class="text" value="" /></div></div><div class="bizmail_SubmitArea"><input type="submit" class="" name="" style="width:66px;" value="\u767b\u5f55" /><a href="https://exmail.qq.com/cgi-bin/readtemplate?check=false&t=biz_rf_portal#recovery" target="_blank">\u5fd8\u8bb0\u5bc6\u7801\uff1f</a></div></form></div></div>',
            l = e.domainlist.split(";");
        if (1 == l.length) var m = '<span>#domain#</span><input type="hidden" name="domain" value="#domain#" />'.replace(/#domain#/g, l[0]);
        else {
            m = '<select name="domain">';
            for (i = 0; i < l.length; i++) m += '<option value="' + l[i] + '">' + l[i] + "</option>";
            m += "</select>"
        }
        e.mode && "vertical" != e.mode && "both" != e.mode || document.write(n.replace(/#domainlist#/g, m)), "horizontal" != e.mode && "both" != e.mode || document.write(t.replace(/#domainlist#/g, m))
    }
}

</script>      

<script type="text/javascript"> writeLoginPanel({domainlist:"mviai.com", mode:"horizontal"});</script>      


        








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>


  




  
<script src="/js/local-search.js"></script>









<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  

  <script>
  NexT.utils.loadComments(document.querySelector('#SOHUCS'), () => {
    var appid = 'cyxmItxjS';
    var conf = 'e5e71132d9086bb54aeeba6e88e87df9';
    var width = window.innerWidth || document.documentElement.clientWidth;
    if (width < 960) {
      window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://cy-cdn.kuaizhan.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>');
    } else {
      var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:appid,conf:conf})});
    }
  });
  </script>
  <script src="https://cy-cdn.kuaizhan.com/upload/plugins/plugins.count.js"></script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"left","width":75,"height":150},"mobile":{"show":true},"log":false});</script></body>
</html>




